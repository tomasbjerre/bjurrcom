-- MySQL dump 10.13  Distrib 5.7.28, for Linux (x86_64)
--
-- Host: localhost    Database: ghost_production
-- ------------------------------------------------------
-- Server version	5.7.28-0ubuntu0.19.04.2

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `accesstokens`
--

DROP TABLE IF EXISTS `accesstokens`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `accesstokens` (
  `id` varchar(24) NOT NULL,
  `token` varchar(191) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  `client_id` varchar(24) NOT NULL,
  `issued_by` varchar(24) DEFAULT NULL,
  `expires` bigint(20) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `accesstokens_token_unique` (`token`),
  KEY `accesstokens_user_id_foreign` (`user_id`),
  KEY `accesstokens_client_id_foreign` (`client_id`),
  CONSTRAINT `accesstokens_client_id_foreign` FOREIGN KEY (`client_id`) REFERENCES `clients` (`id`),
  CONSTRAINT `accesstokens_user_id_foreign` FOREIGN KEY (`user_id`) REFERENCES `users` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `accesstokens`
--

LOCK TABLES `accesstokens` WRITE;
/*!40000 ALTER TABLE `accesstokens` DISABLE KEYS */;
/*!40000 ALTER TABLE `accesstokens` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `api_keys`
--

DROP TABLE IF EXISTS `api_keys`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `api_keys` (
  `id` varchar(24) NOT NULL,
  `type` varchar(50) NOT NULL,
  `secret` varchar(191) NOT NULL,
  `role_id` varchar(24) DEFAULT NULL,
  `integration_id` varchar(24) DEFAULT NULL,
  `last_seen_at` datetime DEFAULT NULL,
  `last_seen_version` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `api_keys_secret_unique` (`secret`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `api_keys`
--

LOCK TABLES `api_keys` WRITE;
/*!40000 ALTER TABLE `api_keys` DISABLE KEYS */;
/*!40000 ALTER TABLE `api_keys` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `app_fields`
--

DROP TABLE IF EXISTS `app_fields`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `app_fields` (
  `id` varchar(24) NOT NULL,
  `key` varchar(50) NOT NULL,
  `value` text,
  `type` varchar(50) NOT NULL DEFAULT 'html',
  `app_id` varchar(24) NOT NULL,
  `relatable_id` varchar(24) NOT NULL,
  `relatable_type` varchar(50) NOT NULL DEFAULT 'posts',
  `active` tinyint(1) NOT NULL DEFAULT '1',
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `app_fields_app_id_foreign` (`app_id`),
  CONSTRAINT `app_fields_app_id_foreign` FOREIGN KEY (`app_id`) REFERENCES `apps` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `app_fields`
--

LOCK TABLES `app_fields` WRITE;
/*!40000 ALTER TABLE `app_fields` DISABLE KEYS */;
/*!40000 ALTER TABLE `app_fields` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `app_settings`
--

DROP TABLE IF EXISTS `app_settings`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `app_settings` (
  `id` varchar(24) NOT NULL,
  `key` varchar(50) NOT NULL,
  `value` text,
  `app_id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `app_settings_key_unique` (`key`),
  KEY `app_settings_app_id_foreign` (`app_id`),
  CONSTRAINT `app_settings_app_id_foreign` FOREIGN KEY (`app_id`) REFERENCES `apps` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `app_settings`
--

LOCK TABLES `app_settings` WRITE;
/*!40000 ALTER TABLE `app_settings` DISABLE KEYS */;
/*!40000 ALTER TABLE `app_settings` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `apps`
--

DROP TABLE IF EXISTS `apps`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `apps` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `version` varchar(50) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'inactive',
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `apps_name_unique` (`name`),
  UNIQUE KEY `apps_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `apps`
--

LOCK TABLES `apps` WRITE;
/*!40000 ALTER TABLE `apps` DISABLE KEYS */;
/*!40000 ALTER TABLE `apps` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `brute`
--

DROP TABLE IF EXISTS `brute`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `brute` (
  `key` varchar(191) NOT NULL,
  `firstRequest` bigint(20) NOT NULL,
  `lastRequest` bigint(20) NOT NULL,
  `lifetime` bigint(20) NOT NULL,
  `count` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `brute`
--

LOCK TABLES `brute` WRITE;
/*!40000 ALTER TABLE `brute` DISABLE KEYS */;
INSERT INTO `brute` VALUES ('oHUubZQTM66eOWJCFaoi+8dO/eXPG5zwBOW8P5YAuKM=',1577958257026,1577958257026,1577961857029,1);
/*!40000 ALTER TABLE `brute` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `client_trusted_domains`
--

DROP TABLE IF EXISTS `client_trusted_domains`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `client_trusted_domains` (
  `id` varchar(24) NOT NULL,
  `client_id` varchar(24) NOT NULL,
  `trusted_domain` varchar(2000) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `client_trusted_domains_client_id_foreign` (`client_id`),
  CONSTRAINT `client_trusted_domains_client_id_foreign` FOREIGN KEY (`client_id`) REFERENCES `clients` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `client_trusted_domains`
--

LOCK TABLES `client_trusted_domains` WRITE;
/*!40000 ALTER TABLE `client_trusted_domains` DISABLE KEYS */;
/*!40000 ALTER TABLE `client_trusted_domains` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `clients`
--

DROP TABLE IF EXISTS `clients`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `clients` (
  `id` varchar(24) NOT NULL,
  `uuid` varchar(36) NOT NULL,
  `name` varchar(50) NOT NULL,
  `slug` varchar(50) NOT NULL,
  `secret` varchar(191) NOT NULL,
  `redirection_uri` varchar(2000) DEFAULT NULL,
  `client_uri` varchar(2000) DEFAULT NULL,
  `auth_uri` varchar(2000) DEFAULT NULL,
  `logo` varchar(2000) DEFAULT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'development',
  `type` varchar(50) NOT NULL DEFAULT 'ua',
  `description` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `clients_name_unique` (`name`),
  UNIQUE KEY `clients_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `clients`
--

LOCK TABLES `clients` WRITE;
/*!40000 ALTER TABLE `clients` DISABLE KEYS */;
INSERT INTO `clients` VALUES ('597b43301ffc934f6c0277b9','40a86af2-e27a-4cdb-a605-d0b39d7635db','Ghost Admin','ghost-admin','c94580c4ecc1',NULL,NULL,NULL,NULL,'enabled','ua',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ba','94d09d82-3d6c-44ba-8c01-24481fc9c6ee','Ghost Frontend','ghost-frontend','859d183cce14',NULL,NULL,NULL,NULL,'enabled','ua',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277bb','a95a2fc5-fd68-4d38-b1ce-01d7852b0c9f','Ghost Scheduler','ghost-scheduler','526b40de63d6',NULL,NULL,NULL,NULL,'enabled','web',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('59a12a16169cdd2cd7fbd507','707e1562-1dc4-4b2c-b9aa-cf914735b5eb','Ghost Backup','ghost-backup','83904ff7dd16',NULL,NULL,NULL,NULL,'enabled','web',NULL,'2017-08-26 07:58:14','1','2017-08-26 07:58:14','1');
/*!40000 ALTER TABLE `clients` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `integrations`
--

DROP TABLE IF EXISTS `integrations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `integrations` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `icon_image` varchar(2000) DEFAULT NULL,
  `description` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `integrations_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `integrations`
--

LOCK TABLES `integrations` WRITE;
/*!40000 ALTER TABLE `integrations` DISABLE KEYS */;
/*!40000 ALTER TABLE `integrations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `invites`
--

DROP TABLE IF EXISTS `invites`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `invites` (
  `id` varchar(24) NOT NULL,
  `role_id` varchar(24) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'pending',
  `token` varchar(191) NOT NULL,
  `email` varchar(191) NOT NULL,
  `expires` bigint(20) NOT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `invites_token_unique` (`token`),
  UNIQUE KEY `invites_email_unique` (`email`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `invites`
--

LOCK TABLES `invites` WRITE;
/*!40000 ALTER TABLE `invites` DISABLE KEYS */;
/*!40000 ALTER TABLE `invites` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members`
--

DROP TABLE IF EXISTS `members`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `members` (
  `id` varchar(24) NOT NULL,
  `email` varchar(191) NOT NULL,
  `name` varchar(191) NOT NULL,
  `password` varchar(60) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `members_email_unique` (`email`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members`
--

LOCK TABLES `members` WRITE;
/*!40000 ALTER TABLE `members` DISABLE KEYS */;
/*!40000 ALTER TABLE `members` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `migrations`
--

DROP TABLE IF EXISTS `migrations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `migrations` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(120) NOT NULL,
  `version` varchar(70) NOT NULL,
  `currentVersion` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `migrations_name_version_unique` (`name`,`version`)
) ENGINE=InnoDB AUTO_INCREMENT=33 DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `migrations`
--

LOCK TABLES `migrations` WRITE;
/*!40000 ALTER TABLE `migrations` DISABLE KEYS */;
INSERT INTO `migrations` VALUES (1,'1-create-tables.js','init','1.0'),(2,'2-create-fixtures.js','init','1.0'),(3,'1-post-excerpt.js','1.3','1.3'),(4,'1-codeinjection-post.js','1.4','1.7'),(5,'1-og-twitter-post.js','1.5','1.7'),(6,'1-add-backup-client.js','1.7','1.7'),(7,'1-custom-template-post.js','1.13','1.14'),(8,'2-theme-permissions.js','1.13','1.14'),(9,'1-add-permissions-redirect.js','1.9','1.14'),(10,'1-add-webhooks-table.js','1.18','1.18'),(11,'1-webhook-permissions.js','1.19','1.19'),(12,'1-remove-settings-keys.js','1.20','1.21'),(13,'1-add-contributor-role.js','1.21','1.21'),(14,'1-multiple-authors-DDL.js','1.22','1.24'),(15,'1-multiple-authors-DML.js','1.22','1.24'),(16,'1-update-koenig-beta-html.js','1.25','1.25'),(17,'2-demo-post.js','1.25','1.25'),(18,'1-rename-amp-column.js','2.0','2.0'),(19,'2-update-posts.js','2.0','2.0'),(20,'3-remove-koenig-labs.js','2.0','2.0'),(21,'4-permalink-setting.js','2.0','2.0'),(22,'5-remove-demo-post.js','2.0','2.0'),(23,'6-replace-fixture-posts.js','2.0','2.0'),(24,'1-add-sessions-table.js','2.2','2.3'),(25,'2-add-integrations-and-api-key-tables.js','2.2','2.3'),(26,'3-insert-admin-integration-role.js','2.2','2.3'),(27,'4-insert-integration-and-api-key-permissions.js','2.2','2.3'),(28,'5-add-mobiledoc-revisions-table.js','2.2','2.3'),(29,'1-add-webhook-columns.js','2.3','2.3'),(30,'2-add-webhook-edit-permission.js','2.3','2.3'),(31,'1-add-webhook-permission-roles.js','2.6','2.11'),(32,'1-add-members-table.js','2.8','2.11');
/*!40000 ALTER TABLE `migrations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `migrations_lock`
--

DROP TABLE IF EXISTS `migrations_lock`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `migrations_lock` (
  `lock_key` varchar(191) NOT NULL,
  `locked` tinyint(1) DEFAULT '0',
  `acquired_at` datetime DEFAULT NULL,
  `released_at` datetime DEFAULT NULL,
  UNIQUE KEY `migrations_lock_lock_key_unique` (`lock_key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `migrations_lock`
--

LOCK TABLES `migrations_lock` WRITE;
/*!40000 ALTER TABLE `migrations_lock` DISABLE KEYS */;
INSERT INTO `migrations_lock` VALUES ('km01',0,'2019-01-18 14:29:16','2019-01-18 14:29:17');
/*!40000 ALTER TABLE `migrations_lock` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mobiledoc_revisions`
--

DROP TABLE IF EXISTS `mobiledoc_revisions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mobiledoc_revisions` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `mobiledoc` longtext,
  `created_at_ts` bigint(20) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `mobiledoc_revisions_post_id_index` (`post_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mobiledoc_revisions`
--

LOCK TABLES `mobiledoc_revisions` WRITE;
/*!40000 ALTER TABLE `mobiledoc_revisions` DISABLE KEYS */;
INSERT INTO `mobiledoc_revisions` VALUES ('5c41e594f581f72431885ba4','597b4433e521cb4fbd9186a8','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Efter över 7 år som anställd konsult (mjukvaruutvecklare) valde jag nyligen att starta eget aktiebolag. Jag gör precis samma sak nu som jag gjorde som anställd men numera i mitt eget bolag istället. Det här inlägget handlar om hur jag gjorde detta. Googlar man kring att starta eget så hittar man mycket. Men jag saknade någon som beskriver det utifrån mina förutsättningar, det är vad jag försöker göra här, eftersom jag tror att många har samma förutsättningar som mig. **Oh, and sorry if you don\'t speak Swedish =) The topic is about starting your own company in Sweden, you are probably not interested.**\\n\\nSka också säga att allt jag tagit reda på här är genom googling och jag tar inget ansvar för att det jag säger här är korrekt och riktigt! Så var kritisk när du läser vad jag skriver nedan!\\n\\n# Varför Starta Eget?\\n\\nDu kan **skatteplanera**. Om du redan innan året är slut vet [gräns för statlig skatt](http://www.regeringen.se/pressmeddelanden/2017/03/upprakningen-av-skiktgranserna-for-statlig-inkomstskatt-2018-begransas-och-den-sarskilda-inkomstskatten-for-utomlands-bosatta-hojs/) och eventuella avdrag du kommer att göra. Då kan du räkna ut exakt vilken lön du kan betala ut från bolaget för att du som privatperson ska hamna precis på gränsen för statlig skatt.\\n\\nUtöver lönen kan du också göra en **utdelning**. Då skattar du 20% på summan upp till `2.75` gånger föregående års *inkomstbasbeloppet* för 2018 blir det `2.75 * 59300 = 163075`. Det är det som kallas förenklingsregeln. Man kan också räkna fram en gräns, som är hälften av utbetald lön, och använda den om den är mer fördelaktig. [Den här blog-posten](https://www.foretagande.se/sa-beraknas-skatt-pa-utdelning-3-12-reglerna/) beskriver detta bra. Första året kan du eventuellt inte använda förenklingsregeln, de diskuterar det i [den här tråden](https://forum.vismaspcs.se/visma_spcs/topics/utdelning-forsta-aret).\\n\\nJag har [gjort ett excel-ark](https://docs.google.com/spreadsheets/d/1jfQSSOEINNq3OVc5pYG5NeFkvgcDqOEu1HjzpU2RQm0/edit?usp=sharing) för att **optimera skatten**. Skatteverket har även [en sida](https://www.skatteverket.se/privat/sjalvservice/allaetjanster/tjanster/raknautdinskatt.4.18e1b10334ebe8bc80001950.html) där man kan räkna ut sin skatt. Du kan även läsa mer om [skatter och avgifter på Verksamt](https://www.verksamt.se/starta/skatter-och-avgifter/aktiebolag). Se även [den här sidan](http://www.driva-eget.se/kalkyler/lonekalkyl) för att få en uppfattning om hur mycket lön man kan ta ut.\\n\\nDet finns anledning att vara rädd för det faktum att man inte får några pengar alls om man inte har ett uppdrag. Men eget bolag kan du använda en **[periodiseringsfond](https://www.skatteverket.se/foretagochorganisationer/foretagare/enskildnaringsverksamhet/periodiseringsfond.4.361dc8c15312eff6fd2b8f2.html)** där du lägger undan pengar mellan räkenskapsår. Du kan alltså optimera din lön för att slippa statlig skatt, lägga undan pengar i en periodiseringsfond och sedan betala ut dem ett år då du har mindre inkomster. På så vis slipper du både bolagsskatten och statlig inkomstskatt.\\n\\nDu är **säkrare som egen**. Bolaget kommer förmodligen att dra in mer pengar än vad som är lämpligt att direkt batala ut som lön. Får du *500*kr/timme (ett väldigt lågt pris!) så drar du in cirka *80 000*kr under en månad. Då kan du betala *60 000*kr i lön. Men du vill förmodligen bara betala ut *36 575*kr eftersom det är gränsen för statlig skatt. Att betala den lönen kostar cirka *48 000*kr för bolaget och du har alltså *32 000*kr kvar i bolaget. Gör du det varje månad har du *384 000*kr kvar i bolaget när räkenskapsåret är slut. Du vill förmodligen använda en del för att göra en utdelning. Men för att jämföra med att vara anställd så skulle du kunna betala ut samma lön *6* månader in på nästa år, trots att du står helt utan inkomster! För att räkna ut hur mycket lön du kan betala ut kan du alltså räkna såhär `timpenning*(52-semesterveckor)*40/12/1.31` så kanske `500*(52-6)*40/12/1.31 = 58 524`.\\n\\nDu väljer själv om du vill betala in till en **tjänstepension** och i så fall vilken. Jag har tvingats betala till SEB under en längre tid. Det enda jag bryr mig om är deras fondutbud. I SEB hittade jag bara en fond, [en räntefond](http://web.msse.se/SEB/SGF/quicktake.aspx?six=SEB-BDSEKA), som kändes \\\"ok\\\". Det är även en fördel att du kan maximera lönen, göra utdelning och om det efter detta fortfarande finns pengar över kan du använda dem till tjänstepension. Du slipper alltså låsa in pengar i en sådan pension om det inte är så att det är en skattemässig fördel. Med tanke på att du, via utdelning, kan få ut kapital med en skatt på under 40% så kanske du inte vill använda en tjänstepension. Pengarna du får ut via utdelningen är ju helt olåsta och du kan investera hur du vill.\\n\\nDu **slipper ha lön efter ålder**. Ju yngre du är, ju mer tjänar du på att ha eget konsultbolag. Den dumma tanken att man ska börja på en låg lönenivå och sedan öka ett par procent per år mappar inte alls mot vad kunderna betala för dig per timme. Kunderna har oftast ett pris på juniorer och ett på seniorer. Att du är 30, 31 eller 40 år gammal spelar ingen roll för priset. Är du yngre anställd konsult betalar du för dina äldre kollegors lön och förmåner. Det kan mycket väl vara så att du drar in mer pengar än någon som är 20 år äldre. Med eget bolag mer än dubblade jag min lön.\\n\\nDu kan **köpa kontorsmaterial väldigt billigt**. Dator, headset, hörlurar, skärmar... till mindre än halva priset jämfört med om du köper det privat. Då tänker jag inte bara på momsen. Tänk även på hur mycket pengar bolaget behöver betala ut för att, efter skatt och arbetsgivaravgift, kunna sätta in summan på det privata kontot. Såklart måste man tänka på att det man köper ägs av företaget och inte dig privat.\\n\\nSå fort du har möjlighet att betala ut **[skattefritt traktamente](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm)** så ska du göra det. Du slipper tjafs med en arbetsgivare som nekar dig något som är så självklart.\\n\\nDu kan **åka på vilken konferens du vill** och låta bolaget betala. Det finns ju intressanta utvecklarkonferenser nästan överallt. Du behöver bara [hitta en](http://lmgtfy.com/?q=sidney+developer+conference) och åka. Inget tjafs med en arbetsgivare som tycker det är för dyrt. Inga dryga övningar, innan eller på plats, utan bara du som bestämmer vilka föreläsningar du tycker är intressanta och som du går på.\\n\\nJag tycker att de flesta möten är meningslösa och väldigt tråkiga. Det blir klart **färre möten** med eget bolag. Du kan spendera din tid med kunden istället för tråkiga månadsmöten =)\\n\\nDu kan få **mindre administrativt jobb**. Som anställd hade jag kvitton att rapportera varje vecka. Jag köpte tågbiljetter via SJ:s hemsida och fick alltså kvittot som PDF. Jag var tvungen att rapporera dem varje vecka med kostnad och moms. Skriva ut ett papper i samband med tidrapportering. Skriva ut mina kvitton. Häffta fast mina kvitton på pappret från tidrapporteringssystemet. Leta upp kuvert samt frimärke. Leta upp brevlåda och posta kuvert med tidrapporten till konsultbolagets kontor. Som egen lägger jag aldrig mer än 2 timmar totalt under en hel månad på att sköta den fullständiga bokföring inklusive alla kvitton. Allt gör jag digitalt.\\n\\nTill sist, och kanske det absolut bästa argumentet. Jag har **alltid sett mig själv som mitt eget varumärke**. Då är ju inte steget långt till att faktiskt vara det också.\\n\\n# Hur Starta Eget?\\nJag listar här kortfattat vad man behöver göra. Längre ner finns vissa steg mer utförligt beskrivna. Du kan räkna med att det tar 1-2 månader från att du skickar in första ansökan till att allt runt bolaget är klart och du kan börja jobba i det. Skadar inte att titta på [andra listor också](https://www.verksamt.se/alla-e-tjanster/din-checklista/aktiebolag) för att säkra att du inte glömt något.\\n\\nJag valde **aktiebolag** framför enskild firma. Då är bolaget en juridisk person som också har ansvaret. En tydlig gräns mellan bolag och privat ekonomi alltså. Jag kan rekommendera [Verksamts jämförelse av företagsformer](https://www.verksamt.se/fundera/valj-foretagsform).\\n\\n**Skapa ansökan** på [Verksamt](https://www.verksamt.se/). Du kommer behöva en verksamhetsbeskrivning. Min ser ut såhär:\\n> Företaget ska bedriva konsultverksamhet företrädesvis inom IT, samt utveckla mjukvaruprodukter, äga och förvalta värdepapper och utöva därmed förenlig verksamhet.\\n\\nNär det gäller **företagsnamn** så rekommenderar jag att bara ta ditt eget namn. AB eller aktiebolag måste vara med i namnet. Så jag valde *Tomas Bjerre AB*. Risken om du väljer något annat är att Bolagsverket nekar dig för att namnet liknar något som redan finns. Onödigt strul och du kan alltid ändra senare.\\n\\nDu kommer behöva ange en **SNI-kod**. Jag valde *62010*.\\n\\nNär du skickat iväg ansökan till Bolagsverket kommer de invänta ett **bankintyg**. Läs mer om att välja bank nedan. Du behöver kontakta en bank för att skapa ett företagskonto. Banken kommer be dig sätta in 50 000 kr på ett speciellt konto. Banken behöver kunna tala om för Bolagsverket att här finns det 50 000 kr som satts in med avsikt att användas för aktiekapitalet. När du gjort detta ger de dig en e-postadress som du sedan ger till Bolagsverket (via verksamt.se). Sedan frågan Bolagsverket banken om pengarna är där och när banken svarar ja registreras bolaget. Så fort bolaget är skapat så har du sedan ett företagskonto där det från start finns 50 000 kr.\\n\\nJag valde **räkenskapsåret** som *0101 - 1231*. Jag kan dock rekommendera brutet räkenskapsår. Alltså att man väljer ett år som inte är ett kalenderår. Fördelen blir då att du kan välja vilket år du betalar ut lönen på, om du når statlig skatt 2017 kan du vänta till 2018 med att fortsätta betala ut lön. Det är inte helt enkelt att [ändra i efterhand](https://www.skatteverket.se/download/18.3aa8c78a1466c584587d059/1408515815569/42405.pdf) och att ändra det bara för att undvika skatt är inte en godtagbar anledning.\\n\\nNär du startar aktiebolag ska bolaget ha **F-skatt** och du som privatperson har A-skatt. Jag valde **kvartalsmoms**. Som **redovisningsmetod** valde jag [kontantmetoden](https://www.e-conomic.se/bokforingsprogram/ordlista/kontantmetoden) (kallas även bokslutsmetoden). Jag uppfattade den som enklast, se hur man [bokför köpt med kreditkort](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) t.ex.\\n\\nDet är väldigt svårt att uppskatta den **förväntade vinsten** första året. Andra året kommer Skatteverket föreslå en. Ett tips är att ange den lågt. I mitt fall skulle jag först betala in 20 000 kr innan jag hunnit få en enda inkomst i bolaget. Skulle du råka ut för det är det bara att göra en ny preliminär inkomstdeklaration via Skatteverket och ange en lägre förväntad vinst. Då fick jag ner denna siffran till 7 000 kr istället.\\n\\nDet finns många smidiga **bokföringsprogram**. [Redovisningen](https://sv.wikipedia.org/wiki/Redovisning) var nog det jag var mest rädd för i början. Jag märkte tidigt att när jag googlade om hur man bokför olika saker så kom jag ofta in på [Visma:s support forum](https://forum.vismaspcs.se/visma_spcs). Jag har tidigare använt deras tidrapportering, PX, vilket suger något helt otroligt. Ser ut som en sommarjobbare hackade ihop det för 15 år sedan och att man inte rört det sedan dess. Men även om jag var väldigt skeptisk till detta bolag så valde jag ändå <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> för bokföring just för att deras support verkar vara helt exemplarisk. Att direkt bokföra ett kvitto i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> är betydligt enklare än det jag tidigare gjort via tidrapportering som anställd. Du kanske t.ex. vill bokföra [bokföringsprogrammet](https://forum.vismaspcs.se/visma_spcs/topics/8fxz374h0czze?topic-reply-list%5Bsettings%5D%5Bfilter_by%5D=all), [köp av dator och försäkring](https://forum.vismaspcs.se/visma_spcs/topics/7h3lv4csqal8w) eller [köp av tågbiljett](https://forum.vismaspcs.se/visma_spcs/topics/con2lpq5fex2f). De [gör t.o.m. filmer](https://www.youtube.com/watch?v=jGgnxd6uBh4) där de visar vissa flöden. Hoppas jag är trovärdig trots att jag gått med i deras affiliate-program och får betalt för att hänvisa andra dit =)\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">\\n\\n![Visma eEkonomi](/content/images/2017/07/affiliate-2014-vismaeekonomi1_728x90_eeko.gif)\\n</a>\\n\\nDu lär behöva en **mobil** i bolaget. Som jag förstår det är det ok att köpa telefonen på bolaget. Även telefoni och SMS är ok. Datatrafiken däremot kan behöva förmånsbeskattas. Det i kombination med att man slipper många dryga samtal om man istället har ett privat abonnemang, gjorde att jag köpte den privat.\\n\\nDu lär behöva en **ansvarsförsäkring**. Jag valde en [från If](https://www.if.se/web/se/foretag/varaforsakringar/offert/pages/default.aspx?varde=startforsakring) och fick betala 5 294 kr. Har du inte det och du råkar ut för något, säg att någon kräver dig på flera miljoner, så konkursar du bara bolaget och slipper undan. Men med en ansvarsförsäkring så kan du, kanske, göra rätt för dig och betala. På så vis blir du en mer attraktiv konsult att anlita och det står även ofta i kontrakten att du ska ha en försäkring.\\n\\nJag valde att kontakta en **redovisningskonsult** som hjälper mig med ekonomiska frågor samt tar hand om **bokslut** / **årsredovisning** / **deklaration**. Jag tar hand om löpande bokföring samt kvartalsmoms, skatter och lön. Det kostar cirka 6 000 kr per år. En redovisningskonsult kan hjälpa dig med allt möjligt i bolaget. Vissa ger dem fullmakt så de kan sköta all ekonomi. Jag fick tag på honom genom en bekant som tipsade men annars är det enkelt att googla. Har du valt Visma så [kan de hjälpa dig](https://vismaspcs.se/produkter/redovisning/visma-eekonomi-byra). I <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du även skapa en inloggning till din redovisningskonsult.\\n\\n**Revisor** är valfritt om man [omsätter under 3 miljoner](http://www.bolagsverket.se/ff/foretagsformer/aktiebolag/starta/revisor). Det använder jag inte.\\n\\nDet kan också vara bra att ha klart för sig **hur man hittar kunder**. Det finns många rena konsultmäklare som tar en procentsats, ofta mellan 10-20% på din timpenning, om du tar ett uppdrag via dem. De har ofta ramavtal med stora konsultköpare. Är du redan anställd som konsult kanske du i början fortsätter med samma kund fast som underkonsult istället. Se, t.ex.:\\n\\n * [Assistera](http://www.assistera.se/sv)\\n * [Brainville](https://www.brainville.com/)\\n * [EWork](https://www.eworkgroup.com/se/)\\n * [Experis](https://cv.experis.se/)\\n * [Kvadrat](http://www.kvadrat.se/bli-kvadratare/)\\n * [Toptal](https://www.toptal.com/)\\n\\nOch till sist, du kan eventuellt göra **[investeraravdraget](http://www.kompred.se/nyttiga-tips/missa-inte-investeraravdraget/)** i din privata deklaration efter att du startat aktiebolag.\\n\\n## Bank\\n\\nJag frågade först några banker via telefon och mail (ICA, Avanza, Swedbank). Det kändes som att de inte riktigt förstod vad jag menade när jag sa att jag ville ha ett **bankintyg**.\\n\\n### Danskebank\\n\\n**Jag valde Danskebank**. Du fyller bara i [deras formulär](https://danskebank.se/sv-se/Foretag/Smaforetag/Pages/Product-packages.aspx) så ringer de upp. Danskebank sköter allt per telefon och det fungerar mycket bra. De bad inte om någon affärsplan eller budget utan litade på mig direkt. De visste också direkt vad det handlade om och de kändes pålitliga. Eftersom jag inte var kund där sedan tidigare så behövde de skicka lite papper till mig med posten först. Men inom en vecka hade jag intyget! Snabbt, enkelt, smidigt och inget meningslöst krångel.\\n\\nJag valde *Danske Business Plus* för 99 kr/mån. **<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> stödjer även Danskebank** på så vis att man kan klistra in kontoutdraget från banken in i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>, för att automatiskt skapa bankhändelser. Sedan matchar man dem mot leverantörsfakturor eller verifikationer.\\n\\n# Hur Driva Eget?\\n\\nJag har lagt in återkommande händelser i Google Calendar för de olika datumen. Sedan angivit att de ska upprepas månadsvis eller, i vissa fall, årsvis. Jag lägger ungefär 2 timmar varje månad på att sköta företaget. Vad jag gör är:\\n\\n* En gång per månaden\\n  * Importerar transaktioner från skattekontot från Skatteverket.\\n  * Importerar transaktioner från företagskontot från banken.\\n  * Skickar fakturor för månaden.\\n  * Betalar ut lön, till anställde.\\n  * Betalar skatt och avgifter till Skatteverket\\n      * Arbetsgivaravgift.\\n      * Inkomstskatt.\\n      * Debiterad preliminärskatt.\\n* En gång per kvartal.\\n  * Betalar in kvartalsmoms.\\n* En gång per år.\\n  * Bokslut, årsredovisning, deklaration.\\n  * I januari, skickar in kontrolluppgit för föregående år till Skatteverket.\\n\\nJag valde **kvartalsmoms**, alltså att jag efter varje kvartal redovisar och betalar in eventuell moms. Mer information om det finns [här]( https://www.verksamt.se/driva/skatter-och-avgifter/moms-redovisa-och-betala). Så fort kvartalet är slut kan man lämna in deklarationen. Man har ungefär 1.5 månad på sig att lämna in den. Även för detta har Visma gjort en film som visar [hur du gör i eEkonomi](https://www.youtube.com/watch?v=gPv_SahMnEw). Som jag förstår det använder man kontot *1630* (eller *2012* om det är enskild firma) just för att [alla händelser mot Skatteverket ska gå via detta konto](https://forum.vismaspcs.se/visma_spcs/topics/maste-man-anvanda-sig-av-skattekontot-1630). Hur det fungerar med att betala, och få tillbaka moms, [beskriver Skatteverkets här](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html). Så kortfattat om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> och kvartalsmoms så ska du vid varje kvartalsslut:\\n\\n * Skapa en momsredovisning för kvartalet via bokföring / momsredovisning.\\n * Ladda ner *eskd*-filen.\\n * Skapa en [momsdeklaration](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms.4.7459477810df5bccdd480006935.html) enligt [Visma:s video](https://www.youtube.com/watch?v=gPv_SahMnEw).\\n * Ladda upp *eskd*-filen hos [Skatteverket](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html).\\n * Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n * När du senare betalat in, eller fått betalt, skapa och bokför bankhändelsen på företagskontot. Det visas i slutet på [den här videon](https://www.youtube.com/watch?v=gPv_SahMnEw). Tänk på att använda *1630* för AB och *2012* för enskild firma.\\n\\nSkatteverket har skickat brev till dig med datum då du ska betala **debiterad preliminärskatt**. Det här är något du betalar varje månad och baserar sig på din förväntade vinst som du angav i din preliminära inkomstdeklaration. Du har fått ett besked från Skatteverket där det står datum och hur mycket du ska betala. Om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du följa deras [film om skattekontot](https://www.youtube.com/watch?v=-7cDnn-NaME).\\n* Betala från företagskontot till företagets skattekonto.\\n* Importerar bankhändelser från företagskontot.\\n* Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n* Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n* När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n* Matcha händelsen mot bokföringsförslaget *Debiterad preliminärskatt*.\\n\\nNär du ska betala ut **Lön** behöver du betala *skatt*, *arbetsgivaravgift* samt själva *lönen*. Både *skatt* och *arbetsgivaravgift* betalas till företagets *skattekonto* så det är 2 betalningar som behöver göras från företagskontot. Om du använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du göra såhär. Finns även en [film här](https://www.youtube.com/watch?v=ig2C9gQg2Eg) och en [bra tråd](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-lon-i-eekonomi) om detta i supportforumet.\\n* Under *lön* i huvudnenyn väljer du *anställda*. Lägg upp den anställde. Du kommer behöva *Skattetabell* att använda. Den anställde kan [logga in](https://www.skatteverket.se/) hos Skatteverket och hittar då skattetabell under \\\"Skatter och deklarationer\\\".\\n* Under *lön* i huvudnenyn väljer du *lönebesked* och skapar ett nytt.\\n* Ange lönen och välj bokför.\\n* Logga in på banken och betala ut lönen, efter skatt, till den anställdes konto.\\n* Importera bankhändelsen till *företagskontot* via *kassa och bankhändelser*.\\n* <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kommer föreslå att du matchar bankhändelsen mot löneutbetalningen, gör det. Nu är du klar med lönen, och nästa moment är att betala skatt och arbetsgivaravgift.\\n\\n**Arbetsgivardeklaration** lämnas enkelt via [Skatteverkets webbsida](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html).\\n\\n* Om du inte har betalat ut någon lön sätter du bara **0** i de obligatoriska fälten och skickar in.\\n* Om du har betalat ut lön, och använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>.\\n  * Gå in under *lön* i huvudmenyn och välj *arbetsgivardeklaration*.\\n  * Skapa ny.\\n  * Välj månaden då lönen betaldes ut.\\n  * Välj *bokför*. \\n  * Nu kan du [exportera en fil](https://www.youtube.com/watch?v=ig2C9gQg2Eg) om du klickar på *åtgärder* för deklarationen.\\n  * [Lämna in arbetsgivardeklarationen](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html) hos Skatteverket genom att importera filen.\\n  * På kvittensen visas ett bankgiro och OCR att använda för att betala in skatten till företagets skattekonto.\\n  * Betala in summan från företagets bankkonto.\\n  * Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n  * När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n  * Matcha händelsen mot bokföringsförslaget arbetsgivardeklaration. Skatteverkat drar arbetsgivardeklartionen i 2 transaktioner, du behöver [manuellt slå ihop dem](https://forum.vismaspcs.se/visma_spcs/topics/hur-matchar-jag-tva-belopp-pa-skattekontot-mot-en-arbetsgivardeklaration) för att kunna använda det här bokföringsföslaget. Men annars är det *1630* som krediteras och *2710* (personalskatt) samt *2731* (arbetsgivaravgift) som debiteras. Jag tycker även [den här sidan](https://www.blinfo.se/foretagskunskap/bokfora-lon__15472) är bra här.\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> har en färdig mall för **fakturor**. Du anger ditt företags bankgiro, och/eller kontonummer, lägger upp kunder och artiklar. Sedan kan du enkelt skapa fakturor som du sedan kan skriva ut, spara som PDF eller maila.\\n\\nOm du **betalar ut traktamente** behöver du kunna bevisa att [resorna ägt rum och varit tjänsteresor](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm). Därför kan det vara klokt att föra anteckningar för resor. I mitt fall åker jag alltid tåg och kommer hänvisa till tågbiljetterna om jag får revision av Skatteverket. Jag skapar exceldokument, med uträkningen, som jag sedan exporterar till PDF och sparar för att motivera traktamentet.\\n\\n![Traktamente](/content/images/2017/11/traktamente-utr.png)\\n\\nMan hör ibland uttrycket **skriva av** och jag tror många missförstår det, kanske även jag =) Men jag tolkar det som att allt man köper för under ett halvt prisbasbelopp (ungefär 22 000 kr) skrivs av direkt, kostar det mer skriver man av det under flera år. Att *skriva av* innebär att man inte tar upp det som en tillgång, en inventarie, i bokföringen. Det är dock inte samma sak som att man får ge bort prylen till sig själv eller någon annan. Men jag är osäker, [och många med mig](https://www.flashback.org/p49423662).\\n\\nAtt bokföra köp gjorda med **kreditkort** är lite speciellt men det har Visma en väldigt [bra artikel](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) om.\\n\\nJag sparar alltid alla **kvitton** direkt på Google Drive. Är det inte digitalt fotar jag av med mobilen, laddar upp bilden och sparar även original-kvittot i en pärm. I bokföringsprogrammet är jag också noga med att **ladda upp bilder på varje verifikation**. [Bokföringslagen](https://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/bokforingslag-19991078_sfs-1999-1078) säger att originalen ska sparas, så digitala ska sparas digitalt och de på papper sparas på papper. Jag tycker även att [den här artikeln](https://vismaspcs.se/ditt-foretagande/byra/de-har-kopiorna-behover-du-inte-skriva-ut) beskriver det bra. Även andra dokument fotar jag av och sparar på Google Drive, som beslut från Skatteverket t.ex. Väldigt smidigt att kunna läsa allt var man än är.  Det är ganska enkelt att få tag på ett rejält brandskyddat kassaskåp ([#1](http://www.clasohlson.com/se/Dokumentsk%C3%A5p-Chubbsafes-Executive-40/Pr342428000) eller [#2](http://www.biltema.se/sv/Hem/Sakerhet-och-larm/Sakerhetsbox/Kassaskap-brandsakert-2000033385/)) så att man kan spara papper korrekt enligt lagstiftningen.\\n\\nPå **bokslutsdagen, sista dagen på räkenskapsåret, måste [obetalda fakturor tas upp i bokföring](https://www.verksamt.se/driva/ekonomi-och-bokforing/lopande-bokforing)** även om du använder kontantmetoden. Det beskriver också Visma [här](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi). Om du har skapat en faktura som ännu inte blivit betald kommer <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> att ge dig följande meddelande när du skapar sista **kvartalsmomsen**.\\n\\n> Enligt Kontantmetoden ska du bokföra dina obetalda kundfordringar och leverantörsskulder vid räkenskapsårets slut och redovisa momsen. Detta kommer nu att göras automatiskt. Du bör kontrollera att alla inbetalningar och utbetalningar på året är avstämda innan du fortsätter. Vill du fortsätta?\\n\\nDet är viktigt att ta en extra titt på de sista transaktionerna innan räkenskapsåret är slut så att det inte dykt upp något i sista sekunden. I mitt fall hade Danskebank skapat en faktura, och dragit pengarna, för bankavgifter sista dagen i december.\\n\\nNär räkenskapsåret är slut ska **kontrolluppgifter** för anställda skickas till Skatteverket. Det här gäller 2017 men är på gång att ändras så att man skickar samma uppgifter i arbetsgivardeklarationen istället. Men i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> finns en funktion under *lön* i huvudmenyn som kan användas om du skapat lönespecar.\\n\\n* Klicka på *Ny kontrolluppgift*.\\n* Fyll i dina uppgifter.\\n* Spara.\\n* Nu kan man ladda ner en *xml-fil* som kan laddas upp hos Skatteverket för att fylla i *KU10*-blanketten.\\n\\nVisma har en väldigt bra [gratis broschyr om bokföring](http://download.vismaspcs.se/pdf/bokf%C3%B6ring-f%C3%B6r-nyb%C3%B6rjare.pdf?utm_source=Eloqua&utm_medium=email&utm_content=SE_SP_SI_Onboarding-eEko-Std-4&utm_campaign=SE_SP_SI_Onboarding-eA-Std-1704).\\n\\n# Övrigt\\n\\nOm du **veckopendlar** som jag så kanske du har en **lägenhet på arbetsorten**. Då kan man tänka sig att man [låter bolaget betala den](http://www.skatter.se/?q=node/2619). Jag blev rekommenderad att ta den privat och betala ut mer lön från bolaget istället. Då räknar jag på detta vid utbetalning av lön så att lön efter avdrag hamnar på gränsen för statlig skatt. Avdraget gör jag alltså i min privata deklaration. Väljer man att låta bolaget direkt betala lägenheten blir det lite krångligare med bokföring och deklaration. Det kan också vara så att Skatteverket ser det som en förmån. Så det här kändes enklast.\\n\\nDet finns bolag som har som **affärsidé att bara ha underkonsulter**. De åker på konferensresor och har gemensam kontorslokal precis som vilket annat bolag som helst. Skillnaden är bara att alla är underkonsulter. Detta bolag hjälper dig att starta ditt eget bolag och hitta kunder. De har ramavtal med stora konsultköpare och kan ordna bra timpriser. Ett sånt bolag är [Kvadrat](http://www.kvadrat.se/).\\n\\nJag valde först att ta över kontraktet jag var på och då gå från anställd konsult till underkonsult. Senare bytte jag uppdrag och då anslöt jag mig till [Kvadrat](http://www.kvadrat.se/).\\n\\nHar du frågor om skatter så är det smidigt att man kan [maila Skatteverket](https://www.skatteverket.se/omoss/kontaktaoss/mejla/).\"}]],\"markups\":[],\"sections\":[[10,0]]}',1547822484639,'2019-01-18 14:41:24'),('5c41e594f581f72431885ba5','597b4433e521cb4fbd9186a8','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Efter över 7 år som anställd konsult (mjukvaruutvecklare) valde jag nyligen att starta eget aktiebolag. Jag gör precis samma sak nu som jag gjorde som anställd men numera i mitt eget bolag istället. Det här inlägget handlar om hur jag gjorde detta. Googlar man kring att starta eget så hittar man mycket. Men jag saknade någon som beskriver det utifrån mina förutsättningar, det är vad jag försöker göra här, eftersom jag tror att många har samma förutsättningar som mig. **Oh, and sorry if you don\'t speak Swedish =) The topic is about starting your own company in Sweden, you are probably not interested.**\\n\\nSka också säga att allt jag tagit reda på här är genom googling och jag tar inget ansvar för att det jag säger här är korrekt och riktigt! Så var kritisk när du läser vad jag skriver nedan!\\n\\n# Varför Starta Eget?\\n\\nDu kan **skatteplanera**. Om du redan innan året är slut vet [gräns för statlig skatt](http://www.regeringen.se/pressmeddelanden/2017/03/upprakningen-av-skiktgranserna-for-statlig-inkomstskatt-2018-begransas-och-den-sarskilda-inkomstskatten-for-utomlands-bosatta-hojs/) och eventuella avdrag du kommer att göra. Då kan du räkna ut exakt vilken lön du kan betala ut från bolaget för att du som privatperson ska hamna precis på gränsen för statlig skatt.\\n\\nUtöver lönen kan du också göra en **utdelning**. Då skattar du 20% på summan upp till `2.75` gånger föregående års *inkomstbasbeloppet* för 2018 blir det `2.75 * 59300 = 163075`. Det är det som kallas förenklingsregeln. Man kan också räkna fram en gräns, som är hälften av utbetald lön, och använda den om den är mer fördelaktig. [Den här blog-posten](https://www.foretagande.se/sa-beraknas-skatt-pa-utdelning-3-12-reglerna/) beskriver detta bra. Första året kan du eventuellt inte använda förenklingsregeln, de diskuterar det i [den här tråden](https://forum.vismaspcs.se/visma_spcs/topics/utdelning-forsta-aret).\\n\\nJag har [gjort ett excel-ark](https://docs.google.com/spreadsheets/d/1jfQSSOEINNq3OVc5pYG5NeFkvgcDqOEu1HjzpU2RQm0/edit?usp=sharing) för att **optimera skatten**. Skatteverket har även [en sida](https://www.skatteverket.se/privat/sjalvservice/allaetjanster/tjanster/raknautdinskatt.4.18e1b10334ebe8bc80001950.html) där man kan räkna ut sin skatt. Du kan även läsa mer om [skatter och avgifter på Verksamt](https://www.verksamt.se/starta/skatter-och-avgifter/aktiebolag). Se även [den här sidan](http://www.driva-eget.se/kalkyler/lonekalkyl) för att få en uppfattning om hur mycket lön man kan ta ut.\\n\\nDet finns anledning att vara rädd för det faktum att man inte får några pengar alls om man inte har ett uppdrag. Men eget bolag kan du använda en **[periodiseringsfond](https://www.skatteverket.se/foretagochorganisationer/foretagare/enskildnaringsverksamhet/periodiseringsfond.4.361dc8c15312eff6fd2b8f2.html)** där du lägger undan pengar mellan räkenskapsår. Du kan alltså optimera din lön för att slippa statlig skatt, lägga undan pengar i en periodiseringsfond och sedan betala ut dem ett år då du har mindre inkomster. På så vis slipper du både bolagsskatten och statlig inkomstskatt.\\n\\nDu är **säkrare som egen**. Bolaget kommer förmodligen att dra in mer pengar än vad som är lämpligt att direkt batala ut som lön. Får du *500*kr/timme (ett väldigt lågt pris!) så drar du in cirka *80 000*kr under en månad. Då kan du betala *60 000*kr i lön. Men du vill förmodligen bara betala ut *36 575*kr eftersom det är gränsen för statlig skatt. Att betala den lönen kostar cirka *48 000*kr för bolaget och du har alltså *32 000*kr kvar i bolaget. Gör du det varje månad har du *384 000*kr kvar i bolaget när räkenskapsåret är slut. Du vill förmodligen använda en del för att göra en utdelning. Men för att jämföra med att vara anställd så skulle du kunna betala ut samma lön *6* månader in på nästa år, trots att du står helt utan inkomster! För att räkna ut hur mycket lön du kan betala ut kan du alltså räkna såhär `timpenning*(52-semesterveckor)*40/12/1.31` så kanske `500*(52-6)*40/12/1.31 = 58 524`.\\n\\nDu väljer själv om du vill betala in till en **tjänstepension** och i så fall vilken. Jag har tvingats betala till SEB under en längre tid. Det enda jag bryr mig om är deras fondutbud. I SEB hittade jag bara en fond, [en räntefond](http://web.msse.se/SEB/SGF/quicktake.aspx?six=SEB-BDSEKA), som kändes \\\"ok\\\". Det är även en fördel att du kan maximera lönen, göra utdelning och om det efter detta fortfarande finns pengar över kan du använda dem till tjänstepension. Du slipper alltså låsa in pengar i en sådan pension om det inte är så att det är en skattemässig fördel. Med tanke på att du, via utdelning, kan få ut kapital med en skatt på under 40% så kanske du inte vill använda en tjänstepension. Pengarna du får ut via utdelningen är ju helt olåsta och du kan investera hur du vill.\\n\\nDu **slipper ha lön efter ålder**. Ju yngre du är, ju mer tjänar du på att ha eget konsultbolag. Den dumma tanken att man ska börja på en låg lönenivå och sedan öka ett par procent per år mappar inte alls mot vad kunderna betala för dig per timme. Kunderna har oftast ett pris på juniorer och ett på seniorer. Att du är 30, 31 eller 40 år gammal spelar ingen roll för priset. Är du yngre anställd konsult betalar du för dina äldre kollegors lön och förmåner. Det kan mycket väl vara så att du drar in mer pengar än någon som är 20 år äldre. Med eget bolag mer än dubblade jag min lön.\\n\\nDu kan **köpa kontorsmaterial väldigt billigt**. Dator, headset, hörlurar, skärmar... till mindre än halva priset jämfört med om du köper det privat. Då tänker jag inte bara på momsen. Tänk även på hur mycket pengar bolaget behöver betala ut för att, efter skatt och arbetsgivaravgift, kunna sätta in summan på det privata kontot. Såklart måste man tänka på att det man köper ägs av företaget och inte dig privat.\\n\\nSå fort du har möjlighet att betala ut **[skattefritt traktamente](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm)** så ska du göra det. Du slipper tjafs med en arbetsgivare som nekar dig något som är så självklart.\\n\\nDu kan **åka på vilken konferens du vill** och låta bolaget betala. Det finns ju intressanta utvecklarkonferenser nästan överallt. Du behöver bara [hitta en](http://lmgtfy.com/?q=sidney+developer+conference) och åka. Inget tjafs med en arbetsgivare som tycker det är för dyrt. Inga dryga övningar, innan eller på plats, utan bara du som bestämmer vilka föreläsningar du tycker är intressanta och som du går på.\\n\\nJag tycker att de flesta möten är meningslösa och väldigt tråkiga. Det blir klart **färre möten** med eget bolag. Du kan spendera din tid med kunden istället för tråkiga månadsmöten =)\\n\\nDu kan få **mindre administrativt jobb**. Som anställd hade jag kvitton att rapportera varje vecka. Jag köpte tågbiljetter via SJ:s hemsida och fick alltså kvittot som PDF. Jag var tvungen att rapporera dem varje vecka med kostnad och moms. Skriva ut ett papper i samband med tidrapportering. Skriva ut mina kvitton. Häffta fast mina kvitton på pappret från tidrapporteringssystemet. Leta upp kuvert samt frimärke. Leta upp brevlåda och posta kuvert med tidrapporten till konsultbolagets kontor. Som egen lägger jag aldrig mer än 2 timmar totalt under en hel månad på att sköta den fullständiga bokföring inklusive alla kvitton. Allt gör jag digitalt.\\n\\nTill sist, och kanske det absolut bästa argumentet. Jag har **alltid sett mig själv som mitt eget varumärke**. Då är ju inte steget långt till att faktiskt vara det också.\\n\\n# Hur Starta Eget?\\nJag listar här kortfattat vad man behöver göra. Längre ner finns vissa steg mer utförligt beskrivna. Du kan räkna med att det tar 1-2 månader från att du skickar in första ansökan till att allt runt bolaget är klart och du kan börja jobba i det. Skadar inte att titta på [andra listor också](https://www.verksamt.se/alla-e-tjanster/din-checklista/aktiebolag) för att säkra att du inte glömt något.\\n\\nJag valde **aktiebolag** framför enskild firma. Då är bolaget en juridisk person som också har ansvaret. En tydlig gräns mellan bolag och privat ekonomi alltså. Jag kan rekommendera [Verksamts jämförelse av företagsformer](https://www.verksamt.se/fundera/valj-foretagsform).\\n\\n**Skapa ansökan** på [Verksamt](https://www.verksamt.se/). Du kommer behöva en verksamhetsbeskrivning. Min ser ut såhär:\\n> Företaget ska bedriva konsultverksamhet företrädesvis inom IT, samt utveckla mjukvaruprodukter, äga och förvalta värdepapper och utöva därmed förenlig verksamhet.\\n\\nNär det gäller **företagsnamn** så rekommenderar jag att bara ta ditt eget namn. AB eller aktiebolag måste vara med i namnet. Så jag valde *Tomas Bjerre AB*. Risken om du väljer något annat är att Bolagsverket nekar dig för att namnet liknar något som redan finns. Onödigt strul och du kan alltid ändra senare.\\n\\nDu kommer behöva ange en **SNI-kod**. Jag valde *62010*.\\n\\nNär du skickat iväg ansökan till Bolagsverket kommer de invänta ett **bankintyg**. Läs mer om att välja bank nedan. Du behöver kontakta en bank för att skapa ett företagskonto. Banken kommer be dig sätta in 50 000 kr på ett speciellt konto. Banken behöver kunna tala om för Bolagsverket att här finns det 50 000 kr som satts in med avsikt att användas för aktiekapitalet. När du gjort detta ger de dig en e-postadress som du sedan ger till Bolagsverket (via verksamt.se). Sedan frågan Bolagsverket banken om pengarna är där och när banken svarar ja registreras bolaget. Så fort bolaget är skapat så har du sedan ett företagskonto där det från start finns 50 000 kr.\\n\\nJag valde **räkenskapsåret** som *0101 - 1231*. Jag kan dock rekommendera brutet räkenskapsår. Alltså att man väljer ett år som inte är ett kalenderår. Fördelen blir då att du kan välja vilket år du betalar ut lönen på, om du når statlig skatt 2017 kan du vänta till 2018 med att fortsätta betala ut lön. Det är inte helt enkelt att [ändra i efterhand](https://www.skatteverket.se/download/18.3aa8c78a1466c584587d059/1408515815569/42405.pdf) och att ändra det bara för att undvika skatt är inte en godtagbar anledning.\\n\\nNär du startar aktiebolag ska bolaget ha **F-skatt** och du som privatperson har A-skatt. Jag valde **kvartalsmoms**. Som **redovisningsmetod** valde jag [kontantmetoden](https://www.e-conomic.se/bokforingsprogram/ordlista/kontantmetoden) (kallas även bokslutsmetoden). Jag uppfattade den som enklast, se hur man [bokför köpt med kreditkort](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) t.ex.\\n\\nDet är väldigt svårt att uppskatta den **förväntade vinsten** första året. Andra året kommer Skatteverket föreslå en. Ett tips är att ange den lågt. I mitt fall skulle jag först betala in 20 000 kr innan jag hunnit få en enda inkomst i bolaget. Skulle du råka ut för det är det bara att göra en ny preliminär inkomstdeklaration via Skatteverket och ange en lägre förväntad vinst. Då fick jag ner denna siffran till 7 000 kr istället.\\n\\nDet finns många smidiga **bokföringsprogram**. [Redovisningen](https://sv.wikipedia.org/wiki/Redovisning) var nog det jag var mest rädd för i början. Jag märkte tidigt att när jag googlade om hur man bokför olika saker så kom jag ofta in på [Visma:s support forum](https://forum.vismaspcs.se/visma_spcs). Jag har tidigare använt deras tidrapportering, PX, vilket suger något helt otroligt. Ser ut som en sommarjobbare hackade ihop det för 15 år sedan och att man inte rört det sedan dess. Men även om jag var väldigt skeptisk till detta bolag så valde jag ändå <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> för bokföring just för att deras support verkar vara helt exemplarisk. Att direkt bokföra ett kvitto i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> är betydligt enklare än det jag tidigare gjort via tidrapportering som anställd. Du kanske t.ex. vill bokföra [bokföringsprogrammet](https://forum.vismaspcs.se/visma_spcs/topics/8fxz374h0czze?topic-reply-list%5Bsettings%5D%5Bfilter_by%5D=all), [köp av dator och försäkring](https://forum.vismaspcs.se/visma_spcs/topics/7h3lv4csqal8w) eller [köp av tågbiljett](https://forum.vismaspcs.se/visma_spcs/topics/con2lpq5fex2f). De [gör t.o.m. filmer](https://www.youtube.com/watch?v=jGgnxd6uBh4) där de visar vissa flöden. Hoppas jag är trovärdig trots att jag gått med i deras affiliate-program och får betalt för att hänvisa andra dit =)\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">\\n\\n![Visma eEkonomi](/content/images/2017/07/affiliate-2014-vismaeekonomi1_728x90_eeko.gif)\\n</a>\\n\\nDu lär behöva en **mobil** i bolaget. Som jag förstår det är det ok att köpa telefonen på bolaget. Även telefoni och SMS är ok. Datatrafiken däremot kan behöva förmånsbeskattas. Det i kombination med att man slipper många dryga samtal om man istället har ett privat abonnemang, gjorde att jag köpte den privat.\\n\\nDu lär behöva en **ansvarsförsäkring**. Jag valde en [från If](https://www.if.se/web/se/foretag/varaforsakringar/offert/pages/default.aspx?varde=startforsakring) och fick betala 5 294 kr. Har du inte det och du råkar ut för något, säg att någon kräver dig på flera miljoner, så konkursar du bara bolaget och slipper undan. Men med en ansvarsförsäkring så kan du, kanske, göra rätt för dig och betala. På så vis blir du en mer attraktiv konsult att anlita och det står även ofta i kontrakten att du ska ha en försäkring.\\n\\nJag valde att kontakta en **redovisningskonsult** som hjälper mig med ekonomiska frågor samt tar hand om **bokslut** / **årsredovisning** / **deklaration**. Jag tar hand om löpande bokföring samt kvartalsmoms, skatter och lön. Det kostar cirka 6 000 kr per år. En redovisningskonsult kan hjälpa dig med allt möjligt i bolaget. Vissa ger dem fullmakt så de kan sköta all ekonomi. Jag fick tag på honom genom en bekant som tipsade men annars är det enkelt att googla. Har du valt Visma så [kan de hjälpa dig](https://vismaspcs.se/produkter/redovisning/visma-eekonomi-byra). I <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du även skapa en inloggning till din redovisningskonsult.\\n\\n**Revisor** är valfritt om man [omsätter under 3 miljoner](http://www.bolagsverket.se/ff/foretagsformer/aktiebolag/starta/revisor). Det använder jag inte.\\n\\nDet kan också vara bra att ha klart för sig **hur man hittar kunder**. Det finns många rena konsultmäklare som tar en procentsats, ofta mellan 10-20% på din timpenning, om du tar ett uppdrag via dem. De har ofta ramavtal med stora konsultköpare. Är du redan anställd som konsult kanske du i början fortsätter med samma kund fast som underkonsult istället. Se, t.ex.:\\n\\n * [Assistera](http://www.assistera.se/sv)\\n * [Brainville](https://www.brainville.com/)\\n * [EWork](https://www.eworkgroup.com/se/)\\n * [Experis](https://cv.experis.se/)\\n * [Kvadrat](http://www.kvadrat.se/bli-kvadratare/)\\n * [Toptal](https://www.toptal.com/)\\n\\nOch till sist, du kan eventuellt göra **[investeraravdraget](http://www.kompred.se/nyttiga-tips/missa-inte-investeraravdraget/)** i din privata deklaration efter att du startat aktiebolag.\\n\\n## Bank\\n\\nJag frågade först några banker via telefon och mail (ICA, Avanza, Swedbank). Det kändes som att de inte riktigt förstod vad jag menade när jag sa att jag ville ha ett **bankintyg**.\\n\\n### Danskebank\\n\\n**Jag valde Danskebank**. Du fyller bara i [deras formulär](https://danskebank.se/sv-se/Foretag/Smaforetag/Pages/Product-packages.aspx) så ringer de upp. Danskebank sköter allt per telefon och det fungerar mycket bra. De bad inte om någon affärsplan eller budget utan litade på mig direkt. De visste också direkt vad det handlade om och de kändes pålitliga. Eftersom jag inte var kund där sedan tidigare så behövde de skicka lite papper till mig med posten först. Men inom en vecka hade jag intyget! Snabbt, enkelt, smidigt och inget meningslöst krångel.\\n\\nJag valde *Danske Business Plus* för 99 kr/mån. **<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> stödjer även Danskebank** på så vis att man kan klistra in kontoutdraget från banken in i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>, för att automatiskt skapa bankhändelser. Sedan matchar man dem mot leverantörsfakturor eller verifikationer.\\n\\n# Hur Driva Eget?\\n\\nJag har lagt in återkommande händelser i Google Calendar för de olika datumen. Sedan angivit att de ska upprepas månadsvis eller, i vissa fall, årsvis. Jag lägger ungefär 2 timmar varje månad på att sköta företaget. Vad jag gör är:\\n\\n* En gång per månaden\\n  * Importerar transaktioner från skattekontot från Skatteverket.\\n  * Importerar transaktioner från företagskontot från banken.\\n  * Skickar fakturor för månaden.\\n  * Betalar ut lön, till anställde.\\n  * Betalar skatt och avgifter till Skatteverket\\n      * Arbetsgivaravgift.\\n      * Inkomstskatt.\\n      * Debiterad preliminärskatt.\\n* En gång per kvartal.\\n  * Betalar in kvartalsmoms.\\n* En gång per år.\\n  * Bokslut, årsredovisning, deklaration.\\n  * I januari skickar in kontrolluppgit\\n      * Av utbetald lön för föregående år till Skatteverket.\\n      * Av föregående års utdelning (KU31).\\n  * Lämnar K10 i privata deklarationen, om utdelning mottagits under året som deklareras.\\n\\nJag valde **kvartalsmoms**, alltså att jag efter varje kvartal redovisar och betalar in eventuell moms. Mer information om det finns [här]( https://www.verksamt.se/driva/skatter-och-avgifter/moms-redovisa-och-betala). Så fort kvartalet är slut kan man lämna in deklarationen. Man har ungefär 1.5 månad på sig att lämna in den. Även för detta har Visma gjort en film som visar [hur du gör i eEkonomi](https://www.youtube.com/watch?v=gPv_SahMnEw). Som jag förstår det använder man kontot *1630* (eller *2012* om det är enskild firma) just för att [alla händelser mot Skatteverket ska gå via detta konto](https://forum.vismaspcs.se/visma_spcs/topics/maste-man-anvanda-sig-av-skattekontot-1630). Hur det fungerar med att betala, och få tillbaka moms, [beskriver Skatteverkets här](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html). Så kortfattat om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> och kvartalsmoms så ska du vid varje kvartalsslut:\\n\\n * Skapa en momsredovisning för kvartalet via bokföring / momsredovisning.\\n * Ladda ner *eskd*-filen.\\n * Skapa en [momsdeklaration](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms.4.7459477810df5bccdd480006935.html) enligt [Visma:s video](https://www.youtube.com/watch?v=gPv_SahMnEw).\\n * Ladda upp *eskd*-filen hos [Skatteverket](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html).\\n * Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n * När du senare betalat in, eller fått betalt, skapa och bokför bankhändelsen på företagskontot. Det visas i slutet på [den här videon](https://www.youtube.com/watch?v=gPv_SahMnEw). Tänk på att använda *1630* för AB och *2012* för enskild firma.\\n\\nSkatteverket har skickat brev till dig med datum då du ska betala **debiterad preliminärskatt**. Det här är något du betalar varje månad och baserar sig på din förväntade vinst som du angav i din preliminära inkomstdeklaration. Du har fått ett besked från Skatteverket där det står datum och hur mycket du ska betala. Om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du följa deras [film om skattekontot](https://www.youtube.com/watch?v=-7cDnn-NaME).\\n* Betala från företagskontot till företagets skattekonto.\\n* Importerar bankhändelser från företagskontot.\\n* Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n* Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n* När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n* Matcha händelsen mot bokföringsförslaget *Debiterad preliminärskatt*.\\n\\nNär du ska betala ut **Lön** behöver du betala *skatt*, *arbetsgivaravgift* samt själva *lönen*. Både *skatt* och *arbetsgivaravgift* betalas till företagets *skattekonto* så det är 2 betalningar som behöver göras från företagskontot. Om du använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du göra såhär. Finns även en [film här](https://www.youtube.com/watch?v=ig2C9gQg2Eg) och en [bra tråd](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-lon-i-eekonomi) om detta i supportforumet.\\n* Under *lön* i huvudnenyn väljer du *anställda*. Lägg upp den anställde. Du kommer behöva *Skattetabell* att använda. Den anställde kan [logga in](https://www.skatteverket.se/) hos Skatteverket och hittar då skattetabell under \\\"Skatter och deklarationer\\\".\\n* Under *lön* i huvudnenyn väljer du *lönebesked* och skapar ett nytt.\\n* Ange lönen och välj bokför.\\n* Logga in på banken och betala ut lönen, efter skatt, till den anställdes konto.\\n* Importera bankhändelsen till *företagskontot* via *kassa och bankhändelser*.\\n* <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kommer föreslå att du matchar bankhändelsen mot löneutbetalningen, gör det. Nu är du klar med lönen, och nästa moment är att betala skatt och arbetsgivaravgift.\\n\\n**Arbetsgivardeklaration** lämnas enkelt via [Skatteverkets webbsida](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html).\\n\\n* Om du inte har betalat ut någon lön sätter du bara **0** i de obligatoriska fälten och skickar in.\\n* Om du har betalat ut lön, och använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>.\\n  * Gå in under *lön* i huvudmenyn och välj *arbetsgivardeklaration*.\\n  * Skapa ny.\\n  * Välj månaden då lönen betaldes ut.\\n  * Välj *bokför*. \\n  * Nu kan du [exportera en fil](https://www.youtube.com/watch?v=ig2C9gQg2Eg) om du klickar på *åtgärder* för deklarationen.\\n  * [Lämna in arbetsgivardeklarationen](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html) hos Skatteverket genom att importera filen.\\n  * På kvittensen visas ett bankgiro och OCR att använda för att betala in skatten till företagets skattekonto.\\n  * Betala in summan från företagets bankkonto.\\n  * Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n  * När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n  * Matcha händelsen mot bokföringsförslaget arbetsgivardeklaration. Skatteverkat drar arbetsgivardeklartionen i 2 transaktioner, du behöver [manuellt slå ihop dem](https://forum.vismaspcs.se/visma_spcs/topics/hur-matchar-jag-tva-belopp-pa-skattekontot-mot-en-arbetsgivardeklaration) för att kunna använda det här bokföringsföslaget. Men annars är det *1630* som krediteras och *2710* (personalskatt) samt *2731* (arbetsgivaravgift) som debiteras. Jag tycker även [den här sidan](https://www.blinfo.se/foretagskunskap/bokfora-lon__15472) är bra här.\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> har en färdig mall för **fakturor**. Du anger ditt företags bankgiro, och/eller kontonummer, lägger upp kunder och artiklar. Sedan kan du enkelt skapa fakturor som du sedan kan skriva ut, spara som PDF eller maila.\\n\\nOm du **betalar ut traktamente** behöver du kunna bevisa att [resorna ägt rum och varit tjänsteresor](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm). Därför kan det vara klokt att föra anteckningar för resor. I mitt fall åker jag alltid tåg och kommer hänvisa till tågbiljetterna om jag får revision av Skatteverket. Jag skapar exceldokument, med uträkningen, som jag sedan exporterar till PDF och sparar för att motivera traktamentet.\\n\\n![Traktamente](/content/images/2017/11/traktamente-utr.png)\\n\\nMan hör ibland uttrycket **skriva av** och jag tror många missförstår det, kanske även jag =) Men jag tolkar det som att allt man köper för under ett halvt prisbasbelopp (ungefär 22 000 kr) skrivs av direkt, kostar det mer skriver man av det under flera år. Att *skriva av* innebär att man inte tar upp det som en tillgång, en inventarie, i bokföringen. Det är dock inte samma sak som att man får ge bort prylen till sig själv eller någon annan. Men jag är osäker, [och många med mig](https://www.flashback.org/p49423662).\\n\\nAtt bokföra köp gjorda med **kreditkort** är lite speciellt men det har Visma en väldigt [bra artikel](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) om.\\n\\nJag sparar alltid alla **kvitton** direkt på Google Drive. Är det inte digitalt fotar jag av med mobilen, laddar upp bilden och sparar även original-kvittot i en pärm. I bokföringsprogrammet är jag också noga med att **ladda upp bilder på varje verifikation**. [Bokföringslagen](https://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/bokforingslag-19991078_sfs-1999-1078) säger att originalen ska sparas, så digitala ska sparas digitalt och de på papper sparas på papper. Jag tycker även att [den här artikeln](https://vismaspcs.se/ditt-foretagande/byra/de-har-kopiorna-behover-du-inte-skriva-ut) beskriver det bra. Även andra dokument fotar jag av och sparar på Google Drive, som beslut från Skatteverket t.ex. Väldigt smidigt att kunna läsa allt var man än är.  Det är ganska enkelt att få tag på ett rejält brandskyddat kassaskåp ([#1](http://www.clasohlson.com/se/Dokumentsk%C3%A5p-Chubbsafes-Executive-40/Pr342428000) eller [#2](http://www.biltema.se/sv/Hem/Sakerhet-och-larm/Sakerhetsbox/Kassaskap-brandsakert-2000033385/)) så att man kan spara papper korrekt enligt lagstiftningen.\\n\\nPå **bokslutsdagen, sista dagen på räkenskapsåret, måste [obetalda fakturor tas upp i bokföring](https://www.verksamt.se/driva/ekonomi-och-bokforing/lopande-bokforing)** även om du använder kontantmetoden. Det beskriver också Visma [här](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi). Om du har skapat en faktura som ännu inte blivit betald kommer <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> att ge dig följande meddelande när du skapar sista **kvartalsmomsen**.\\n\\n> Enligt Kontantmetoden ska du bokföra dina obetalda kundfordringar och leverantörsskulder vid räkenskapsårets slut och redovisa momsen. Detta kommer nu att göras automatiskt. Du bör kontrollera att alla inbetalningar och utbetalningar på året är avstämda innan du fortsätter. Vill du fortsätta?\\n\\nDet är viktigt att ta en extra titt på de sista transaktionerna innan räkenskapsåret är slut så att det inte dykt upp något i sista sekunden. I mitt fall hade Danskebank skapat en faktura, och dragit pengarna, för bankavgifter sista dagen i december.\\n\\nNär räkenskapsåret är slut ska **kontrolluppgifter** för anställda skickas till Skatteverket. Det här gäller 2017 men är på gång att ändras så att man skickar samma uppgifter i arbetsgivardeklarationen istället. Men i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> finns en funktion under *lön* i huvudmenyn som kan användas om du skapat lönespecar.\\n\\n* Klicka på *Ny kontrolluppgift*.\\n* Fyll i dina uppgifter.\\n* Spara.\\n* Nu kan man ladda ner en *xml-fil* som kan laddas upp hos Skatteverket för att fylla i *KU10*-blanketten.\\n\\nVisma har en väldigt bra [gratis broschyr om bokföring](http://download.vismaspcs.se/pdf/bokf%C3%B6ring-f%C3%B6r-nyb%C3%B6rjare.pdf?utm_source=Eloqua&utm_medium=email&utm_content=SE_SP_SI_Onboarding-eEko-Std-4&utm_campaign=SE_SP_SI_Onboarding-eA-Std-1704).\\n\\n# Övrigt\\n\\nOm du **veckopendlar** som jag så kanske du har en **lägenhet på arbetsorten**. Då kan man tänka sig att man [låter bolaget betala den](http://www.skatter.se/?q=node/2619). Jag blev rekommenderad att ta den privat och betala ut mer lön från bolaget istället. Då räknar jag på detta vid utbetalning av lön så att lön efter avdrag hamnar på gränsen för statlig skatt. Avdraget gör jag alltså i min privata deklaration. Väljer man att låta bolaget direkt betala lägenheten blir det lite krångligare med bokföring och deklaration. Det kan också vara så att Skatteverket ser det som en förmån. Så det här kändes enklast.\\n\\nDet finns bolag som har som **affärsidé att bara ha underkonsulter**. De åker på konferensresor och har gemensam kontorslokal precis som vilket annat bolag som helst. Skillnaden är bara att alla är underkonsulter. Detta bolag hjälper dig att starta ditt eget bolag och hitta kunder. De har ramavtal med stora konsultköpare och kan ordna bra timpriser. Ett sånt bolag är [Kvadrat](http://www.kvadrat.se/).\\n\\nJag valde först att ta över kontraktet jag var på och då gå från anställd konsult till underkonsult. Senare bytte jag uppdrag och då anslöt jag mig till [Kvadrat](http://www.kvadrat.se/).\\n\\nHar du frågor om skatter så är det smidigt att man kan [maila Skatteverket](https://www.skatteverket.se/omoss/kontaktaoss/mejla/).\"}]],\"markups\":[],\"sections\":[[10,0]]}',1547822484640,'2019-01-18 14:41:24'),('5c4604b327d96a0621fc1761','5a755597c56a61057b219788','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This is a pattern that I find simple, easy/quick while still keeping you in control of your build flow. Should be no problem applying it in an organization with a huge amount of repositories!\\n\\nI am exclusively involved in **Java projects**. This post will sometimes assume *Maven* is being used. But it can probably inspire a solution in other projects too. I will not supply the complete running code. I\'m just going to include small snippets and focus on explaining the general idea.\\n\\n# Problems\\nSplitting a big application into several smaller artifacts is a good thing! To me, that is obvious but still I find myself talking to people that don\'t agree on that. Here are some of the arguments I hear on why not to split applications.\\n\\n> \\\"I want to build whatever I have checked out, locally, on my filesystem. We will need to spend many frustrating hours stepping dependencies between artifacts.\\\"\\n\\n> \\\"It is hard to detect when artifacts no longer fit together. We will find severe problems late because we don\'t continuously integrate all the artifacts.\\\"\\n\\n> \\\"We will need a huge amount of jobs in Jenkins (releasing, testing, integrating, deploying, snapshots...). We will need to spend much time managing them.\\\"\\n\\nOk! All valid points! And **all of them are more or less show stoppers if you don\'t do continuous integration right!**\\n\\n# Solution\\nIn short, I propose a solution where you:\\n\\n * Define a clear branching strategy.\\n * Define a translation strategy between branch and Maven artifact version.\\n * Define how any given repo should be built.\\n * Automate and define how repos depend on each other.\\n * Add a *Jenkinsfile* to each repo.\\n * Create a [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/).\\n * Automate the creation of the jobs.\\n\\nYou might consider [Pipeline Multibranch Plugin](https://wiki.jenkins.io/display/JENKINS/Pipeline+Multibranch+Plugin). I use [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) and [Folders Plugin](https://plugins.jenkins.io/cloudbees-folder). I have it create ordinary [pipeline](https://jenkins.io/doc/book/pipeline/)-jobs in a folder structure with a folder for each repository. \\n\\n* It gives me something static, the name of each job, to use when chaining jobs that depend on each other. There is a folder for each repository and it contains the jobs.\\n* It also allows several jobs to work with the same branch. I can easily create a release-job and a snapshot-job that both work with the *develop* branch.\\n\\nYou could have a static release-job and use mutlibranch to dynamically create every other job. But still, I feel I have more control with *Job DSL* and I feel it makes Jenkins look more organized.\\n\\n## Branching strategy\\nYou must know the meaing of the branches, in any given repo, in order to automate things. A defined branching strategy enables you to:\\n\\n * Clone any given repo.\\n * Detect what branches exists.\\n * Be sure which branch to use for *snapshots* or *releases*.\\n\\nIf your strategy is [GitFlow](https://bjurr.com/gitflow-and-when-you-should-use-it/), then:\\n\\n * The *snapshot*-job will\\n   * Build snapshots from *develop*.\\n   * Step dependencies in *develop*.\\n * The *release*-job will\\n   * Build releases from *hotfix* if it exists or else  *release*.\\n * The *feature*-job will build any *feature/X*-branches.\\n\\nEach repository has one release-cycle. Several artifacts, in the same repo, with different release-cycles are not allowed.\\n\\n## Branch to version translation\\n\\nThe integration between the Git service and Jenkins is setup so that when a commit is pushed to a feature-branch:\\n* A job is triggered.\\n* The branch name is identified.\\n* A version is derived form the branch name.\\n* Check to see if there is global a `bom` with that version\\n  * If no `bom` fall back to some default, fail or automate creation of that ´bom´-version.\\n* The artifacts are built, with a `bom` with the version, and uploaded to a Maven repository.\\n\\nThe `bom` -repo may function in the same way. Developers branch out of *develop*, getting all default versions. They set specific versions for some artifacts and commit/push. Or you automate that same procedure whenever a `bom`-version is missing. Then the artifact that originally triggered the creation of that bom can find its version and set it to itself.\\n\\nThen it will be possible to automatically create *deploy*-jobs for any \\\"deployaple\\\" repository. Where a dropdown list can automatically be populated with features. Features are found by listing feature-branches and translating to versions.\\n\\nDevelopers won\'t have to fiddle with versions locally, they just have whatever version that is also in *develop*-branches. And can clone a bunch of repos, built locally, and work with that fitting together.\\n\\n## Building\\n\\nYou must know how to build any given repo. With Maven, the aggreement might be as simple as:\\n\\n* The project is built from the root of the repo.\\n* The version, of the repository, is specified in the root of the repo. When using Maven, it is the version of the *pom.xml*.\\n* Different Maven profiles are allowed. Any profiles that produces artifacts should be specified as metadata about the repo in the *Jenkinsfile*.\\n\\nThe important thing is to define these rules. Do not start treating specific repos differently in the global build scripts. Instead specify global rules that all repos should follow.\\n\\n## Depending repositories\\n\\nTo be able to automatically chain jobs and have them trigger each other, I need to know *depending repos* of each repo. The opposite of what you have in `pom.xml`.\\n\\nOne way of doing that is with a job that:\\n\\n * Regularly finds all repos, perhaps via the Git service REST-API.\\n * Parse the `pom.xml`-files.\\n   * Find out what `artifacts` are contains in what repos.\\n   * Find out what `artifacts` are used in what repos.\\n * Create a structure with the depending repos per repo.\\n * Optimize that structure so that transitive dependencies are removed from list of direct dependencies.\\n * Store that structure as a json text -file in a repo. Making it available for snapshot/release-jobs to clone and include.\\n\\nHaving this information pre-calculated saves alot of time when it is needed by some job.\\n\\nPerhaps the depending repo structure can look something like this.\\n```\\n{\\n ...\\n \\\"PROJECT-A/example-repo-d\\\": [\\n  \\\"PROJECT-B/example-repo-b\\\",\\n  \\\"PROJECT-C/example-repo-d\\\"\\n ],\\n \\\"PROJECT-C/example-repo-d\\\": [\\n  \\\"PROJECT-E/example-repo-b\\\"\\n ]\\n ...\\n}\\n```\\n\\n## Jenkinsfile\\nIt is very small and contains only metadata about the repo. This is just like [Jenkins Infra](https://github.com/jenkins-infra/pipeline-library/blob/master/vars/buildPlugin.groovy) handles their 1000+ plugins.\\n\\nWhen using Maven, you might want to specify *profiles* to be built.\\n\\nA repo that needs to be built, nothing else, may look like this:\\n```\\nbuildRepo()\\n```\\n\\nIt may specify profiles:\\n```\\nbuildRepo(\\n profiles: [\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nAnd if a profiles are needed as well as no profile, it may look like:\\n```\\nbuildRepo(\\n profiles: [\\n  \'\',\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nThis is all there is in the repositories. Only one *Jenkinsfile* and this is the only information it contains. I\'m not saying you only need this. I\'m just recommending to keep it light! Perhaps you invent thnkgs like `deployable: true` or `autoDeployEnv: \'TEST-XY\'`...\\n\\n## Shared library\\nA [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/) allows you to, only once, define how to do releases, snapshots and all other tasks.\\n\\nWith the above *Jenkinsfile* there should be a `/vars/buildRepo.groovy` containing something like:\\n```\\n...\\ndef call(Map params = [:]) {\\n...\\n if (JOB_BASE_NAME.equals(\\\"snapshot\\\")) {\\n ...\\n } else if (JOB_BASE_NAME.equals(\\\"release\\\")) {\\n ...\\n }\\n...\\n}\\n...\\n```\\n\\n## Automate creation of jobs\\nMost Git services (GitHub, GitLab, Bitbucket...) provide REST API:s. You can use that to automate creation/deletion/adjustment of jobs and always be in sync with the repos you have in your Git service. The job DSL would loop through all repositories.\\n\\n```\\n...\\n folder(\\\"gen\\\") {\\n  displayName(\\\"Generated jobs\\\")\\n  description(\\\"\\\"\\\"\\n   These are generated by ${JOB_URL}\\n  \\\"\\\"\\\")\\n }\\n\\n getJson(server+ \\\"/rest/request/to/repos...\\\")\\n  .values\\n  .each { repo ->\\n  folder(\\\"gen\\\") {\\n   displayName(\\\"gen/\\\" + repo.name)\\n   description(\\\"\\\"\\\"\\n    Generated by ${JOB_URL}\\n   \\\"\\\"\\\")\\n  }\\n\\n  pipelineJob(\\\"gen/\\\" + repo.name + \\\"/snapshot\\\") {\\n...\\n```\\n\\n## Templates\\nI use the [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) plugin. Perhaps you want these jobs for every repository:\\n\\n * *snapshot*\\n * *release*\\n * *feature*\\n * *pull-request*\\n\\nAlso a global job, *release-orchestration*.\\n\\nAll of these templates are pipelines. Their logic is implemented in the shared library. The shared library will find the Git repo to use from `scm.getUserRemoteConfigs().get(0).getUrl()` and the kind of job to build from `JOB_BASE_NAME`.\\n\\n### Snapshot\\nThis job will:\\n* Make sure *develop* is using latest dependencies ([found in Maven repository](http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html)). If there are newer versions:\\n  * Step dependencies to latest version.\\n  * Commit changes.\\n  * Push changes.\\n  * Re-trigger self, to help Jenkins understand that this new commit does not need to be built again. Done.\\n* Build a snapshot version.\\n* Upload snapshot-version to Maven repository.\\n* Trigger *dependingRepos* configured in *Jenkinsfile*.\\n* Done.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release\\nThis job will:\\n* Start from commit *C1*.\\n* Set dependencies to latest released versions.\\n* Step version to next release-version.\\n* Make a commit *C2*.\\n* Set dependencies to latest snapshots.\\n* Step version to next snapshot-version.\\n* Make a commit *C3*.\\n* Try to push changes. If not successful:\\n  * Hard reset to *C1*.\\n  * Pull.\\n  * Start again with creating *C2*.\\n  * Do this loop, perhaps 5 times before giving up and fail.\\n\\nThis allows developers to work in the branches during the release-process.\\n\\nNow that we know we are in sync with remote Git repo on where to perform the release, we can continue doing so.\\n\\n* Tag *C2* with the release-version.\\n* Perform the build commands, `mvn package` and loop any profiles needed.\\n* Deploy in Maven repository, `mvn deploy`.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release orchestration\\nThis job will:\\n * Orchestrates a release.\\n * Is parameterized with each repo.\\n * When triggered:\\n   * Calculates the order to release selected repos. With the information found in their *Jenkinsfile*:s.\\n   * Invokes the *release*-jobs of each selected repo.\\n\\n# Features\\nHere is what features this setup can provide and how I intend them to be used.\\n## Release\\n\\nA release of a single repo can be performed from its *release*-job.\\n\\nThis will look at each repo and release from the branch that is first found in this order:\\n\\n1. *hotfix*\\n2. *release*\\n3. *develop*\\n4. *master*\\n\\nSo if you want to release from a specific commit, not latest *develop*, just push a *release*, or *hotfix*, -branch that points to that commit.\\n\\n### Orchestrating a release\\n\\nA release of one, or more, repos can be performed from the global *release-orchestration*-job. This will:\\n\\n* Ensure the release of each repo\\n  * Is done in the right order.\\n  * Their dependencies will be released first, if selected.\\n  * Use the latest release of their dependencies.\\n* After release, trigger the snapshot-job of the first repo that was released. So that all the snapshot-jobs will run and step snapshot-versions.\\n\\nIt will invoke the release-jobs of each repo. This means you can have a look there for more details on that specific release.\\n\\n## Hotfix\\n\\nHaving the priority among branches, mentioned above, will enable you to push a *hotfix*-branch from any commit and have the release being performed from that commit. If your *master* points to latest installed version:\\n\\n* `git checkout master`\\n* `git checkout -b hotfix`\\n* `git push -u origin hotfix`\\n\\nThen just trigger the release.\\n# Advantages\\nYour entire **Jenkins configuration is put under version control**. Well... you need to create one Job DSL -job manually that polls, or is triggered by changes in, the git service. But that job can have its DSL in a Git-repo. This has a bunch of advantages.\\n\\n * No more browsing around in Jenkins and fiddling with settings.\\n * You can track changes in the jobs. Just use `git blame`, it is all code now!\\n * All your jobs are backed up with Git.\\n * You can easily setup a development instance of Jenkins that behaves very much like your production instance.\\n * You can generate release-jobs in one Jenkins and snapshot jobs in another. Letting only a few people use the release-jenkins and anyone use the other instance.\"}]],\"markups\":[],\"sections\":[[10,0]]}',1548092595375,'2019-01-21 17:43:15'),('5c4604b327d96a0621fc1762','5a755597c56a61057b219788','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This is a pattern that I find simple, easy/quick while still keeping you in control of your build flow. Should be no problem applying it in an organization with a huge amount of repositories!\\n\\nI am exclusively involved in **Java projects**. This post will sometimes assume *Maven* is being used. But it can probably inspire a solution in other projects too. I will not supply the complete running code. I\'m just going to include small snippets and focus on explaining the general idea.\\n\\nSome of this is implemented with Jenkins Configuration as Code, here: https://github.com/tomasbjerre/docker-jenkins-sandbox\\n\\n# Problems\\nSplitting a big application into several smaller artifacts is a good thing! To me, that is obvious but still I find myself talking to people that don\'t agree on that. Here are some of the arguments I hear on why not to split applications.\\n\\n> \\\"I want to build whatever I have checked out, locally, on my filesystem. We will need to spend many frustrating hours stepping dependencies between artifacts.\\\"\\n\\n> \\\"It is hard to detect when artifacts no longer fit together. We will find severe problems late because we don\'t continuously integrate all the artifacts.\\\"\\n\\n> \\\"We will need a huge amount of jobs in Jenkins (releasing, testing, integrating, deploying, snapshots...). We will need to spend much time managing them.\\\"\\n\\nOk! All valid points! And **all of them are more or less show stoppers if you don\'t do continuous integration right!**\\n\\n# Solution\\nIn short, I propose a solution where you:\\n\\n * Define a clear branching strategy.\\n * Define a translation strategy between branch and Maven artifact version.\\n * Define how any given repo should be built.\\n * Automate and define how repos depend on each other.\\n * Add a *Jenkinsfile* to each repo.\\n * Create a [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/).\\n * Automate the creation of the jobs.\\n\\nYou might consider [Pipeline Multibranch Plugin](https://wiki.jenkins.io/display/JENKINS/Pipeline+Multibranch+Plugin). I use [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) and [Folders Plugin](https://plugins.jenkins.io/cloudbees-folder). I have it create ordinary [pipeline](https://jenkins.io/doc/book/pipeline/)-jobs in a folder structure with a folder for each repository. \\n\\n* It gives me something static, the name of each job, to use when chaining jobs that depend on each other. There is a folder for each repository and it contains the jobs.\\n* It also allows several jobs to work with the same branch. I can easily create a release-job and a snapshot-job that both work with the *develop* branch.\\n\\nYou could have a static release-job and use mutlibranch to dynamically create every other job. But still, I feel I have more control with *Job DSL* and I feel it makes Jenkins look more organized.\\n\\n## Branching strategy\\nYou must know the meaing of the branches, in any given repo, in order to automate things. A defined branching strategy enables you to:\\n\\n * Clone any given repo.\\n * Detect what branches exists.\\n * Be sure which branch to use for *snapshots* or *releases*.\\n\\nIf your strategy is [GitFlow](https://bjurr.com/gitflow-and-when-you-should-use-it/), then:\\n\\n * The *snapshot*-job will\\n   * Build snapshots from *develop*.\\n   * Step dependencies in *develop*.\\n * The *release*-job will\\n   * Build releases from *hotfix* if it exists or else  *release*.\\n * The *feature*-job will build any *feature/X*-branches.\\n\\nEach repository has one release-cycle. Several artifacts, in the same repo, with different release-cycles are not allowed.\\n\\n## Branch to version translation\\n\\nThe integration between the Git service and Jenkins is setup so that when a commit is pushed to a feature-branch:\\n* A job is triggered.\\n* The branch name is identified.\\n* A version is derived form the branch name.\\n* Check to see if there is global a `bom` with that version\\n  * If no `bom` fall back to some default, fail or automate creation of that ´bom´-version.\\n* The artifacts are built, with a `bom` with the version, and uploaded to a Maven repository.\\n\\nThe `bom` -repo may function in the same way. Developers branch out of *develop*, getting all default versions. They set specific versions for some artifacts and commit/push. Or you automate that same procedure whenever a `bom`-version is missing. Then the artifact that originally triggered the creation of that bom can find its version and set it to itself.\\n\\nThen it will be possible to automatically create *deploy*-jobs for any \\\"deployaple\\\" repository. Where a dropdown list can automatically be populated with features. Features are found by listing feature-branches and translating to versions.\\n\\nDevelopers won\'t have to fiddle with versions locally, they just have whatever version that is also in *develop*-branches. And can clone a bunch of repos, built locally, and work with that fitting together.\\n\\n## Building\\n\\nYou must know how to build any given repo. With Maven, the aggreement might be as simple as:\\n\\n* The project is built from the root of the repo.\\n* The version, of the repository, is specified in the root of the repo. When using Maven, it is the version of the *pom.xml*.\\n* Different Maven profiles are allowed. Any profiles that produces artifacts should be specified as metadata about the repo in the *Jenkinsfile*.\\n\\nThe important thing is to define these rules. Do not start treating specific repos differently in the global build scripts. Instead specify global rules that all repos should follow.\\n\\n## Depending repositories\\n\\nTo be able to automatically chain jobs and have them trigger each other, I need to know *depending repos* of each repo. The opposite of what you have in `pom.xml`.\\n\\nOne way of doing that is with a job that:\\n\\n * Regularly finds all repos, perhaps via the Git service REST-API.\\n * Parse the `pom.xml`-files.\\n   * Find out what `artifacts` are contains in what repos.\\n   * Find out what `artifacts` are used in what repos.\\n * Create a structure with the depending repos per repo.\\n * Optimize that structure so that transitive dependencies are removed from list of direct dependencies.\\n * Store that structure as a json text -file in a repo. Making it available for snapshot/release-jobs to clone and include.\\n\\nHaving this information pre-calculated saves alot of time when it is needed by some job.\\n\\nPerhaps the depending repo structure can look something like this.\\n```\\n{\\n ...\\n \\\"PROJECT-A/example-repo-d\\\": [\\n  \\\"PROJECT-B/example-repo-b\\\",\\n  \\\"PROJECT-C/example-repo-d\\\"\\n ],\\n \\\"PROJECT-C/example-repo-d\\\": [\\n  \\\"PROJECT-E/example-repo-b\\\"\\n ]\\n ...\\n}\\n```\\n\\n## Jenkinsfile\\nIt is very small and contains only metadata about the repo. This is just like [Jenkins Infra](https://github.com/jenkins-infra/pipeline-library/blob/master/vars/buildPlugin.groovy) handles their 1000+ plugins.\\n\\nWhen using Maven, you might want to specify *profiles* to be built.\\n\\nA repo that needs to be built, nothing else, may look like this:\\n```\\nbuildRepo()\\n```\\n\\nIt may specify profiles:\\n```\\nbuildRepo(\\n profiles: [\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nAnd if a profiles are needed as well as no profile, it may look like:\\n```\\nbuildRepo(\\n profiles: [\\n  \'\',\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nThis is all there is in the repositories. Only one *Jenkinsfile* and this is the only information it contains. I\'m not saying you only need this. I\'m just recommending to keep it light! Perhaps you invent thnkgs like `deployable: true` or `autoDeployEnv: \'TEST-XY\'`...\\n\\n## Shared library\\nA [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/) allows you to, only once, define how to do releases, snapshots and all other tasks.\\n\\nWith the above *Jenkinsfile* there should be a `/vars/buildRepo.groovy` containing something like:\\n```\\n...\\ndef call(Map params = [:]) {\\n...\\n if (JOB_BASE_NAME.equals(\\\"snapshot\\\")) {\\n ...\\n } else if (JOB_BASE_NAME.equals(\\\"release\\\")) {\\n ...\\n }\\n...\\n}\\n...\\n```\\n\\n## Automate creation of jobs\\nMost Git services (GitHub, GitLab, Bitbucket...) provide REST API:s. You can use that to automate creation/deletion/adjustment of jobs and always be in sync with the repos you have in your Git service. The job DSL would loop through all repositories.\\n\\n```\\n...\\n folder(\\\"gen\\\") {\\n  displayName(\\\"Generated jobs\\\")\\n  description(\\\"\\\"\\\"\\n   These are generated by ${JOB_URL}\\n  \\\"\\\"\\\")\\n }\\n\\n getJson(server+ \\\"/rest/request/to/repos...\\\")\\n  .values\\n  .each { repo ->\\n  folder(\\\"gen\\\") {\\n   displayName(\\\"gen/\\\" + repo.name)\\n   description(\\\"\\\"\\\"\\n    Generated by ${JOB_URL}\\n   \\\"\\\"\\\")\\n  }\\n\\n  pipelineJob(\\\"gen/\\\" + repo.name + \\\"/snapshot\\\") {\\n...\\n```\\n\\n## Templates\\nI use the [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) plugin. Perhaps you want these jobs for every repository:\\n\\n * *snapshot*\\n * *release*\\n * *feature*\\n * *pull-request*\\n\\nAlso a global job, *release-orchestration*.\\n\\nAll of these templates are pipelines. Their logic is implemented in the shared library. The shared library will find the Git repo to use from `scm.getUserRemoteConfigs().get(0).getUrl()` and the kind of job to build from `JOB_BASE_NAME`.\\n\\n### Snapshot\\nThis job will:\\n* Make sure *develop* is using latest dependencies ([found in Maven repository](http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html)). If there are newer versions:\\n  * Step dependencies to latest version.\\n  * Commit changes.\\n  * Push changes.\\n  * Re-trigger self, to help Jenkins understand that this new commit does not need to be built again. Done.\\n* Build a snapshot version.\\n* Upload snapshot-version to Maven repository.\\n* Trigger *dependingRepos* configured in *Jenkinsfile*.\\n* Done.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release\\nThis job will:\\n* Start from commit *C1*.\\n* Set dependencies to latest released versions.\\n* Step version to next release-version.\\n* Make a commit *C2*.\\n* Set dependencies to latest snapshots.\\n* Step version to next snapshot-version.\\n* Make a commit *C3*.\\n* Try to push changes. If not successful:\\n  * Hard reset to *C1*.\\n  * Pull.\\n  * Start again with creating *C2*.\\n  * Do this loop, perhaps 5 times before giving up and fail.\\n\\nThis allows developers to work in the branches during the release-process.\\n\\nNow that we know we are in sync with remote Git repo on where to perform the release, we can continue doing so.\\n\\n* Tag *C2* with the release-version.\\n* Perform the build commands, `mvn package` and loop any profiles needed.\\n* Deploy in Maven repository, `mvn deploy`.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release orchestration\\nThis job will:\\n * Orchestrates a release.\\n * Is parameterized with each repo.\\n * When triggered:\\n   * Calculates the order to release selected repos. With the information found in their *Jenkinsfile*:s.\\n   * Invokes the *release*-jobs of each selected repo.\\n\\n# Features\\nHere is what features this setup can provide and how I intend them to be used.\\n## Release\\n\\nA release of a single repo can be performed from its *release*-job.\\n\\nThis will look at each repo and release from the branch that is first found in this order:\\n\\n1. *hotfix*\\n2. *release*\\n3. *develop*\\n4. *master*\\n\\nSo if you want to release from a specific commit, not latest *develop*, just push a *release*, or *hotfix*, -branch that points to that commit.\\n\\n### Orchestrating a release\\n\\nA release of one, or more, repos can be performed from the global *release-orchestration*-job. This will:\\n\\n* Ensure the release of each repo\\n  * Is done in the right order.\\n  * Their dependencies will be released first, if selected.\\n  * Use the latest release of their dependencies.\\n* After release, trigger the snapshot-job of the first repo that was released. So that all the snapshot-jobs will run and step snapshot-versions.\\n\\nIt will invoke the release-jobs of each repo. This means you can have a look there for more details on that specific release.\\n\\n## Hotfix\\n\\nHaving the priority among branches, mentioned above, will enable you to push a *hotfix*-branch from any commit and have the release being performed from that commit. If your *master* points to latest installed version:\\n\\n* `git checkout master`\\n* `git checkout -b hotfix`\\n* `git push -u origin hotfix`\\n\\nThen just trigger the release.\\n# Advantages\\nYour entire **Jenkins configuration is put under version control**. Well... you need to create one Job DSL -job manually that polls, or is triggered by changes in, the git service. But that job can have its DSL in a Git-repo. This has a bunch of advantages.\\n\\n * No more browsing around in Jenkins and fiddling with settings.\\n * You can track changes in the jobs. Just use `git blame`, it is all code now!\\n * All your jobs are backed up with Git.\\n * You can easily setup a development instance of Jenkins that behaves very much like your production instance.\\n * You can generate release-jobs in one Jenkins and snapshot jobs in another. Letting only a few people use the release-jenkins and anyone use the other instance.\"}]],\"markups\":[],\"sections\":[[10,0]]}',1548092595376,'2019-01-21 17:43:15'),('5c4612f527d96a0621fc1763','5a755597c56a61057b219788','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This is a pattern that I find simple, easy/quick while still keeping you in control of your build flow. Should be no problem applying it in an organization with a huge amount of repositories!\\n\\nI am exclusively involved in **Java projects**. This post will sometimes assume *Maven* is being used. But it can probably inspire a solution in other projects too. I will not supply the complete running code. I\'m just going to include small snippets and focus on explaining the general idea.\\n\\nSome of this is implemented with Jenkins Configuration as Code, here: \\nhttps://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\\n\\n# Problems\\nSplitting a big application into several smaller artifacts is a good thing! To me, that is obvious but still I find myself talking to people that don\'t agree on that. Here are some of the arguments I hear on why not to split applications.\\n\\n> \\\"I want to build whatever I have checked out, locally, on my filesystem. We will need to spend many frustrating hours stepping dependencies between artifacts.\\\"\\n\\n> \\\"It is hard to detect when artifacts no longer fit together. We will find severe problems late because we don\'t continuously integrate all the artifacts.\\\"\\n\\n> \\\"We will need a huge amount of jobs in Jenkins (releasing, testing, integrating, deploying, snapshots...). We will need to spend much time managing them.\\\"\\n\\nOk! All valid points! And **all of them are more or less show stoppers if you don\'t do continuous integration right!**\\n\\n# Solution\\nIn short, I propose a solution where you:\\n\\n * Define a clear branching strategy.\\n * Define a translation strategy between branch and Maven artifact version.\\n * Define how any given repo should be built.\\n * Automate and define how repos depend on each other.\\n * Add a *Jenkinsfile* to each repo.\\n * Create a [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/).\\n * Automate the creation of the jobs.\\n\\nYou might consider [Pipeline Multibranch Plugin](https://wiki.jenkins.io/display/JENKINS/Pipeline+Multibranch+Plugin). I use [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) and [Folders Plugin](https://plugins.jenkins.io/cloudbees-folder). I have it create ordinary [pipeline](https://jenkins.io/doc/book/pipeline/)-jobs in a folder structure with a folder for each repository. \\n\\n* It gives me something static, the name of each job, to use when chaining jobs that depend on each other. There is a folder for each repository and it contains the jobs.\\n* It also allows several jobs to work with the same branch. I can easily create a release-job and a snapshot-job that both work with the *develop* branch.\\n\\nYou could have a static release-job and use mutlibranch to dynamically create every other job. But still, I feel I have more control with *Job DSL* and I feel it makes Jenkins look more organized.\\n\\n## Branching strategy\\nYou must know the meaing of the branches, in any given repo, in order to automate things. A defined branching strategy enables you to:\\n\\n * Clone any given repo.\\n * Detect what branches exists.\\n * Be sure which branch to use for *snapshots* or *releases*.\\n\\nIf your strategy is [GitFlow](https://bjurr.com/gitflow-and-when-you-should-use-it/), then:\\n\\n * The *snapshot*-job will\\n   * Build snapshots from *develop*.\\n   * Step dependencies in *develop*.\\n * The *release*-job will\\n   * Build releases from *hotfix* if it exists or else  *release*.\\n * The *feature*-job will build any *feature/X*-branches.\\n\\nEach repository has one release-cycle. Several artifacts, in the same repo, with different release-cycles are not allowed.\\n\\n## Branch to version translation\\n\\nThe integration between the Git service and Jenkins is setup so that when a commit is pushed to a feature-branch:\\n* A job is triggered.\\n* The branch name is identified.\\n* A version is derived form the branch name.\\n* Check to see if there is global a `bom` with that version\\n  * If no `bom` fall back to some default, fail or automate creation of that ´bom´-version.\\n* The artifacts are built, with a `bom` with the version, and uploaded to a Maven repository.\\n\\nThe `bom` -repo may function in the same way. Developers branch out of *develop*, getting all default versions. They set specific versions for some artifacts and commit/push. Or you automate that same procedure whenever a `bom`-version is missing. Then the artifact that originally triggered the creation of that bom can find its version and set it to itself.\\n\\nThen it will be possible to automatically create *deploy*-jobs for any \\\"deployaple\\\" repository. Where a dropdown list can automatically be populated with features. Features are found by listing feature-branches and translating to versions.\\n\\nDevelopers won\'t have to fiddle with versions locally, they just have whatever version that is also in *develop*-branches. And can clone a bunch of repos, built locally, and work with that fitting together.\\n\\n## Building\\n\\nYou must know how to build any given repo. With Maven, the aggreement might be as simple as:\\n\\n* The project is built from the root of the repo.\\n* The version, of the repository, is specified in the root of the repo. When using Maven, it is the version of the *pom.xml*.\\n* Different Maven profiles are allowed. Any profiles that produces artifacts should be specified as metadata about the repo in the *Jenkinsfile*.\\n\\nThe important thing is to define these rules. Do not start treating specific repos differently in the global build scripts. Instead specify global rules that all repos should follow.\\n\\n## Depending repositories\\n\\nTo be able to automatically chain jobs and have them trigger each other, I need to know *depending repos* of each repo. The opposite of what you have in `pom.xml`.\\n\\nOne way of doing that is with a job that:\\n\\n * Regularly finds all repos, perhaps via the Git service REST-API.\\n * Parse the `pom.xml`-files.\\n   * Find out what `artifacts` are contains in what repos.\\n   * Find out what `artifacts` are used in what repos.\\n * Create a structure with the depending repos per repo.\\n * Optimize that structure so that transitive dependencies are removed from list of direct dependencies.\\n * Store that structure as a json text -file in a repo. Making it available for snapshot/release-jobs to clone and include.\\n\\nHaving this information pre-calculated saves alot of time when it is needed by some job.\\n\\nPerhaps the depending repo structure can look something like this.\\n```\\n{\\n ...\\n \\\"PROJECT-A/example-repo-d\\\": [\\n  \\\"PROJECT-B/example-repo-b\\\",\\n  \\\"PROJECT-C/example-repo-d\\\"\\n ],\\n \\\"PROJECT-C/example-repo-d\\\": [\\n  \\\"PROJECT-E/example-repo-b\\\"\\n ]\\n ...\\n}\\n```\\n\\n## Jenkinsfile\\nIt is very small and contains only metadata about the repo. This is just like [Jenkins Infra](https://github.com/jenkins-infra/pipeline-library/blob/master/vars/buildPlugin.groovy) handles their 1000+ plugins.\\n\\nWhen using Maven, you might want to specify *profiles* to be built.\\n\\nA repo that needs to be built, nothing else, may look like this:\\n```\\nbuildRepo()\\n```\\n\\nIt may specify profiles:\\n```\\nbuildRepo(\\n profiles: [\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nAnd if a profiles are needed as well as no profile, it may look like:\\n```\\nbuildRepo(\\n profiles: [\\n  \'\',\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nThis is all there is in the repositories. Only one *Jenkinsfile* and this is the only information it contains. I\'m not saying you only need this. I\'m just recommending to keep it light! Perhaps you invent thnkgs like `deployable: true` or `autoDeployEnv: \'TEST-XY\'`...\\n\\n## Shared library\\nA [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/) allows you to, only once, define how to do releases, snapshots and all other tasks.\\n\\nWith the above *Jenkinsfile* there should be a `/vars/buildRepo.groovy` containing something like:\\n```\\n...\\ndef call(Map params = [:]) {\\n...\\n if (JOB_BASE_NAME.equals(\\\"snapshot\\\")) {\\n ...\\n } else if (JOB_BASE_NAME.equals(\\\"release\\\")) {\\n ...\\n }\\n...\\n}\\n...\\n```\\n\\n## Automate creation of jobs\\nMost Git services (GitHub, GitLab, Bitbucket...) provide REST API:s. You can use that to automate creation/deletion/adjustment of jobs and always be in sync with the repos you have in your Git service. The job DSL would loop through all repositories.\\n\\n```\\n...\\n folder(\\\"gen\\\") {\\n  displayName(\\\"Generated jobs\\\")\\n  description(\\\"\\\"\\\"\\n   These are generated by ${JOB_URL}\\n  \\\"\\\"\\\")\\n }\\n\\n getJson(server+ \\\"/rest/request/to/repos...\\\")\\n  .values\\n  .each { repo ->\\n  folder(\\\"gen\\\") {\\n   displayName(\\\"gen/\\\" + repo.name)\\n   description(\\\"\\\"\\\"\\n    Generated by ${JOB_URL}\\n   \\\"\\\"\\\")\\n  }\\n\\n  pipelineJob(\\\"gen/\\\" + repo.name + \\\"/snapshot\\\") {\\n...\\n```\\n\\n## Templates\\nI use the [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) plugin. Perhaps you want these jobs for every repository:\\n\\n * *snapshot*\\n * *release*\\n * *feature*\\n * *pull-request*\\n\\nAlso a global job, *release-orchestration*.\\n\\nAll of these templates are pipelines. Their logic is implemented in the shared library. The shared library will find the Git repo to use from `scm.getUserRemoteConfigs().get(0).getUrl()` and the kind of job to build from `JOB_BASE_NAME`.\\n\\n### Snapshot\\nThis job will:\\n* Make sure *develop* is using latest dependencies ([found in Maven repository](http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html)). If there are newer versions:\\n  * Step dependencies to latest version.\\n  * Commit changes.\\n  * Push changes.\\n  * Re-trigger self, to help Jenkins understand that this new commit does not need to be built again. Done.\\n* Build a snapshot version.\\n* Upload snapshot-version to Maven repository.\\n* Trigger *dependingRepos* configured in *Jenkinsfile*.\\n* Done.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release\\nThis job will:\\n* Start from commit *C1*.\\n* Set dependencies to latest released versions.\\n* Step version to next release-version.\\n* Make a commit *C2*.\\n* Set dependencies to latest snapshots.\\n* Step version to next snapshot-version.\\n* Make a commit *C3*.\\n* Try to push changes. If not successful:\\n  * Hard reset to *C1*.\\n  * Pull.\\n  * Start again with creating *C2*.\\n  * Do this loop, perhaps 5 times before giving up and fail.\\n\\nThis allows developers to work in the branches during the release-process.\\n\\nNow that we know we are in sync with remote Git repo on where to perform the release, we can continue doing so.\\n\\n* Tag *C2* with the release-version.\\n* Perform the build commands, `mvn package` and loop any profiles needed.\\n* Deploy in Maven repository, `mvn deploy`.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release orchestration\\nThis job will:\\n * Orchestrates a release.\\n * Is parameterized with each repo.\\n * When triggered:\\n   * Calculates the order to release selected repos. With the information found in their *Jenkinsfile*:s.\\n   * Invokes the *release*-jobs of each selected repo.\\n\\n# Features\\nHere is what features this setup can provide and how I intend them to be used.\\n## Release\\n\\nA release of a single repo can be performed from its *release*-job.\\n\\nThis will look at each repo and release from the branch that is first found in this order:\\n\\n1. *hotfix*\\n2. *release*\\n3. *develop*\\n4. *master*\\n\\nSo if you want to release from a specific commit, not latest *develop*, just push a *release*, or *hotfix*, -branch that points to that commit.\\n\\n### Orchestrating a release\\n\\nA release of one, or more, repos can be performed from the global *release-orchestration*-job. This will:\\n\\n* Ensure the release of each repo\\n  * Is done in the right order.\\n  * Their dependencies will be released first, if selected.\\n  * Use the latest release of their dependencies.\\n* After release, trigger the snapshot-job of the first repo that was released. So that all the snapshot-jobs will run and step snapshot-versions.\\n\\nIt will invoke the release-jobs of each repo. This means you can have a look there for more details on that specific release.\\n\\n## Hotfix\\n\\nHaving the priority among branches, mentioned above, will enable you to push a *hotfix*-branch from any commit and have the release being performed from that commit. If your *master* points to latest installed version:\\n\\n* `git checkout master`\\n* `git checkout -b hotfix`\\n* `git push -u origin hotfix`\\n\\nThen just trigger the release.\\n# Advantages\\nYour entire **Jenkins configuration is put under version control**. Well... you need to create one Job DSL -job manually that polls, or is triggered by changes in, the git service. But that job can have its DSL in a Git-repo. This has a bunch of advantages.\\n\\n * No more browsing around in Jenkins and fiddling with settings.\\n * You can track changes in the jobs. Just use `git blame`, it is all code now!\\n * All your jobs are backed up with Git.\\n * You can easily setup a development instance of Jenkins that behaves very much like your production instance.\\n * You can generate release-jobs in one Jenkins and snapshot jobs in another. Letting only a few people use the release-jenkins and anyone use the other instance.\"}]],\"markups\":[],\"sections\":[[10,0]]}',1548096245546,'2019-01-21 18:44:05'),('5c5e8f81d32e6b08b8dfd525','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrate with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the automatically created jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[]],[10,0],[10,1],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"p\",[]]]}',1549700993770,'2019-02-09 08:29:53'),('5c5e957fd32e6b08b8dfd526','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrate with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the automatically created jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\". The solution encourages innovation.\"]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"p\",[]]]}',1549702527958,'2019-02-09 08:55:27'),('5c5e95d2d32e6b08b8dfd527','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrate with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the automatically created jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\". The solution encourages innovation.\"]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"p\",[]]]}',1549702610918,'2019-02-09 08:56:50'),('5c5e9b13d32e6b08b8dfd528','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrate with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the automatically created jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}',1549703955128,'2019-02-09 09:19:15'),('5c5e9eb7d32e6b08b8dfd529','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrated with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that \"],[0,[0],1,\"J\"],[0,[],0,\"CasC is released we have all parts need to do this. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}',1549704887675,'2019-02-09 09:34:47'),('5c5eaad1d32e6b08b8dfd52a','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrated with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that \"],[0,[0],1,\"JCasC\"],[0,[],0,\" is released we have all parts need. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}',1549707985462,'2019-02-09 10:26:25'),('5c5eaafed32e6b08b8dfd52b','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrated with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that \"],[0,[0],1,\"JCasC\"],[0,[],0,\" is released we have all parts need. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project has their own installation of Jenkins. They can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}',1549708029998,'2019-02-09 10:27:10'),('5c5eab70d32e6b08b8dfd52c','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrated with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that \"],[0,[0],1,\"JCasC\"],[0,[],0,\" is released we have all parts need. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project have their own installation of Jenkins. They can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}',1549708144421,'2019-02-09 10:29:04'),('5c5efce6d0921a0675804957','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}],[\"embed\",{\"url\":\"https://youtu.be/mj90dy-A1mQ\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/mj90dy-A1mQ?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\",\"caption\":\"Video showing this setup in Jenkins\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrated with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that \"],[0,[0],1,\"JCasC\"],[0,[],0,\" is released we have all parts need. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project have their own installation of Jenkins. They can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[10,2],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}',1549728998301,'2019-02-09 16:16:38'),('5c5fca486219ca08a904675f','5c5e7a20d32e6b08b8dfd508','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}],[\"embed\",{\"url\":\"https://youtu.be/3R39J-6SjV8\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/3R39J-6SjV8?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrated with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that \"],[0,[0],1,\"JCasC\"],[0,[],0,\" is released we have all parts need. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project have their own installation of Jenkins. They can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[10,2],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}',1549781575966,'2019-02-10 06:52:56'),('5cc35015bbdcf706bbdbc751','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}',1556303893916,'2019-04-26 18:38:13'),('5cc3508dbbdcf706bbdbc75b','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"My use case was to create a module structure for a Java project where, among other artifacts, an API (with JAX-RS annotated resources) and a API-MOCK artifact was produced. I used this to produce the stub-json and to package Wiremock-standalone together with this JSON into a self contained JAR (What I call API-MOCK). This jar can then be run from command line to get a reliable (type safe) mock server to use for other development, like GUI. So something like this will give me a reliable server responding with correct headers and datastructures: java -jar mymodule-api-mock.jar I often see copy-pasted hard coded json-files in frontend projects. That are used for frontend development. Very likely to not match the true responses from the actual API.\"]]]]}',1556304013439,'2019-04-26 18:40:13'),('5cc350a0bbdcf706bbdbc75c','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"My use case was to create a module structure for a Java project where, among other artifacts, an API (with JAX-RS annotated resources) and a API-MOCK artifact was produced.\"]]],[1,\"p\",[[0,[],0,\"I used this to produce the stub-json and to package Wiremock-standalone together with this JSON into a self contained JAR (What I call API-MOCK). This jar can then be run from command line to get a reliable (type safe) mock server to use for other development, like GUI. So something like this will give me a reliable server responding with correct headers and datastructures: java -jar mymodule-api-mock.jar I often see copy-pasted hard coded json-files in frontend projects. That are used for frontend development. Very likely to not match the true responses from the actual API.\"]]]]}',1556304032064,'2019-04-26 18:40:32'),('5cc350adbbdcf706bbdbc75d','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"My use case was to create a module structure for a Java project where, among other artifacts, an API (with JAX-RS annotated resources) and a API-MOCK artifact was produced.\"]]],[1,\"p\",[[0,[],0,\"I used this to produce the stub-json and to package Wiremock-standalone together with this JSON into a self contained JAR (What I call API-MOCK). This jar can then be run from command line to get a reliable (type safe) mock server to use for other development, like GUI.\"]]],[1,\"p\",[[0,[],0,\"So something like this will give me a reliable server responding with correct headers and datastructures: java -jar mymodule-api-mock.jar I often see copy-pasted hard coded json-files in frontend projects. That are used for frontend development. Very likely to not match the true responses from the actual API.\"]]]]}',1556304045254,'2019-04-26 18:40:45'),('5cc350b4bbdcf706bbdbc75e','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"My use case was to create a module structure for a Java project where, among other artifacts, an API (with JAX-RS annotated resources) and a API-MOCK artifact was produced.\"]]],[1,\"p\",[[0,[],0,\"I used this to produce the stub-json and to package Wiremock-standalone together with this JSON into a self contained JAR (What I call API-MOCK). This jar can then be run from command line to get a reliable (type safe) mock server to use for other development, like GUI.\"]]],[1,\"p\",[[0,[],0,\"So something like this will give me a reliable server responding with correct headers and datastructures: java -jar mymodule-api-mock.jar\"]]],[1,\"p\",[[0,[],0,\"I often see copy-pasted hard coded json-files in frontend projects. That are used for frontend development. Very likely to not match the true responses from the actual API.\"]]]]}',1556304052723,'2019-04-26 18:40:52'),('5cc357d0bbdcf706bbdbc75f','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```java\\n@Path(\\\"/\\\")\\npublic interface ExampleResource {\\n  @GET\\n  @Path(\\\"/get\\\")\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public ItemDTO getItem(@QueryParam(\\\"filter1\\\") String filter1);\\n\\n  @POST\\n  @Path(\\\"/create\\\")\\n  @Consumes(MediaType.APPLICATION_JSON)\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public void createItem(ItemDTO item);\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```java\\nstubFor(\\n   invocation(ExampleResource.class, (r) -> r.getItem(\\\"abc\\\")) //\\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\\n\\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.getItem(itemDto.getAttr1())) \\n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\\n\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.createItem(itemDto)) //\\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\\n}\\n```\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs\"]],[\"a\",[\"href\",\"http://wiremock.org/docs/stubbing/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I often see \"],[0,[0],1,\"mock\"],[0,[],0,\", or \"],[0,[0],1,\"stub\"],[0,[],0,\", servers being configured in frontend projects. I see hard coded \"],[0,[0],1,\"JSON\"],[0,[],0,\" files that are very likely to not match the true responses of the actual API. Here is how I automate creation of a type safe, reliable, stub server in a runnable, self contained, JAR. Packaged whenever the API is packaged.\"]]],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A fully working example is implemented here, with Maven:\"],[1,[],0,0],[0,[1],1,\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]]],[1,\"p\",[[0,[],0,\"I have a multi module Maven setup with these modules:\"]]],[3,\"ul\",[[[0,[2],1,\"module\"],[0,[],0,\" - Parent pom.\"]],[[0,[2],1,\"module-api\"],[0,[],0,\" - JAX-RS annotated API. Very slim API jar.\"]],[[0,[2],1,\"module-api-mock\"],[0,[],0,\" - Runnable mock-server based on \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[0],1,\"module-api-mock\"],[0,[],0,\" I create a test case that produces the configuration of the stub. I use a test case just because that is an easy way to get some code to run during the build.\"]]],[1,\"p\",[[0,[],0,\"I use \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\" to automatically configure the stub. By looking at how may JAX-RS resource is annotated.\"]]],[1,\"p\",[[0,[],0,\"My resource looks something like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"And I configure the stub something like this:\"]]],[10,1],[1,\"p\",[[0,[],0,\"This will make \"],[0,[4],0,\"Wiremock produce \"],[0,[0],2,\"JSON\"],[0,[],0,\". I then package Wiremock-standalone together with this \"],[0,[0],1,\"JSON\"],[0,[],0,\" into a self contained \"],[0,[0],1,\"JAR\"],[0,[],0,\". This \"],[0,[0],1,\"JAR\"],[0,[],0,\" can then be run from command line to get a reliable (type safe) mock server to use for other development, like frontend.\"]]],[1,\"p\",[[0,[],0,\"Something like this will give me a reliable server responding with correct headers and datastructures:\"]]],[1,\"p\",[[0,[0],1,\"java -jar mymodule-api-mock.jar\"]]],[1,\"h1\",[[0,[],0,\"Advantages\"]]],[1,\"p\",[[0,[],0,\"The stubs are created with the same language, Java, as the service is created in. Easier if you are a Java developer. While still the result is \"],[0,[0],1,\"JSON\"],[0,[],0,\", easy for frontend developers to understand.\"]]],[1,\"p\",[[0,[],0,\"If the \"],[0,[0],1,\"API\"],[0,[],0,\" changes, the stub will automatically change. The stub will always match the \"],[0,[0],1,\"API\"],[0,[],0,\". Same types and same headers (accept, content type).\"]]],[1,\"h1\",[[0,[],0,\"What about Mockito?\"]]],[1,\"p\",[[0,[],0,\"You may also do this instead of something like Mockito to in the test cases.\"]]],[3,\"ul\",[[[0,[],0,\"Mockito will not test that datastructures serialize and deserialize.\"]],[[0,[],0,\"Will not test that you are using annotations in a \\\"sane\\\" way.\"]]]],[1,\"p\",[]]]}',1556305872081,'2019-04-26 19:11:12'),('5cc3dd6deca08e06d016a0b8','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```java\\n@Path(\\\"/\\\")\\npublic interface ExampleResource {\\n  @GET\\n  @Path(\\\"/get\\\")\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public ItemDTO getItem(@QueryParam(\\\"filter1\\\") String filter1);\\n\\n  @POST\\n  @Path(\\\"/create\\\")\\n  @Consumes(MediaType.APPLICATION_JSON)\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public void createItem(ItemDTO item);\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```java\\nstubFor(\\n   invocation(ExampleResource.class, (r) -> r.getItem(\\\"abc\\\")) //\\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\\n\\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.getItem(itemDto.getAttr1())) \\n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\\n\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.createItem(itemDto)) //\\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\\n}\\n```\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs\"]],[\"a\",[\"href\",\"http://wiremock.org/docs/stubbing/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I often see \"],[0,[0],1,\"mock\"],[0,[],0,\", or \"],[0,[0],1,\"stub\"],[0,[],0,\", servers being configured in frontend projects. I see hard coded \"],[0,[0],1,\"JSON\"],[0,[],0,\" files that are very likely to not match the true responses of the actual API. Here is how I automate creation of a type safe, reliable, stub server in a runnable, self contained, \"],[0,[0],1,\"JAR\"],[0,[],0,\". Packaged whenever the \"],[0,[0],1,\"API\"],[0,[],0,\" is packaged.\"]]],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A fully working example is implemented here, with Maven:\"],[1,[],0,0],[0,[1],1,\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]]],[1,\"p\",[[0,[],0,\"I have a multi module Maven setup with these modules:\"]]],[3,\"ul\",[[[0,[2],1,\"module\"],[0,[],0,\" - Parent pom.\"]],[[0,[2],1,\"module-api\"],[0,[],0,\" - JAX-RS annotated API. Very slim API jar.\"]],[[0,[2],1,\"module-api-mock\"],[0,[],0,\" - Runnable mock-server based on \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[0],1,\"module-api-mock\"],[0,[],0,\" I create a test case that produces the configuration of the stub. I use a test case just because that is an easy way to get some code to run during the build.\"]]],[1,\"p\",[[0,[],0,\"I use \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\" to automatically configure the stub. By looking at how may \"],[0,[0],1,\"JAX-RS\"],[0,[],0,\" resource is annotated.\"]]],[1,\"p\",[[0,[],0,\"My resource looks something like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"And I configure the stub something like this:\"]]],[10,1],[1,\"p\",[[0,[],0,\"In this example the `invocation(...)` comes from \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"This will make \"],[0,[4],0,\"Wiremock produce \"],[0,[0],2,\"JSON\"],[0,[],0,\". I then package Wiremock-standalone together with this \"],[0,[0],1,\"JSON\"],[0,[],0,\" into a self contained \"],[0,[0],1,\"JAR\"],[0,[],0,\". This \"],[0,[0],1,\"JAR\"],[0,[],0,\" can then be run from command line to get a reliable (type safe) mock server to use for other development, like frontend.\"]]],[1,\"p\",[[0,[],0,\"Something like this will give me a reliable server responding with correct headers and datastructures:\"]]],[1,\"p\",[[0,[0],1,\"java -jar mymodule-api-mock.jar\"]]],[1,\"h1\",[[0,[],0,\"Advantages\"]]],[1,\"p\",[[0,[],0,\"The stubs are created with the same language, Java, as the service is created in. Convenient if you are a Java developer. While still the result is \"],[0,[0],1,\"JSON\"],[0,[],0,\", convenient for frontend developers.\"]]],[1,\"p\",[[0,[],0,\"If the \"],[0,[0],1,\"API\"],[0,[],0,\" changes, the stub will automatically change. The stub will always match the \"],[0,[0],1,\"API\"],[0,[],0,\". Same types and same headers (accept, content type).\"]]],[1,\"h1\",[[0,[],0,\"What about Mockito?\"]]],[1,\"p\",[[0,[],0,\"You may also do this instead of something like Mockito to in the test cases.\"]]],[3,\"ul\",[[[0,[],0,\"Mockito will not test that datastructures serialize and deserialize.\"]],[[0,[],0,\"Will not test that you are using annotations in a \\\"sane\\\" way.\"]]]],[1,\"p\",[]]]}',1556340077352,'2019-04-27 04:41:17'),('5cc3ddd3eca08e06d016a0b9','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```java\\n@Path(\\\"/\\\")\\npublic interface ExampleResource {\\n  @GET\\n  @Path(\\\"/get\\\")\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public ItemDTO getItem(@QueryParam(\\\"filter1\\\") String filter1);\\n\\n  @POST\\n  @Path(\\\"/create\\\")\\n  @Consumes(MediaType.APPLICATION_JSON)\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public void createItem(ItemDTO item);\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```java\\nstubFor(\\n   invocation(ExampleResource.class, (r) -> r.getItem(\\\"abc\\\")) //\\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\\n\\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.getItem(itemDto.getAttr1())) \\n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\\n\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.createItem(itemDto)) //\\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\\n}\\n```\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs\"]],[\"a\",[\"href\",\"http://wiremock.org/docs/stubbing/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I often see \"],[0,[0],1,\"mock\"],[0,[],0,\", or \"],[0,[0],1,\"stub\"],[0,[],0,\", servers being configured in frontend projects. I see hard coded \"],[0,[0],1,\"JSON\"],[0,[],0,\" files that are very likely to not match the true responses of the actual API. Here is how I automate creation of a type safe, reliable, stub server in a runnable, self contained, \"],[0,[0],1,\"JAR\"],[0,[],0,\". Packaged whenever the \"],[0,[0],1,\"API\"],[0,[],0,\" is packaged.\"]]],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A fully working example is implemented here, with Maven:\"],[1,[],0,0],[0,[1],1,\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]]],[1,\"p\",[[0,[],0,\"I have a multi module Maven setup with these modules:\"]]],[3,\"ul\",[[[0,[2],1,\"module\"],[0,[],0,\" - Parent pom.\"]],[[0,[2],1,\"module-api\"],[0,[],0,\" - JAX-RS annotated API. Very slim API jar.\"]],[[0,[2],1,\"module-api-mock\"],[0,[],0,\" - Runnable mock-server based on \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[0],1,\"module-api-mock\"],[0,[],0,\" I create a test case that produces the configuration of the stub. I use a test case just because that is an easy way to get some code to run during the build.\"]]],[1,\"p\",[[0,[],0,\"I use \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\" to automatically configure the stub. By looking at how may \"],[0,[0],1,\"JAX-RS\"],[0,[],0,\" resource is annotated.\"]]],[1,\"p\",[[0,[],0,\"My resource looks something like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"And I configure the stub something like this:\"]]],[10,1],[1,\"p\",[[0,[],0,\"In this example the invocation(...) comes from \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"This will make \"],[0,[4],0,\"Wiremock produce \"],[0,[0],2,\"JSON\"],[0,[],0,\". I then package Wiremock-standalone together with this \"],[0,[0],1,\"JSON\"],[0,[],0,\" into a self contained \"],[0,[0],1,\"JAR\"],[0,[],0,\". This \"],[0,[0],1,\"JAR\"],[0,[],0,\" can then be run from command line to get a reliable (type safe) mock server to use for other development, like frontend.\"]]],[1,\"p\",[[0,[],0,\"Something like this will give me a reliable server responding with correct headers and datastructures:\"]]],[1,\"p\",[[0,[0],1,\"java -jar mymodule-api-mock.jar\"]]],[1,\"h1\",[[0,[],0,\"Advantages\"]]],[1,\"p\",[[0,[],0,\"The stubs are created with the same language, Java, as the service is created in. Convenient if you are a Java developer. While still the result is \"],[0,[0],1,\"JSON\"],[0,[],0,\", convenient for frontend developers.\"]]],[1,\"p\",[[0,[],0,\"If the \"],[0,[0],1,\"API\"],[0,[],0,\" changes, the stub will automatically change. The stub will always match the \"],[0,[0],1,\"API\"],[0,[],0,\". Same types and same headers (accept, content type).\"]]],[1,\"h1\",[[0,[],0,\"What about Mockito?\"]]],[1,\"p\",[[0,[],0,\"You may also do this instead of something like Mockito to in the test cases.\"]]],[3,\"ul\",[[[0,[],0,\"Mockito will not test that datastructures serialize and deserialize.\"]],[[0,[],0,\"Will not test that you are using annotations in a \\\"sane\\\" way.\"]]]],[1,\"p\",[]]]}',1556340179635,'2019-04-27 04:42:59'),('5cc3ded8eca08e06d016a0ba','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```java\\n@Path(\\\"/\\\")\\npublic interface ExampleResource {\\n  @GET\\n  @Path(\\\"/get\\\")\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public ItemDTO getItem(@QueryParam(\\\"filter1\\\") String filter1);\\n\\n  @POST\\n  @Path(\\\"/create\\\")\\n  @Consumes(MediaType.APPLICATION_JSON)\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public void createItem(ItemDTO item);\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```java\\nstubFor(\\n   invocation(ExampleResource.class, (r) -> r.getItem(\\\"abc\\\")) //\\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\\n\\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.getItem(itemDto.getAttr1())) \\n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\\n\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.createItem(itemDto)) //\\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\\n}\\n```\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs\"]],[\"a\",[\"href\",\"http://wiremock.org/docs/stubbing/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I often see \"],[0,[0],1,\"mock\"],[0,[],0,\", or \"],[0,[0],1,\"stub\"],[0,[],0,\", servers being configured in frontend projects. I see hard coded \"],[0,[0],1,\"JSON\"],[0,[],0,\" files that are very likely to not match the true responses of the actual API. Here is how I automate creation of a type safe, reliable, stub server in a runnable, self contained, \"],[0,[0],1,\"JAR\"],[0,[],0,\". Packaged whenever the \"],[0,[0],1,\"API\"],[0,[],0,\" is packaged.\"]]],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A fully working example is implemented here, with Maven:\"],[1,[],0,0],[0,[1],1,\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]]],[1,\"p\",[[0,[],0,\"I have a multi module Maven setup with these modules:\"]]],[3,\"ul\",[[[0,[2],1,\"module\"],[0,[],0,\" - Parent pom.\"]],[[0,[2],1,\"module-api\"],[0,[],0,\" - JAX-RS annotated API. Very slim API jar.\"]],[[0,[2],1,\"module-api-mock\"],[0,[],0,\" - Runnable mock-server based on \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[0],1,\"module-api-mock\"],[0,[],0,\" I create a test case that produces the configuration of the stub. I use a test case just because that is an easy way to get some code to run during the build.\"]]],[1,\"p\",[[0,[],0,\"I use \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\" to automatically configure the stub. By looking at how may \"],[0,[0],1,\"JAX-RS\"],[0,[],0,\" resource is annotated.\"]]],[1,\"p\",[[0,[],0,\"My resource looks something like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"And I configure the stub something like this:\"]]],[10,1],[1,\"p\",[[0,[],0,\"In this example the `invocation(...)` comes from \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"This will make \"],[0,[4],0,\"Wiremock produce \"],[0,[0],2,\"JSON\"],[0,[],0,\". I then package Wiremock-standalone together with this \"],[0,[0],1,\"JSON\"],[0,[],0,\" into a self contained \"],[0,[0],1,\"JAR\"],[0,[],0,\". This \"],[0,[0],1,\"JAR\"],[0,[],0,\" can then be run from command line to get a reliable (type safe) mock server to use for other development, like frontend.\"]]],[1,\"p\",[[0,[],0,\"Something like this will give me a reliable server responding with correct headers and datastructures:\"]]],[1,\"p\",[[0,[0],1,\"java -jar mymodule-api-mock.jar\"]]],[1,\"h1\",[[0,[],0,\"Advantages\"]]],[1,\"p\",[[0,[],0,\"The stubs are created with the same language, Java, as the service is created in. Convenient if you are a Java developer. While still the result is \"],[0,[0],1,\"JSON\"],[0,[],0,\", convenient for frontend developers.\"]]],[1,\"p\",[[0,[],0,\"If the \"],[0,[0],1,\"API\"],[0,[],0,\" changes, the stub will automatically change. The stub will always match the \"],[0,[0],1,\"API\"],[0,[],0,\". Same types and same headers (accept, content type).\"]]],[1,\"h1\",[[0,[],0,\"What about Mockito?\"]]],[1,\"p\",[[0,[],0,\"You may also do this instead of something like Mockito to in the test cases.\"]]],[3,\"ul\",[[[0,[],0,\"Mockito will not test that datastructures serialize and deserialize.\"]],[[0,[],0,\"Will not test that you are using annotations in a \\\"sane\\\" way.\"]]]],[1,\"p\",[]]]}',1556340440242,'2019-04-27 04:47:20'),('5cc3df85eca08e06d016a0bb','5cc35015bbdcf706bbdbc74f','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```java\\n@Path(\\\"/\\\")\\npublic interface ExampleResource {\\n  @GET\\n  @Path(\\\"/get\\\")\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public ItemDTO getItem(@QueryParam(\\\"filter1\\\") String filter1);\\n\\n  @POST\\n  @Path(\\\"/create\\\")\\n  @Consumes(MediaType.APPLICATION_JSON)\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public void createItem(ItemDTO item);\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```java\\nstubFor(\\n   invocation(ExampleResource.class, (r) -> r.getItem(\\\"abc\\\")) //\\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\\n\\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.getItem(itemDto.getAttr1())) \\n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\\n\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.createItem(itemDto)) //\\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\\n}\\n```\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs\"]],[\"a\",[\"href\",\"http://wiremock.org/docs/stubbing/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I often see \"],[0,[0],1,\"mock\"],[0,[],0,\", or \"],[0,[0],1,\"stub\"],[0,[],0,\", servers being configured in frontend projects. I see hard coded \"],[0,[0],1,\"JSON\"],[0,[],0,\" files that are very likely to not match the true responses of the actual API. Here is how I automate creation of a type safe, reliable, stub server in a runnable, self contained, \"],[0,[0],1,\"JAR\"],[0,[],0,\". Packaged whenever the \"],[0,[0],1,\"API\"],[0,[],0,\" is packaged.\"]]],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A fully working example is implemented here, with Maven:\"],[1,[],0,0],[0,[1],1,\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]]],[1,\"p\",[[0,[],0,\"I have a multi module Maven setup with these modules:\"]]],[3,\"ul\",[[[0,[2],1,\"module\"],[0,[],0,\" - Parent pom.\"]],[[0,[2],1,\"module-api\"],[0,[],0,\" - JAX-RS annotated API. Very slim API jar.\"]],[[0,[2],1,\"module-api-mock\"],[0,[],0,\" - Runnable mock-server based on \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[0],1,\"module-api-mock\"],[0,[],0,\" I create a test case that produces the configuration of the stub. I use a test case just because that is an easy way to get some code to run during the build.\"]]],[1,\"p\",[[0,[],0,\"I use \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\" to automatically configure the stub. By looking at how may \"],[0,[0],1,\"JAX-RS\"],[0,[],0,\" resource is annotated.\"]]],[1,\"p\",[[0,[],0,\"My resource looks something like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"And I configure the stub something like this:\"]]],[10,1],[1,\"p\",[[0,[],0,\"In this example the \"],[0,[0],1,\"invocation(...)\"],[0,[],0,\" comes from \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"This will make \"],[0,[4],0,\"Wiremock produce \"],[0,[0],2,\"JSON\"],[0,[],0,\". I then package Wiremock-standalone together with this \"],[0,[0],1,\"JSON\"],[0,[],0,\" into a self contained \"],[0,[0],1,\"JAR\"],[0,[],0,\". This \"],[0,[0],1,\"JAR\"],[0,[],0,\" can then be run from command line to get a reliable (type safe) mock server to use for other development, like frontend.\"]]],[1,\"p\",[[0,[],0,\"Something like this will give me a reliable server responding with correct headers and datastructures:\"]]],[1,\"p\",[[0,[0],1,\"java -jar mymodule-api-mock.jar\"]]],[1,\"h1\",[[0,[],0,\"Advantages\"]]],[1,\"p\",[[0,[],0,\"The stubs are created with the same language, Java, as the service is created in. Convenient if you are a Java developer. While still the result is \"],[0,[0],1,\"JSON\"],[0,[],0,\", convenient for frontend developers.\"]]],[1,\"p\",[[0,[],0,\"If the \"],[0,[0],1,\"API\"],[0,[],0,\" changes, the stub will automatically change. The stub will always match the \"],[0,[0],1,\"API\"],[0,[],0,\". Same types and same headers (accept, content type).\"]]],[1,\"h1\",[[0,[],0,\"What about Mockito?\"]]],[1,\"p\",[[0,[],0,\"You may also do this instead of something like Mockito to in the test cases.\"]]],[3,\"ul\",[[[0,[],0,\"Mockito will not test that datastructures serialize and deserialize.\"]],[[0,[],0,\"Will not test that you are using annotations in a \\\"sane\\\" way.\"]]]],[1,\"p\",[]]]}',1556340613729,'2019-04-27 04:50:13'),('5cc3fe49eca08e06d016a0bc','597b4433e521cb4fbd9186a8','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Efter över 7 år som anställd konsult (mjukvaruutvecklare) valde jag nyligen att starta eget aktiebolag. Jag gör precis samma sak nu som jag gjorde som anställd men numera i mitt eget bolag istället. Det här inlägget handlar om hur jag gjorde detta. Googlar man kring att starta eget så hittar man mycket. Men jag saknade någon som beskriver det utifrån mina förutsättningar, det är vad jag försöker göra här, eftersom jag tror att många har samma förutsättningar som mig. **Oh, and sorry if you don\'t speak Swedish =) The topic is about starting your own company in Sweden, you are probably not interested.**\\n\\nSka också säga att allt jag tagit reda på här är genom googling och jag tar inget ansvar för att det jag säger här är korrekt och riktigt! Så var kritisk när du läser vad jag skriver nedan!\\n\\n# Varför Starta Eget?\\n\\nDu kan **skatteplanera**. Om du redan innan året är slut vet [gräns för statlig skatt](http://www.regeringen.se/pressmeddelanden/2017/03/upprakningen-av-skiktgranserna-for-statlig-inkomstskatt-2018-begransas-och-den-sarskilda-inkomstskatten-for-utomlands-bosatta-hojs/) och eventuella avdrag du kommer att göra. Då kan du räkna ut exakt vilken lön du kan betala ut från bolaget för att du som privatperson ska hamna precis på gränsen för statlig skatt.\\n\\nUtöver lönen kan du också göra en **utdelning**. Då skattar du 20% på summan upp till `2.75` gånger föregående års *inkomstbasbeloppet* för 2018 blir det `2.75 * 59300 = 163075`. Det är det som kallas förenklingsregeln. Man kan också räkna fram en gräns, som är hälften av utbetald lön, och använda den om den är mer fördelaktig. [Den här blog-posten](https://www.foretagande.se/sa-beraknas-skatt-pa-utdelning-3-12-reglerna/) beskriver detta bra. Första året kan du eventuellt inte använda förenklingsregeln, de diskuterar det i [den här tråden](https://forum.vismaspcs.se/visma_spcs/topics/utdelning-forsta-aret).\\n\\nJag har [gjort ett excel-ark](https://docs.google.com/spreadsheets/d/1jfQSSOEINNq3OVc5pYG5NeFkvgcDqOEu1HjzpU2RQm0/edit?usp=sharing) för att **optimera skatten**. Skatteverket har även [en sida](https://www.skatteverket.se/privat/sjalvservice/allaetjanster/tjanster/raknautdinskatt.4.18e1b10334ebe8bc80001950.html) där man kan räkna ut sin skatt. Du kan även läsa mer om [skatter och avgifter på Verksamt](https://www.verksamt.se/starta/skatter-och-avgifter/aktiebolag). Se även [den här sidan](http://www.driva-eget.se/kalkyler/lonekalkyl) för att få en uppfattning om hur mycket lön man kan ta ut.\\n\\nDet finns anledning att vara rädd för det faktum att man inte får några pengar alls om man inte har ett uppdrag. Men eget bolag kan du använda en **[periodiseringsfond](https://www.skatteverket.se/foretagochorganisationer/foretagare/enskildnaringsverksamhet/periodiseringsfond.4.361dc8c15312eff6fd2b8f2.html)** där du lägger undan pengar mellan räkenskapsår. Du kan alltså optimera din lön för att slippa statlig skatt, lägga undan pengar i en periodiseringsfond och sedan betala ut dem ett år då du har mindre inkomster. På så vis slipper du både bolagsskatten och statlig inkomstskatt.\\n\\nDu är **säkrare som egen**. Bolaget kommer förmodligen att dra in mer pengar än vad som är lämpligt att direkt batala ut som lön. Får du *500*kr/timme (ett väldigt lågt pris!) så drar du in cirka *80 000*kr under en månad. Då kan du betala *60 000*kr i lön. Men du vill förmodligen bara betala ut *36 575*kr eftersom det är gränsen för statlig skatt. Att betala den lönen kostar cirka *48 000*kr för bolaget och du har alltså *32 000*kr kvar i bolaget. Gör du det varje månad har du *384 000*kr kvar i bolaget när räkenskapsåret är slut. Du vill förmodligen använda en del för att göra en utdelning. Men för att jämföra med att vara anställd så skulle du kunna betala ut samma lön *6* månader in på nästa år, trots att du står helt utan inkomster! För att räkna ut hur mycket lön du kan betala ut kan du alltså räkna såhär `timpenning*(52-semesterveckor)*40/12/1.31` så kanske `500*(52-6)*40/12/1.31 = 58 524`.\\n\\nDu väljer själv om du vill betala in till en **tjänstepension** och i så fall vilken. Jag har tvingats betala till SEB under en längre tid. Det enda jag bryr mig om är deras fondutbud. I SEB hittade jag bara en fond, [en räntefond](http://web.msse.se/SEB/SGF/quicktake.aspx?six=SEB-BDSEKA), som kändes \\\"ok\\\". Det är även en fördel att du kan maximera lönen, göra utdelning och om det efter detta fortfarande finns pengar över kan du använda dem till tjänstepension. Du slipper alltså låsa in pengar i en sådan pension om det inte är så att det är en skattemässig fördel. Med tanke på att du, via utdelning, kan få ut kapital med en skatt på under 40% så kanske du inte vill använda en tjänstepension. Pengarna du får ut via utdelningen är ju helt olåsta och du kan investera hur du vill.\\n\\nDu **slipper ha lön efter ålder**. Ju yngre du är, ju mer tjänar du på att ha eget konsultbolag. Den dumma tanken att man ska börja på en låg lönenivå och sedan öka ett par procent per år mappar inte alls mot [vad kunderna betalar](https://computersweden.idg.se/2.2683/1.717731/java-utvecklare-heta) för dig per timme. Kunderna har oftast ett pris på juniorer och ett på seniorer. Att du är 30, 31 eller 40 år gammal spelar ingen roll för priset. Är du yngre anställd konsult betalar du för dina äldre kollegors lön och förmåner. Det kan mycket väl vara så att du drar in mer pengar än någon som är 20 år äldre. Med eget bolag mer än dubblade jag min lön.\\n\\nDu kan **köpa kontorsmaterial väldigt billigt**. Dator, headset, hörlurar, skärmar... till mindre än halva priset jämfört med om du köper det privat. Då tänker jag inte bara på momsen. Tänk även på hur mycket pengar bolaget behöver betala ut för att, efter skatt och arbetsgivaravgift, kunna sätta in summan på det privata kontot. Såklart måste man tänka på att det man köper ägs av företaget och inte dig privat.\\n\\nSå fort du har möjlighet att betala ut **[skattefritt traktamente](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm)** så ska du göra det. Du slipper tjafs med en arbetsgivare som nekar dig något som är så självklart.\\n\\nDu kan **åka på vilken konferens du vill** och låta bolaget betala. Det finns ju intressanta utvecklarkonferenser nästan överallt. Du behöver bara [hitta en](http://lmgtfy.com/?q=sidney+developer+conference) och åka. Inget tjafs med en arbetsgivare som tycker det är för dyrt. Inga dryga övningar, innan eller på plats, utan bara du som bestämmer vilka föreläsningar du tycker är intressanta och som du går på.\\n\\nJag tycker att de flesta möten är meningslösa och väldigt tråkiga. Det blir klart **färre möten** med eget bolag. Du kan spendera din tid med kunden istället för tråkiga månadsmöten =)\\n\\nDu kan få **mindre administrativt jobb**. Som anställd hade jag kvitton att rapportera varje vecka. Jag köpte tågbiljetter via SJ:s hemsida och fick alltså kvittot som PDF. Jag var tvungen att rapporera dem varje vecka med kostnad och moms. Skriva ut ett papper i samband med tidrapportering. Skriva ut mina kvitton. Häffta fast mina kvitton på pappret från tidrapporteringssystemet. Leta upp kuvert samt frimärke. Leta upp brevlåda och posta kuvert med tidrapporten till konsultbolagets kontor. Som egen lägger jag aldrig mer än 2 timmar totalt under en hel månad på att sköta den fullständiga bokföring inklusive alla kvitton. Allt gör jag digitalt.\\n\\nTill sist, och kanske det absolut bästa argumentet. Jag har **alltid sett mig själv som mitt eget varumärke**. Då är ju inte steget långt till att faktiskt vara det också.\\n\\n# Hur Starta Eget?\\nJag listar här kortfattat vad man behöver göra. Längre ner finns vissa steg mer utförligt beskrivna. Du kan räkna med att det tar 1-2 månader från att du skickar in första ansökan till att allt runt bolaget är klart och du kan börja jobba i det. Skadar inte att titta på [andra listor också](https://www.verksamt.se/alla-e-tjanster/din-checklista/aktiebolag) för att säkra att du inte glömt något.\\n\\nJag valde **aktiebolag** framför enskild firma. Då är bolaget en juridisk person som också har ansvaret. En tydlig gräns mellan bolag och privat ekonomi alltså. Jag kan rekommendera [Verksamts jämförelse av företagsformer](https://www.verksamt.se/fundera/valj-foretagsform).\\n\\n**Skapa ansökan** på [Verksamt](https://www.verksamt.se/). Du kommer behöva en verksamhetsbeskrivning. Min ser ut såhär:\\n> Företaget ska bedriva konsultverksamhet företrädesvis inom IT, samt utveckla mjukvaruprodukter, äga och förvalta värdepapper och utöva därmed förenlig verksamhet.\\n\\nNär det gäller **företagsnamn** så rekommenderar jag att bara ta ditt eget namn. AB eller aktiebolag måste vara med i namnet. Så jag valde *Tomas Bjerre AB*. Risken om du väljer något annat är att Bolagsverket nekar dig för att namnet liknar något som redan finns. Onödigt strul och du kan alltid ändra senare.\\n\\nDu kommer behöva ange en **SNI-kod**. Jag valde *62010*.\\n\\nNär du skickat iväg ansökan till Bolagsverket kommer de invänta ett **bankintyg**. Läs mer om att välja bank nedan. Du behöver kontakta en bank för att skapa ett företagskonto. Banken kommer be dig sätta in 50 000 kr på ett speciellt konto. Banken behöver kunna tala om för Bolagsverket att här finns det 50 000 kr som satts in med avsikt att användas för aktiekapitalet. När du gjort detta ger de dig en e-postadress som du sedan ger till Bolagsverket (via verksamt.se). Sedan frågan Bolagsverket banken om pengarna är där och när banken svarar ja registreras bolaget. Så fort bolaget är skapat så har du sedan ett företagskonto där det från start finns 50 000 kr.\\n\\nJag valde **räkenskapsåret** som *0101 - 1231*. Jag kan dock rekommendera brutet räkenskapsår. Alltså att man väljer ett år som inte är ett kalenderår. Fördelen blir då att du kan välja vilket år du betalar ut lönen på, om du når statlig skatt 2017 kan du vänta till 2018 med att fortsätta betala ut lön. Det är inte helt enkelt att [ändra i efterhand](https://www.skatteverket.se/download/18.3aa8c78a1466c584587d059/1408515815569/42405.pdf) och att ändra det bara för att undvika skatt är inte en godtagbar anledning.\\n\\nNär du startar aktiebolag ska bolaget ha **F-skatt** och du som privatperson har A-skatt. Jag valde **kvartalsmoms**. Som **redovisningsmetod** valde jag [kontantmetoden](https://www.e-conomic.se/bokforingsprogram/ordlista/kontantmetoden) (kallas även bokslutsmetoden). Jag uppfattade den som enklast, se hur man [bokför köpt med kreditkort](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) t.ex.\\n\\nDet är väldigt svårt att uppskatta den **förväntade vinsten** första året. Andra året kommer Skatteverket föreslå en. Ett tips är att ange den lågt. I mitt fall skulle jag först betala in 20 000 kr innan jag hunnit få en enda inkomst i bolaget. Skulle du råka ut för det är det bara att göra en ny preliminär inkomstdeklaration via Skatteverket och ange en lägre förväntad vinst. Då fick jag ner denna siffran till 7 000 kr istället.\\n\\nDet finns många smidiga **bokföringsprogram**. [Redovisningen](https://sv.wikipedia.org/wiki/Redovisning) var nog det jag var mest rädd för i början. Jag märkte tidigt att när jag googlade om hur man bokför olika saker så kom jag ofta in på [Visma:s support forum](https://forum.vismaspcs.se/visma_spcs). Jag har tidigare använt deras tidrapportering, PX, vilket suger något helt otroligt. Ser ut som en sommarjobbare hackade ihop det för 15 år sedan och att man inte rört det sedan dess. Men även om jag var väldigt skeptisk till detta bolag så valde jag ändå <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> för bokföring just för att deras support verkar vara helt exemplarisk. Att direkt bokföra ett kvitto i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> är betydligt enklare än det jag tidigare gjort via tidrapportering som anställd. Du kanske t.ex. vill bokföra [bokföringsprogrammet](https://forum.vismaspcs.se/visma_spcs/topics/8fxz374h0czze?topic-reply-list%5Bsettings%5D%5Bfilter_by%5D=all), [köp av dator och försäkring](https://forum.vismaspcs.se/visma_spcs/topics/7h3lv4csqal8w) eller [köp av tågbiljett](https://forum.vismaspcs.se/visma_spcs/topics/con2lpq5fex2f). De [gör t.o.m. filmer](https://www.youtube.com/watch?v=jGgnxd6uBh4) där de visar vissa flöden. Hoppas jag är trovärdig trots att jag gått med i deras affiliate-program och får betalt för att hänvisa andra dit =)\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">\\n\\n![Visma eEkonomi](/content/images/2017/07/affiliate-2014-vismaeekonomi1_728x90_eeko.gif)\\n</a>\\n\\nDu lär behöva en **mobil** i bolaget. Som jag förstår det är det ok att köpa telefonen på bolaget. Även telefoni och SMS är ok. Datatrafiken däremot kan behöva förmånsbeskattas. Det i kombination med att man slipper många dryga samtal om man istället har ett privat abonnemang, gjorde att jag köpte den privat.\\n\\nDu lär behöva en **ansvarsförsäkring**. Jag valde en [från If](https://www.if.se/web/se/foretag/varaforsakringar/offert/pages/default.aspx?varde=startforsakring) och fick betala 5 294 kr. Har du inte det och du råkar ut för något, säg att någon kräver dig på flera miljoner, så konkursar du bara bolaget och slipper undan. Men med en ansvarsförsäkring så kan du, kanske, göra rätt för dig och betala. På så vis blir du en mer attraktiv konsult att anlita och det står även ofta i kontrakten att du ska ha en försäkring.\\n\\nJag valde att kontakta en **redovisningskonsult** som hjälper mig med ekonomiska frågor samt tar hand om **bokslut** / **årsredovisning** / **deklaration**. Jag tar hand om löpande bokföring samt kvartalsmoms, skatter och lön. Det kostar cirka 6 000 kr per år. En redovisningskonsult kan hjälpa dig med allt möjligt i bolaget. Vissa ger dem fullmakt så de kan sköta all ekonomi. Jag fick tag på honom genom en bekant som tipsade men annars är det enkelt att googla. Har du valt Visma så [kan de hjälpa dig](https://vismaspcs.se/produkter/redovisning/visma-eekonomi-byra). I <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du även skapa en inloggning till din redovisningskonsult.\\n\\n**Revisor** är valfritt om man [omsätter under 3 miljoner](http://www.bolagsverket.se/ff/foretagsformer/aktiebolag/starta/revisor). Det använder jag inte.\\n\\nDet kan också vara bra att ha klart för sig **hur man hittar kunder**. Det finns många rena konsultmäklare som tar en procentsats, ofta mellan 10-20% på din timpenning, om du tar ett uppdrag via dem. De har ofta ramavtal med stora konsultköpare. Är du redan anställd som konsult kanske du i början fortsätter med samma kund fast som underkonsult istället. Se, t.ex.:\\n\\n * [Assistera](http://www.assistera.se/sv)\\n * [Brainville](https://www.brainville.com/)\\n * [EWork](https://www.eworkgroup.com/se/)\\n * [Experis](https://cv.experis.se/)\\n * [Kvadrat](http://www.kvadrat.se/bli-kvadratare/)\\n * [Toptal](https://www.toptal.com/)\\n\\nOch till sist, du kan eventuellt göra **[investeraravdraget](http://www.kompred.se/nyttiga-tips/missa-inte-investeraravdraget/)** i din privata deklaration efter att du startat aktiebolag.\\n\\n## Bank\\n\\nJag frågade först några banker via telefon och mail (ICA, Avanza, Swedbank). Det kändes som att de inte riktigt förstod vad jag menade när jag sa att jag ville ha ett **bankintyg**.\\n\\n### Danskebank\\n\\n**Jag valde Danskebank**. Du fyller bara i [deras formulär](https://danskebank.se/sv-se/Foretag/Smaforetag/Pages/Product-packages.aspx) så ringer de upp. Danskebank sköter allt per telefon och det fungerar mycket bra. De bad inte om någon affärsplan eller budget utan litade på mig direkt. De visste också direkt vad det handlade om och de kändes pålitliga. Eftersom jag inte var kund där sedan tidigare så behövde de skicka lite papper till mig med posten först. Men inom en vecka hade jag intyget! Snabbt, enkelt, smidigt och inget meningslöst krångel.\\n\\nJag valde *Danske Business Plus* för 99 kr/mån. **<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> stödjer även Danskebank** på så vis att man kan klistra in kontoutdraget från banken in i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>, för att automatiskt skapa bankhändelser. Sedan matchar man dem mot leverantörsfakturor eller verifikationer.\\n\\n# Hur Driva Eget?\\n\\nJag har lagt in återkommande händelser i Google Calendar för de olika datumen. Sedan angivit att de ska upprepas månadsvis eller, i vissa fall, årsvis. Jag lägger ungefär 2 timmar varje månad på att sköta företaget. Vad jag gör är:\\n\\n* En gång per månaden\\n  * Importerar transaktioner från skattekontot från Skatteverket.\\n  * Importerar transaktioner från företagskontot från banken.\\n  * Skickar fakturor för månaden.\\n  * Betalar ut lön, till anställde.\\n  * Betalar skatt och avgifter till Skatteverket\\n      * Arbetsgivaravgift.\\n      * Inkomstskatt.\\n      * Debiterad preliminärskatt.\\n* En gång per kvartal.\\n  * Betalar in kvartalsmoms.\\n* En gång per år.\\n  * Bokslut, årsredovisning, deklaration.\\n  * I januari skickar in kontrolluppgit\\n      * Av utbetald lön för föregående år till Skatteverket.\\n      * Av föregående års utdelning (KU31).\\n  * Lämnar K10 i privata deklarationen, om utdelning mottagits under året som deklareras.\\n\\nJag valde **kvartalsmoms**, alltså att jag efter varje kvartal redovisar och betalar in eventuell moms. Mer information om det finns [här]( https://www.verksamt.se/driva/skatter-och-avgifter/moms-redovisa-och-betala). Så fort kvartalet är slut kan man lämna in deklarationen. Man har ungefär 1.5 månad på sig att lämna in den. Även för detta har Visma gjort en film som visar [hur du gör i eEkonomi](https://www.youtube.com/watch?v=gPv_SahMnEw). Som jag förstår det använder man kontot *1630* (eller *2012* om det är enskild firma) just för att [alla händelser mot Skatteverket ska gå via detta konto](https://forum.vismaspcs.se/visma_spcs/topics/maste-man-anvanda-sig-av-skattekontot-1630). Hur det fungerar med att betala, och få tillbaka moms, [beskriver Skatteverkets här](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html). Så kortfattat om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> och kvartalsmoms så ska du vid varje kvartalsslut:\\n\\n * Skapa en momsredovisning för kvartalet via bokföring / momsredovisning.\\n * Ladda ner *eskd*-filen.\\n * Skapa en [momsdeklaration](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms.4.7459477810df5bccdd480006935.html) enligt [Visma:s video](https://www.youtube.com/watch?v=gPv_SahMnEw).\\n * Ladda upp *eskd*-filen hos [Skatteverket](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html).\\n * Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n * När du senare betalat in, eller fått betalt, skapa och bokför bankhändelsen på företagskontot. Det visas i slutet på [den här videon](https://www.youtube.com/watch?v=gPv_SahMnEw). Tänk på att använda *1630* för AB och *2012* för enskild firma.\\n\\nSkatteverket har skickat brev till dig med datum då du ska betala **debiterad preliminärskatt**. Det här är något du betalar varje månad och baserar sig på din förväntade vinst som du angav i din preliminära inkomstdeklaration. Du har fått ett besked från Skatteverket där det står datum och hur mycket du ska betala. Om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du följa deras [film om skattekontot](https://www.youtube.com/watch?v=-7cDnn-NaME).\\n* Betala från företagskontot till företagets skattekonto.\\n* Importerar bankhändelser från företagskontot.\\n* Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n* Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n* När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n* Matcha händelsen mot bokföringsförslaget *Debiterad preliminärskatt*.\\n\\nNär du ska betala ut **Lön** behöver du betala *skatt*, *arbetsgivaravgift* samt själva *lönen*. Både *skatt* och *arbetsgivaravgift* betalas till företagets *skattekonto* så det är 2 betalningar som behöver göras från företagskontot. Om du använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du göra såhär. Finns även en [film här](https://www.youtube.com/watch?v=ig2C9gQg2Eg) och en [bra tråd](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-lon-i-eekonomi) om detta i supportforumet.\\n* Under *lön* i huvudnenyn väljer du *anställda*. Lägg upp den anställde. Du kommer behöva *Skattetabell* att använda. Den anställde kan [logga in](https://www.skatteverket.se/) hos Skatteverket och hittar då skattetabell under \\\"Skatter och deklarationer\\\".\\n* Under *lön* i huvudnenyn väljer du *lönebesked* och skapar ett nytt.\\n* Ange lönen och välj bokför.\\n* Logga in på banken och betala ut lönen, efter skatt, till den anställdes konto.\\n* Importera bankhändelsen till *företagskontot* via *kassa och bankhändelser*.\\n* <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kommer föreslå att du matchar bankhändelsen mot löneutbetalningen, gör det. Nu är du klar med lönen, och nästa moment är att betala skatt och arbetsgivaravgift.\\n\\n**Arbetsgivardeklaration** lämnas enkelt via [Skatteverkets webbsida](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html).\\n\\n* Om du inte har betalat ut någon lön sätter du bara **0** i de obligatoriska fälten och skickar in.\\n* Om du har betalat ut lön, och använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>.\\n  * Gå in under *lön* i huvudmenyn och välj *arbetsgivardeklaration*.\\n  * Skapa ny.\\n  * Välj månaden då lönen betaldes ut.\\n  * Välj *bokför*. \\n  * Nu kan du [exportera en fil](https://www.youtube.com/watch?v=ig2C9gQg2Eg) om du klickar på *åtgärder* för deklarationen.\\n  * [Lämna in arbetsgivardeklarationen](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html) hos Skatteverket genom att importera filen.\\n  * På kvittensen visas ett bankgiro och OCR att använda för att betala in skatten till företagets skattekonto.\\n  * Betala in summan från företagets bankkonto.\\n  * Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n  * När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n  * Matcha händelsen mot bokföringsförslaget arbetsgivardeklaration. Skatteverkat drar arbetsgivardeklartionen i 2 transaktioner, du behöver [manuellt slå ihop dem](https://forum.vismaspcs.se/visma_spcs/topics/hur-matchar-jag-tva-belopp-pa-skattekontot-mot-en-arbetsgivardeklaration) för att kunna använda det här bokföringsföslaget. Men annars är det *1630* som krediteras och *2710* (personalskatt) samt *2731* (arbetsgivaravgift) som debiteras. Jag tycker även [den här sidan](https://www.blinfo.se/foretagskunskap/bokfora-lon__15472) är bra här.\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> har en färdig mall för **fakturor**. Du anger ditt företags bankgiro, och/eller kontonummer, lägger upp kunder och artiklar. Sedan kan du enkelt skapa fakturor som du sedan kan skriva ut, spara som PDF eller maila.\\n\\nOm du **betalar ut traktamente** behöver du kunna bevisa att [resorna ägt rum och varit tjänsteresor](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm). Därför kan det vara klokt att föra anteckningar för resor. I mitt fall åker jag alltid tåg och kommer hänvisa till tågbiljetterna om jag får revision av Skatteverket. Jag skapar exceldokument, med uträkningen, som jag sedan exporterar till PDF och sparar för att motivera traktamentet.\\n\\n![Traktamente](/content/images/2017/11/traktamente-utr.png)\\n\\nMan hör ibland uttrycket **skriva av** och jag tror många missförstår det, kanske även jag =) Men jag tolkar det som att allt man köper för under ett halvt prisbasbelopp (ungefär 22 000 kr) skrivs av direkt, kostar det mer skriver man av det under flera år. Att *skriva av* innebär att man inte tar upp det som en tillgång, en inventarie, i bokföringen. Det är dock inte samma sak som att man får ge bort prylen till sig själv eller någon annan. Men jag är osäker, [och många med mig](https://www.flashback.org/p49423662).\\n\\nAtt bokföra köp gjorda med **kreditkort** är lite speciellt men det har Visma en väldigt [bra artikel](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) om.\\n\\nJag sparar alltid alla **kvitton** direkt på Google Drive. Är det inte digitalt fotar jag av med mobilen, laddar upp bilden och sparar även original-kvittot i en pärm. I bokföringsprogrammet är jag också noga med att **ladda upp bilder på varje verifikation**. [Bokföringslagen](https://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/bokforingslag-19991078_sfs-1999-1078) säger att originalen ska sparas, så digitala ska sparas digitalt och de på papper sparas på papper. Jag tycker även att [den här artikeln](https://vismaspcs.se/ditt-foretagande/byra/de-har-kopiorna-behover-du-inte-skriva-ut) beskriver det bra. Även andra dokument fotar jag av och sparar på Google Drive, som beslut från Skatteverket t.ex. Väldigt smidigt att kunna läsa allt var man än är.  Det är ganska enkelt att få tag på ett rejält brandskyddat kassaskåp ([#1](http://www.clasohlson.com/se/Dokumentsk%C3%A5p-Chubbsafes-Executive-40/Pr342428000) eller [#2](http://www.biltema.se/sv/Hem/Sakerhet-och-larm/Sakerhetsbox/Kassaskap-brandsakert-2000033385/)) så att man kan spara papper korrekt enligt lagstiftningen.\\n\\nPå **bokslutsdagen, sista dagen på räkenskapsåret, måste [obetalda fakturor tas upp i bokföring](https://www.verksamt.se/driva/ekonomi-och-bokforing/lopande-bokforing)** även om du använder kontantmetoden. Det beskriver också Visma [här](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi). Om du har skapat en faktura som ännu inte blivit betald kommer <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> att ge dig följande meddelande när du skapar sista **kvartalsmomsen**.\\n\\n> Enligt Kontantmetoden ska du bokföra dina obetalda kundfordringar och leverantörsskulder vid räkenskapsårets slut och redovisa momsen. Detta kommer nu att göras automatiskt. Du bör kontrollera att alla inbetalningar och utbetalningar på året är avstämda innan du fortsätter. Vill du fortsätta?\\n\\nDet är viktigt att ta en extra titt på de sista transaktionerna innan räkenskapsåret är slut så att det inte dykt upp något i sista sekunden. I mitt fall hade Danskebank skapat en faktura, och dragit pengarna, för bankavgifter sista dagen i december.\\n\\nNär räkenskapsåret är slut ska **kontrolluppgifter** för anställda skickas till Skatteverket. Det här gäller 2017 men är på gång att ändras så att man skickar samma uppgifter i arbetsgivardeklarationen istället. Men i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> finns en funktion under *lön* i huvudmenyn som kan användas om du skapat lönespecar.\\n\\n* Klicka på *Ny kontrolluppgift*.\\n* Fyll i dina uppgifter.\\n* Spara.\\n* Nu kan man ladda ner en *xml-fil* som kan laddas upp hos Skatteverket för att fylla i *KU10*-blanketten.\\n\\nVisma har en väldigt bra [gratis broschyr om bokföring](http://download.vismaspcs.se/pdf/bokf%C3%B6ring-f%C3%B6r-nyb%C3%B6rjare.pdf?utm_source=Eloqua&utm_medium=email&utm_content=SE_SP_SI_Onboarding-eEko-Std-4&utm_campaign=SE_SP_SI_Onboarding-eA-Std-1704).\\n\\n# Övrigt\\n\\nOm du **veckopendlar** som jag så kanske du har en **lägenhet på arbetsorten**. Då kan man tänka sig att man [låter bolaget betala den](http://www.skatter.se/?q=node/2619). Jag blev rekommenderad att ta den privat och betala ut mer lön från bolaget istället. Då räknar jag på detta vid utbetalning av lön så att lön efter avdrag hamnar på gränsen för statlig skatt. Avdraget gör jag alltså i min privata deklaration. Väljer man att låta bolaget direkt betala lägenheten blir det lite krångligare med bokföring och deklaration. Det kan också vara så att Skatteverket ser det som en förmån. Så det här kändes enklast.\\n\\nDet finns bolag som har som **affärsidé att bara ha underkonsulter**. De åker på konferensresor och har gemensam kontorslokal precis som vilket annat bolag som helst. Skillnaden är bara att alla är underkonsulter. Detta bolag hjälper dig att starta ditt eget bolag och hitta kunder. De har ramavtal med stora konsultköpare och kan ordna bra timpriser. Ett sånt bolag är [Kvadrat](http://www.kvadrat.se/).\\n\\nJag valde först att ta över kontraktet jag var på och då gå från anställd konsult till underkonsult. Senare bytte jag uppdrag och då anslöt jag mig till [Kvadrat](http://www.kvadrat.se/).\\n\\nHar du frågor om skatter så är det smidigt att man kan [maila Skatteverket](https://www.skatteverket.se/omoss/kontaktaoss/mejla/).\"}]],\"markups\":[],\"sections\":[[10,0]]}',1556348489008,'2019-04-27 07:01:29'),('5cd9ac7e52156f04cb9dc6fa','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it \"]]]]}',1557769341992,'2019-05-13 17:42:22'),('5cd9ac8452156f04cb9dc6fb','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in a git repository\"]]]]}',1557769348657,'2019-05-13 17:42:28'),('5cd9ac8d52156f04cb9dc6fc','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in a Git repository\"]]]]}',1557769357197,'2019-05-13 17:42:37'),('5cd9ac9252156f04cb9dc6fd','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository\"]]]]}',1557769362096,'2019-05-13 17:42:42'),('5cd9acbb52156f04cb9dc6fe','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.\"]]]]}',1557769403329,'2019-05-13 17:43:23'),('5cd9acc852156f04cb9dc6ff','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.\"]]],[1,\"p\",[]]]}',1557769416903,'2019-05-13 17:43:36'),('5cd9ad4752156f04cb9dc700','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.\"]]],[1,\"h1\",[[0,[],0,\"Example\"]]],[1,\"p\",[[0,[],0,\"I have my example running here:\"]]],[1,\"p\",[[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],[1,\"p\",[]]]}',1557769543860,'2019-05-13 17:45:43'),('5cd9ad5e52156f04cb9dc701','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.\"]]],[1,\"h1\",[[0,[],0,\"Example\"]]],[1,\"p\",[[0,[],0,\"I have my example running here: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"],[1,[],0,6],[0,[],0,\"So that you can checkout, for example, this: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],[1,\"p\",[]]]}',1557769566061,'2019-05-13 17:46:06'),('5cd9ad6a52156f04cb9dc702','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.\"]]],[1,\"h1\",[[0,[],0,\"Example\"]]],[1,\"p\",[[0,[],0,\"I have my example running here: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"],[1,[],0,6],[0,[],0,\"So that you can view dependents of a specific version, for example, this: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],[1,\"p\",[]]]}',1557769578538,'2019-05-13 17:46:18'),('5cd9ad7252156f04cb9dc703','5cd9a5ee52156f04cb9dc69e','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.\"]]],[1,\"h1\",[[0,[],0,\"Example\"]]],[1,\"p\",[[0,[],0,\"I have my example running here: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"],[1,[],0,6],[0,[],0,\"So that you can view dependents of a specific version like this: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],[1,\"p\",[]]]}',1557769586621,'2019-05-13 17:46:26'),('5d59901b5a223c04f054fb22','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A job will printout some&nbsp;\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when a tags is pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases with \"],[0,[0],1,\"only read permission to Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfile in the repositories\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The feature is turned on for any repository by configuring it with a webhook that triggers on push events. In Bitbucket Server, this includes tags.\"]]],[1,\"p\",[[0,[],0,\"TODO: bild BBS\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[]]]}',1566150682979,'2019-08-18 17:51:23'),('5d59901f5a223c04f054fb23','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A job will print out some details\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when a tags is pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases with \"],[0,[0],1,\"only read permission to Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfile in the repositories\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The feature is turned on for any repository by configuring it with a webhook that triggers on push events. In Bitbucket Server, this includes tags.\"]]],[1,\"p\",[[0,[],0,\"TODO: bild BBS\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[]]]}',1566150687780,'2019-08-18 17:51:27'),('5d5990295a223c04f054fb24','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when a tags is pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases with \"],[0,[0],1,\"only read permission to Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfile in the repositories\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The feature is turned on for any repository by configuring it with a webhook that triggers on push events. In Bitbucket Server, this includes tags.\"]]],[1,\"p\",[[0,[],0,\"TODO: bild BBS\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[]]]}',1566150696963,'2019-08-18 17:51:37'),('5d5990305a223c04f054fb25','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when a tags is pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases with \"],[0,[0],1,\"only read permission to Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfile in the repositories\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The feature is turned on for any repository by configuring it with a webhook that triggers on push events. In Bitbucket Server, this includes tags.\"]]],[1,\"p\",[[0,[],0,\"TODO: bild BBS\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[]]]}',1566150704373,'2019-08-18 17:51:44'),('5d5990465a223c04f054fb26','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when a tags is pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases with \"],[0,[0],1,\"only read permission to Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfile in the repositories\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The feature is turned on for any repository by configuring it with a webhook that triggers on push events. In Bitbucket Server, this includes tags.\"]]],[1,\"p\",[[0,[],0,\"TODO: bild BBS\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"TODO: youtube\"]]]]}',1566150726015,'2019-08-18 17:52:06'),('5d59926e5a223c04f054fb27','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-push.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-eventlog.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-response.png\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when a tags is pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases with \"],[0,[0],1,\"only read permission to Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfile in the repositories\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A single pipeline is created in Jenkins, completely generic and can handle any number of Git repositories. The webhook needs to be added to the Git repositories that should use the job.\"]]],[1,\"h2\",[[0,[],0,\"Jenkins\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"h2\",[[0,[],0,\"Bitbucket Server\"]]],[1,\"p\",[[0,[],0,\"The feature is turned on for any repository by configuring it with a webhook that triggers on push events. In Bitbucket Server, this includes tags.\"]]],[10,3],[1,\"p\",[[0,[],0,\"When fiddling with it you may want to check the event log.\"]]],[10,4],[1,\"p\",[[0,[],0,\"It may have some useful info in the response.\"]]],[10,5],[1,\"h1\",[[0,[],0,\"Youtube\"]]],[1,\"p\",[[0,[],0,\"Here is a Youtube clip showing this flow:\"]]],[1,\"p\",[[0,[],0,\"TODO\"]]]]}',1566151278184,'2019-08-18 18:01:18'),('5d599db05a223c04f054fb28','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-push.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-eventlog.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-response.png\"}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=8mrJNkofxq4\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/8mrJNkofxq4?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when a tags is pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases with \"],[0,[0],1,\"only read permission to Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfile in the repositories\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A single pipeline is created in Jenkins, completely generic and can handle any number of Git repositories. The webhook needs to be added to the Git repositories that should use the job.\"]]],[1,\"h2\",[[0,[],0,\"Jenkins\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"h2\",[[0,[],0,\"Bitbucket Server\"]]],[1,\"p\",[[0,[],0,\"The feature is turned on for any repository by configuring it with a webhook that triggers on push events. In Bitbucket Server, this includes tags.\"]]],[10,3],[1,\"p\",[[0,[],0,\"When fiddling with it you may want to check the event log.\"]]],[10,4],[1,\"p\",[[0,[],0,\"It may have some useful info in the response.\"]]],[10,5],[1,\"h1\",[[0,[],0,\"Youtube\"]]],[1,\"p\",[[0,[],0,\"Here is a Youtube clip showing this flow:\"]]],[10,6],[1,\"p\",[]]]}',1566154160499,'2019-08-18 18:49:20'),('5d5acf4ac52f1404cd0d0f5c','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-push.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-eventlog.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-response.png\"}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=8mrJNkofxq4\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/8mrJNkofxq4?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when tags are pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases without \"],[0,[0],1,\"admin permissions in Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]],[[0,[],0,\"Eliminate code duplication, \"],[0,[0],1,\"no Jenkinsfiles in the repositories\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A single pipeline is created in Jenkins, completely generic and can handle any number of Git repositories. The webhook needs to be added to the Git repositories that should use the job.\"]]],[1,\"h2\",[[0,[],0,\"Jenkins\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"The build log may look like this: \"],[0,[5],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],[1,\"h2\",[[0,[],0,\"Bitbucket Server\"]]],[1,\"p\",[[0,[],0,\"This workflow is turned on for any repository by configuring the repository with a webhook that triggers on push events. In Bitbucket Server, this includes tag events.\"]]],[10,3],[1,\"p\",[[0,[],0,\"When fiddling with it you may want to check the event log.\"]]],[10,4],[1,\"p\",[[0,[],0,\"It may have some useful info in the response.\"]]],[10,5],[1,\"h1\",[[0,[],0,\"Youtube\"]]],[1,\"p\",[[0,[],0,\"Here is a Youtube clip showing this flow:\"]]],[10,6],[1,\"p\",[]]]}',1566232394252,'2019-08-19 16:33:14'),('5d5adcd959fbe504bc65e2e3','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-push.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-eventlog.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-response.png\"}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=8mrJNkofxq4\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/8mrJNkofxq4?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when tags are pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases \"],[0,[0],1,\"without admin permissions in Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]],[[0,[],0,\"No Jenkinsfiles in the repositories, \"],[0,[0],1,\"eliminate code duplication\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A single pipeline is created in Jenkins, completely generic and can handle any number of Git repositories. The webhook needs to be added to the Git repositories that should use the job.\"]]],[1,\"h2\",[[0,[],0,\"Jenkins\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"The build log may look like this: \"],[0,[5],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],[1,\"h2\",[[0,[],0,\"Bitbucket Server\"]]],[1,\"p\",[[0,[],0,\"This workflow is turned on for any repository by configuring the repository with a webhook that triggers on push events. In Bitbucket Server, this includes tag events.\"]]],[10,3],[1,\"p\",[[0,[],0,\"When fiddling with it you may want to check the event log.\"]]],[10,4],[1,\"p\",[[0,[],0,\"It may have some useful info in the response.\"]]],[10,5],[1,\"h1\",[[0,[],0,\"Youtube\"]]],[1,\"p\",[[0,[],0,\"Here is a Youtube clip showing this flow:\"]]],[10,6],[1,\"p\",[]]]}',1566235865906,'2019-08-19 17:31:05'),('5d7e68409e1be504b533af9a','5d5988255a223c04f054fad7','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-push.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-eventlog.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-response.png\"}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=8mrJNkofxq4\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/8mrJNkofxq4?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when tags are pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases \"],[0,[0],1,\"without admin permissions in Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]],[[0,[],0,\"No Jenkinsfiles in the repositories, \"],[0,[0],1,\"eliminate code duplication\"],[0,[],0,\".\"]],[[0,[],0,\"Trace whoever made the release by looking at the \"],[0,[0],1,\"Git log\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A single pipeline is created in Jenkins, completely generic and can handle any number of Git repositories. The webhook needs to be added to the Git repositories that should use the job.\"]]],[1,\"h2\",[[0,[],0,\"Jenkins\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"The build log may look like this: \"],[0,[5],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],[1,\"h2\",[[0,[],0,\"Bitbucket Server\"]]],[1,\"p\",[[0,[],0,\"This workflow is turned on for any repository by configuring the repository with a webhook that triggers on push events. In Bitbucket Server, this includes tag events.\"]]],[10,3],[1,\"p\",[[0,[],0,\"When fiddling with it you may want to check the event log.\"]]],[10,4],[1,\"p\",[[0,[],0,\"It may have some useful info in the response.\"]]],[10,5],[1,\"h1\",[[0,[],0,\"Youtube\"]]],[1,\"p\",[[0,[],0,\"Here is a Youtube clip showing this flow:\"]]],[10,6],[1,\"p\",[]]]}',1568565312630,'2019-09-15 16:35:12'),('5e0dbbe7bab3f604b5c1c801','597b4433e521cb4fbd9186a8','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Efter över 7 år som anställd konsult (mjukvaruutvecklare) valde jag nyligen att starta eget aktiebolag. Jag gör precis samma sak nu som jag gjorde som anställd men numera i mitt eget bolag istället. Det här inlägget handlar om hur jag gjorde detta. Googlar man kring att starta eget så hittar man mycket. Men jag saknade någon som beskriver det utifrån mina förutsättningar, det är vad jag försöker göra här, eftersom jag tror att många har samma förutsättningar som mig. **Oh, and sorry if you don\'t speak Swedish =) The topic is about starting your own company in Sweden, you are probably not interested.**\\n\\nSka också säga att allt jag tagit reda på här är genom googling och jag tar inget ansvar för att det jag säger här är korrekt och riktigt! Så var kritisk när du läser vad jag skriver nedan!\\n\\nJag blir ofta kontaktad angående det här blogginlägget, men jag vill gärna hålla diskussionerna öppet här istället: https://www.facebook.com/groups/791240028009906/\\n\\n# Varför Starta Eget?\\n\\nDu kan **skatteplanera**. Om du redan innan året är slut vet [gräns för statlig skatt](http://www.regeringen.se/pressmeddelanden/2017/03/upprakningen-av-skiktgranserna-for-statlig-inkomstskatt-2018-begransas-och-den-sarskilda-inkomstskatten-for-utomlands-bosatta-hojs/) och eventuella avdrag du kommer att göra. Då kan du räkna ut exakt vilken lön du kan betala ut från bolaget för att du som privatperson ska hamna precis på gränsen för statlig skatt.\\n\\nUtöver lönen kan du också göra en **utdelning**. Då skattar du 20% på summan upp till `2.75` gånger föregående års *inkomstbasbeloppet* för 2018 blir det `2.75 * 59300 = 163075`. Det är det som kallas förenklingsregeln. Man kan också räkna fram en gräns, som är hälften av utbetald lön, och använda den om den är mer fördelaktig. [Den här blog-posten](https://www.foretagande.se/sa-beraknas-skatt-pa-utdelning-3-12-reglerna/) beskriver detta bra. Första året kan du eventuellt inte använda förenklingsregeln, de diskuterar det i [den här tråden](https://forum.vismaspcs.se/visma_spcs/topics/utdelning-forsta-aret).\\n\\nJag har [gjort ett excel-ark](https://docs.google.com/spreadsheets/d/1jfQSSOEINNq3OVc5pYG5NeFkvgcDqOEu1HjzpU2RQm0/edit?usp=sharing) för att **optimera skatten**. Skatteverket har även [en sida](https://www.skatteverket.se/privat/sjalvservice/allaetjanster/tjanster/raknautdinskatt.4.18e1b10334ebe8bc80001950.html) där man kan räkna ut sin skatt. Du kan även läsa mer om [skatter och avgifter på Verksamt](https://www.verksamt.se/starta/skatter-och-avgifter/aktiebolag). Se även [den här sidan](http://www.driva-eget.se/kalkyler/lonekalkyl) för att få en uppfattning om hur mycket lön man kan ta ut.\\n\\nDet finns anledning att vara rädd för det faktum att man inte får några pengar alls om man inte har ett uppdrag. Men eget bolag kan du använda en **[periodiseringsfond](https://www.skatteverket.se/foretagochorganisationer/foretagare/enskildnaringsverksamhet/periodiseringsfond.4.361dc8c15312eff6fd2b8f2.html)** där du lägger undan pengar mellan räkenskapsår. Du kan alltså optimera din lön för att slippa statlig skatt, lägga undan pengar i en periodiseringsfond och sedan betala ut dem ett år då du har mindre inkomster. På så vis slipper du både bolagsskatten och statlig inkomstskatt.\\n\\nDu är **säkrare som egen**. Bolaget kommer förmodligen att dra in mer pengar än vad som är lämpligt att direkt batala ut som lön. Får du *500*kr/timme (ett väldigt lågt pris!) så drar du in cirka *80 000*kr under en månad. Då kan du betala *60 000*kr i lön. Men du vill förmodligen bara betala ut *36 575*kr eftersom det är gränsen för statlig skatt. Att betala den lönen kostar cirka *48 000*kr för bolaget och du har alltså *32 000*kr kvar i bolaget. Gör du det varje månad har du *384 000*kr kvar i bolaget när räkenskapsåret är slut. Du vill förmodligen använda en del för att göra en utdelning. Men för att jämföra med att vara anställd så skulle du kunna betala ut samma lön *6* månader in på nästa år, trots att du står helt utan inkomster! För att räkna ut hur mycket lön du kan betala ut kan du alltså räkna såhär `timpenning*(52-semesterveckor)*40/12/1.31` så kanske `500*(52-6)*40/12/1.31 = 58 524`.\\n\\nDu väljer själv om du vill betala in till en **tjänstepension** och i så fall vilken. Jag har tvingats betala till SEB under en längre tid. Det enda jag bryr mig om är deras fondutbud. I SEB hittade jag bara en fond, [en räntefond](http://web.msse.se/SEB/SGF/quicktake.aspx?six=SEB-BDSEKA), som kändes \\\"ok\\\". Det är även en fördel att du kan maximera lönen, göra utdelning och om det efter detta fortfarande finns pengar över kan du använda dem till tjänstepension. Du slipper alltså låsa in pengar i en sådan pension om det inte är så att det är en skattemässig fördel. Med tanke på att du, via utdelning, kan få ut kapital med en skatt på under 40% så kanske du inte vill använda en tjänstepension. Pengarna du får ut via utdelningen är ju helt olåsta och du kan investera hur du vill.\\n\\nDu **slipper ha lön efter ålder**. Ju yngre du är, ju mer tjänar du på att ha eget konsultbolag. Den dumma tanken att man ska börja på en låg lönenivå och sedan öka ett par procent per år mappar inte alls mot [vad kunderna betalar](https://computersweden.idg.se/2.2683/1.717731/java-utvecklare-heta) för dig per timme. Kunderna har oftast ett pris på juniorer och ett på seniorer. Att du är 30, 31 eller 40 år gammal spelar ingen roll för priset. Är du yngre anställd konsult betalar du för dina äldre kollegors lön och förmåner. Det kan mycket väl vara så att du drar in mer pengar än någon som är 20 år äldre. Med eget bolag mer än dubblade jag min lön.\\n\\nDu kan **köpa kontorsmaterial väldigt billigt**. Dator, headset, hörlurar, skärmar... till mindre än halva priset jämfört med om du köper det privat. Då tänker jag inte bara på momsen. Tänk även på hur mycket pengar bolaget behöver betala ut för att, efter skatt och arbetsgivaravgift, kunna sätta in summan på det privata kontot. Såklart måste man tänka på att det man köper ägs av företaget och inte dig privat.\\n\\nSå fort du har möjlighet att betala ut **[skattefritt traktamente](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm)** så ska du göra det. Du slipper tjafs med en arbetsgivare som nekar dig något som är så självklart.\\n\\nDu kan **åka på vilken konferens du vill** och låta bolaget betala. Det finns ju intressanta utvecklarkonferenser nästan överallt. Du behöver bara [hitta en](http://lmgtfy.com/?q=sidney+developer+conference) och åka. Inget tjafs med en arbetsgivare som tycker det är för dyrt. Inga dryga övningar, innan eller på plats, utan bara du som bestämmer vilka föreläsningar du tycker är intressanta och som du går på.\\n\\nJag tycker att de flesta möten är meningslösa och väldigt tråkiga. Det blir klart **färre möten** med eget bolag. Du kan spendera din tid med kunden istället för tråkiga månadsmöten =)\\n\\nDu kan få **mindre administrativt jobb**. Som anställd hade jag kvitton att rapportera varje vecka. Jag köpte tågbiljetter via SJ:s hemsida och fick alltså kvittot som PDF. Jag var tvungen att rapporera dem varje vecka med kostnad och moms. Skriva ut ett papper i samband med tidrapportering. Skriva ut mina kvitton. Häffta fast mina kvitton på pappret från tidrapporteringssystemet. Leta upp kuvert samt frimärke. Leta upp brevlåda och posta kuvert med tidrapporten till konsultbolagets kontor. Som egen lägger jag aldrig mer än 2 timmar totalt under en hel månad på att sköta den fullständiga bokföring inklusive alla kvitton. Allt gör jag digitalt.\\n\\nTill sist, och kanske det absolut bästa argumentet. Jag har **alltid sett mig själv som mitt eget varumärke**. Då är ju inte steget långt till att faktiskt vara det också.\\n\\n# Hur Starta Eget?\\nJag listar här kortfattat vad man behöver göra. Längre ner finns vissa steg mer utförligt beskrivna. Du kan räkna med att det tar 1-2 månader från att du skickar in första ansökan till att allt runt bolaget är klart och du kan börja jobba i det. Skadar inte att titta på [andra listor också](https://www.verksamt.se/alla-e-tjanster/din-checklista/aktiebolag) för att säkra att du inte glömt något.\\n\\nJag valde **aktiebolag** framför enskild firma. Då är bolaget en juridisk person som också har ansvaret. En tydlig gräns mellan bolag och privat ekonomi alltså. Jag kan rekommendera [Verksamts jämförelse av företagsformer](https://www.verksamt.se/fundera/valj-foretagsform).\\n\\n**Skapa ansökan** på [Verksamt](https://www.verksamt.se/). Du kommer behöva en verksamhetsbeskrivning. Min ser ut såhär:\\n> Företaget ska bedriva konsultverksamhet företrädesvis inom IT, samt utveckla mjukvaruprodukter, äga och förvalta värdepapper och utöva därmed förenlig verksamhet.\\n\\nNär det gäller **företagsnamn** så rekommenderar jag att bara ta ditt eget namn. AB eller aktiebolag måste vara med i namnet. Så jag valde *Tomas Bjerre AB*. Risken om du väljer något annat är att Bolagsverket nekar dig för att namnet liknar något som redan finns. Onödigt strul och du kan alltid ändra senare.\\n\\nDu kommer behöva ange en **SNI-kod**. Jag valde *62010*.\\n\\nNär du skickat iväg ansökan till Bolagsverket kommer de invänta ett **bankintyg**. Läs mer om att välja bank nedan. Du behöver kontakta en bank för att skapa ett företagskonto. Banken kommer be dig sätta in 50 000 kr på ett speciellt konto. Banken behöver kunna tala om för Bolagsverket att här finns det 50 000 kr som satts in med avsikt att användas för aktiekapitalet. När du gjort detta ger de dig en e-postadress som du sedan ger till Bolagsverket (via verksamt.se). Sedan frågan Bolagsverket banken om pengarna är där och när banken svarar ja registreras bolaget. Så fort bolaget är skapat så har du sedan ett företagskonto där det från start finns 50 000 kr.\\n\\nJag valde **räkenskapsåret** som *0101 - 1231*. Jag kan dock rekommendera brutet räkenskapsår. Alltså att man väljer ett år som inte är ett kalenderår. Fördelen blir då att du kan välja vilket år du betalar ut lönen på, om du når statlig skatt 2017 kan du vänta till 2018 med att fortsätta betala ut lön. Det är inte helt enkelt att [ändra i efterhand](https://www.skatteverket.se/download/18.3aa8c78a1466c584587d059/1408515815569/42405.pdf) och att ändra det bara för att undvika skatt är inte en godtagbar anledning.\\n\\nNär du startar aktiebolag ska bolaget ha **F-skatt** och du som privatperson har A-skatt. Jag valde **kvartalsmoms**. Som **redovisningsmetod** valde jag [kontantmetoden](https://www.e-conomic.se/bokforingsprogram/ordlista/kontantmetoden) (kallas även bokslutsmetoden). Jag uppfattade den som enklast, se hur man [bokför köpt med kreditkort](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) t.ex.\\n\\nDet är väldigt svårt att uppskatta den **förväntade vinsten** första året. Andra året kommer Skatteverket föreslå en. Ett tips är att ange den lågt. I mitt fall skulle jag först betala in 20 000 kr innan jag hunnit få en enda inkomst i bolaget. Skulle du råka ut för det är det bara att göra en ny preliminär inkomstdeklaration via Skatteverket och ange en lägre förväntad vinst. Då fick jag ner denna siffran till 7 000 kr istället.\\n\\nDet finns många smidiga **bokföringsprogram**. [Redovisningen](https://sv.wikipedia.org/wiki/Redovisning) var nog det jag var mest rädd för i början. Jag märkte tidigt att när jag googlade om hur man bokför olika saker så kom jag ofta in på [Visma:s support forum](https://forum.vismaspcs.se/visma_spcs). Jag har tidigare använt deras tidrapportering, PX, vilket suger något helt otroligt. Ser ut som en sommarjobbare hackade ihop det för 15 år sedan och att man inte rört det sedan dess. Men även om jag var väldigt skeptisk till detta bolag så valde jag ändå <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> för bokföring just för att deras support verkar vara helt exemplarisk. Att direkt bokföra ett kvitto i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> är betydligt enklare än det jag tidigare gjort via tidrapportering som anställd. Du kanske t.ex. vill bokföra [bokföringsprogrammet](https://forum.vismaspcs.se/visma_spcs/topics/8fxz374h0czze?topic-reply-list%5Bsettings%5D%5Bfilter_by%5D=all), [köp av dator och försäkring](https://forum.vismaspcs.se/visma_spcs/topics/7h3lv4csqal8w) eller [köp av tågbiljett](https://forum.vismaspcs.se/visma_spcs/topics/con2lpq5fex2f). De [gör t.o.m. filmer](https://www.youtube.com/watch?v=jGgnxd6uBh4) där de visar vissa flöden. Hoppas jag är trovärdig trots att jag gått med i deras affiliate-program och får betalt för att hänvisa andra dit =)\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">\\n\\n![Visma eEkonomi](/content/images/2017/07/affiliate-2014-vismaeekonomi1_728x90_eeko.gif)\\n</a>\\n\\nDu lär behöva en **mobil** i bolaget. Som jag förstår det är det ok att köpa telefonen på bolaget. Även telefoni och SMS är ok. Datatrafiken däremot kan behöva förmånsbeskattas. Det i kombination med att man slipper många dryga samtal om man istället har ett privat abonnemang, gjorde att jag köpte den privat.\\n\\nDu lär behöva en **ansvarsförsäkring**. Jag valde en [från If](https://www.if.se/web/se/foretag/varaforsakringar/offert/pages/default.aspx?varde=startforsakring) och fick betala 5 294 kr. Har du inte det och du råkar ut för något, säg att någon kräver dig på flera miljoner, så konkursar du bara bolaget och slipper undan. Men med en ansvarsförsäkring så kan du, kanske, göra rätt för dig och betala. På så vis blir du en mer attraktiv konsult att anlita och det står även ofta i kontrakten att du ska ha en försäkring.\\n\\nJag valde att kontakta en **redovisningskonsult** som hjälper mig med ekonomiska frågor samt tar hand om **bokslut** / **årsredovisning** / **deklaration**. Jag tar hand om löpande bokföring samt kvartalsmoms, skatter och lön. Det kostar cirka 6 000 kr per år. En redovisningskonsult kan hjälpa dig med allt möjligt i bolaget. Vissa ger dem fullmakt så de kan sköta all ekonomi. Jag fick tag på honom genom en bekant som tipsade men annars är det enkelt att googla. Har du valt Visma så [kan de hjälpa dig](https://vismaspcs.se/produkter/redovisning/visma-eekonomi-byra). I <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du även skapa en inloggning till din redovisningskonsult.\\n\\n**Revisor** är valfritt om man [omsätter under 3 miljoner](http://www.bolagsverket.se/ff/foretagsformer/aktiebolag/starta/revisor). Det använder jag inte.\\n\\nDet kan också vara bra att ha klart för sig **hur man hittar kunder**. Det finns många rena konsultmäklare som tar en procentsats, ofta mellan 10-20% på din timpenning, om du tar ett uppdrag via dem. De har ofta ramavtal med stora konsultköpare. Är du redan anställd som konsult kanske du i början fortsätter med samma kund fast som underkonsult istället. Se, t.ex.:\\n\\n * [Assistera](http://www.assistera.se/sv)\\n * [Brainville](https://www.brainville.com/)\\n * [EWork](https://www.eworkgroup.com/se/)\\n * [Experis](https://cv.experis.se/)\\n * [Kvadrat](http://www.kvadrat.se/bli-kvadratare/)\\n * [Toptal](https://www.toptal.com/)\\n\\nOch till sist, du kan eventuellt göra **[investeraravdraget](http://www.kompred.se/nyttiga-tips/missa-inte-investeraravdraget/)** i din privata deklaration efter att du startat aktiebolag.\\n\\n## Bank\\n\\nJag frågade först några banker via telefon och mail (ICA, Avanza, Swedbank). Det kändes som att de inte riktigt förstod vad jag menade när jag sa att jag ville ha ett **bankintyg**.\\n\\n### Danskebank\\n\\n**Jag valde Danskebank**. Du fyller bara i [deras formulär](https://danskebank.se/sv-se/Foretag/Smaforetag/Pages/Product-packages.aspx) så ringer de upp. Danskebank sköter allt per telefon och det fungerar mycket bra. De bad inte om någon affärsplan eller budget utan litade på mig direkt. De visste också direkt vad det handlade om och de kändes pålitliga. Eftersom jag inte var kund där sedan tidigare så behövde de skicka lite papper till mig med posten först. Men inom en vecka hade jag intyget! Snabbt, enkelt, smidigt och inget meningslöst krångel.\\n\\nJag valde *Danske Business Plus* för 99 kr/mån. **<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> stödjer även Danskebank** på så vis att man kan klistra in kontoutdraget från banken in i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>, för att automatiskt skapa bankhändelser. Sedan matchar man dem mot leverantörsfakturor eller verifikationer.\\n\\n# Hur Driva Eget?\\n\\nJag har lagt in återkommande händelser i Google Calendar för de olika datumen. Sedan angivit att de ska upprepas månadsvis eller, i vissa fall, årsvis. Jag lägger ungefär 2 timmar varje månad på att sköta företaget. Vad jag gör är:\\n\\n* En gång per månaden\\n  * Importerar transaktioner från skattekontot från Skatteverket.\\n  * Importerar transaktioner från företagskontot från banken.\\n  * Skickar fakturor för månaden.\\n  * Betalar ut lön, till anställde.\\n  * Betalar skatt och avgifter till Skatteverket\\n      * Arbetsgivaravgift.\\n      * Inkomstskatt.\\n      * Debiterad preliminärskatt.\\n* En gång per kvartal.\\n  * Betalar in kvartalsmoms.\\n* En gång per år.\\n  * Bokslut, årsredovisning, deklaration.\\n  * I januari skickar in kontrolluppgit\\n      * Av utbetald lön för föregående år till Skatteverket.\\n      * Av föregående års utdelning (KU31).\\n  * Lämnar K10 i privata deklarationen, om utdelning mottagits under året som deklareras.\\n\\nJag valde **kvartalsmoms**, alltså att jag efter varje kvartal redovisar och betalar in eventuell moms. Mer information om det finns [här]( https://www.verksamt.se/driva/skatter-och-avgifter/moms-redovisa-och-betala). Så fort kvartalet är slut kan man lämna in deklarationen. Man har ungefär 1.5 månad på sig att lämna in den. Även för detta har Visma gjort en film som visar [hur du gör i eEkonomi](https://www.youtube.com/watch?v=gPv_SahMnEw). Som jag förstår det använder man kontot *1630* (eller *2012* om det är enskild firma) just för att [alla händelser mot Skatteverket ska gå via detta konto](https://forum.vismaspcs.se/visma_spcs/topics/maste-man-anvanda-sig-av-skattekontot-1630). Hur det fungerar med att betala, och få tillbaka moms, [beskriver Skatteverkets här](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html). Så kortfattat om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> och kvartalsmoms så ska du vid varje kvartalsslut:\\n\\n * Skapa en momsredovisning för kvartalet via bokföring / momsredovisning.\\n * Ladda ner *eskd*-filen.\\n * Skapa en [momsdeklaration](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms.4.7459477810df5bccdd480006935.html) enligt [Visma:s video](https://www.youtube.com/watch?v=gPv_SahMnEw).\\n * Ladda upp *eskd*-filen hos [Skatteverket](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html).\\n * Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n * När du senare betalat in, eller fått betalt, skapa och bokför bankhändelsen på företagskontot. Det visas i slutet på [den här videon](https://www.youtube.com/watch?v=gPv_SahMnEw). Tänk på att använda *1630* för AB och *2012* för enskild firma.\\n\\nSkatteverket har skickat brev till dig med datum då du ska betala **debiterad preliminärskatt**. Det här är något du betalar varje månad och baserar sig på din förväntade vinst som du angav i din preliminära inkomstdeklaration. Du har fått ett besked från Skatteverket där det står datum och hur mycket du ska betala. Om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du följa deras [film om skattekontot](https://www.youtube.com/watch?v=-7cDnn-NaME).\\n* Betala från företagskontot till företagets skattekonto.\\n* Importerar bankhändelser från företagskontot.\\n* Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n* Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n* När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n* Matcha händelsen mot bokföringsförslaget *Debiterad preliminärskatt*.\\n\\nNär du ska betala ut **Lön** behöver du betala *skatt*, *arbetsgivaravgift* samt själva *lönen*. Både *skatt* och *arbetsgivaravgift* betalas till företagets *skattekonto* så det är 2 betalningar som behöver göras från företagskontot. Om du använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du göra såhär. Finns även en [film här](https://www.youtube.com/watch?v=ig2C9gQg2Eg) och en [bra tråd](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-lon-i-eekonomi) om detta i supportforumet.\\n* Under *lön* i huvudnenyn väljer du *anställda*. Lägg upp den anställde. Du kommer behöva *Skattetabell* att använda. Den anställde kan [logga in](https://www.skatteverket.se/) hos Skatteverket och hittar då skattetabell under \\\"Skatter och deklarationer\\\".\\n* Under *lön* i huvudnenyn väljer du *lönebesked* och skapar ett nytt.\\n* Ange lönen och välj bokför.\\n* Logga in på banken och betala ut lönen, efter skatt, till den anställdes konto.\\n* Importera bankhändelsen till *företagskontot* via *kassa och bankhändelser*.\\n* <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kommer föreslå att du matchar bankhändelsen mot löneutbetalningen, gör det. Nu är du klar med lönen, och nästa moment är att betala skatt och arbetsgivaravgift.\\n\\n**Arbetsgivardeklaration** lämnas enkelt via [Skatteverkets webbsida](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html).\\n\\n* Om du inte har betalat ut någon lön sätter du bara **0** i de obligatoriska fälten och skickar in.\\n* Om du har betalat ut lön, och använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>.\\n  * Gå in under *lön* i huvudmenyn och välj *arbetsgivardeklaration*.\\n  * Skapa ny.\\n  * Välj månaden då lönen betaldes ut.\\n  * Välj *bokför*. \\n  * Nu kan du [exportera en fil](https://www.youtube.com/watch?v=ig2C9gQg2Eg) om du klickar på *åtgärder* för deklarationen.\\n  * [Lämna in arbetsgivardeklarationen](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html) hos Skatteverket genom att importera filen.\\n  * På kvittensen visas ett bankgiro och OCR att använda för att betala in skatten till företagets skattekonto.\\n  * Betala in summan från företagets bankkonto.\\n  * Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n  * När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n  * Matcha händelsen mot bokföringsförslaget arbetsgivardeklaration. Skatteverkat drar arbetsgivardeklartionen i 2 transaktioner, du behöver [manuellt slå ihop dem](https://forum.vismaspcs.se/visma_spcs/topics/hur-matchar-jag-tva-belopp-pa-skattekontot-mot-en-arbetsgivardeklaration) för att kunna använda det här bokföringsföslaget. Men annars är det *1630* som krediteras och *2710* (personalskatt) samt *2731* (arbetsgivaravgift) som debiteras. Jag tycker även [den här sidan](https://www.blinfo.se/foretagskunskap/bokfora-lon__15472) är bra här.\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> har en färdig mall för **fakturor**. Du anger ditt företags bankgiro, och/eller kontonummer, lägger upp kunder och artiklar. Sedan kan du enkelt skapa fakturor som du sedan kan skriva ut, spara som PDF eller maila.\\n\\nOm du **betalar ut traktamente** behöver du kunna bevisa att [resorna ägt rum och varit tjänsteresor](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm). Därför kan det vara klokt att föra anteckningar för resor. I mitt fall åker jag alltid tåg och kommer hänvisa till tågbiljetterna om jag får revision av Skatteverket. Jag skapar exceldokument, med uträkningen, som jag sedan exporterar till PDF och sparar för att motivera traktamentet.\\n\\n![Traktamente](/content/images/2017/11/traktamente-utr.png)\\n\\nMan hör ibland uttrycket **skriva av** och jag tror många missförstår det, kanske även jag =) Men jag tolkar det som att allt man köper för under ett halvt prisbasbelopp (ungefär 22 000 kr) skrivs av direkt, kostar det mer skriver man av det under flera år. Att *skriva av* innebär att man inte tar upp det som en tillgång, en inventarie, i bokföringen. Det är dock inte samma sak som att man får ge bort prylen till sig själv eller någon annan. Men jag är osäker, [och många med mig](https://www.flashback.org/p49423662).\\n\\nAtt bokföra köp gjorda med **kreditkort** är lite speciellt men det har Visma en väldigt [bra artikel](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) om.\\n\\nJag sparar alltid alla **kvitton** direkt på Google Drive. Är det inte digitalt fotar jag av med mobilen, laddar upp bilden och sparar även original-kvittot i en pärm. I bokföringsprogrammet är jag också noga med att **ladda upp bilder på varje verifikation**. [Bokföringslagen](https://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/bokforingslag-19991078_sfs-1999-1078) säger att originalen ska sparas, så digitala ska sparas digitalt och de på papper sparas på papper. Jag tycker även att [den här artikeln](https://vismaspcs.se/ditt-foretagande/byra/de-har-kopiorna-behover-du-inte-skriva-ut) beskriver det bra. Även andra dokument fotar jag av och sparar på Google Drive, som beslut från Skatteverket t.ex. Väldigt smidigt att kunna läsa allt var man än är.  Det är ganska enkelt att få tag på ett rejält brandskyddat kassaskåp ([#1](http://www.clasohlson.com/se/Dokumentsk%C3%A5p-Chubbsafes-Executive-40/Pr342428000) eller [#2](http://www.biltema.se/sv/Hem/Sakerhet-och-larm/Sakerhetsbox/Kassaskap-brandsakert-2000033385/)) så att man kan spara papper korrekt enligt lagstiftningen.\\n\\nPå **bokslutsdagen, sista dagen på räkenskapsåret, måste [obetalda fakturor tas upp i bokföring](https://www.verksamt.se/driva/ekonomi-och-bokforing/lopande-bokforing)** även om du använder kontantmetoden. Det beskriver också Visma [här](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi). Om du har skapat en faktura som ännu inte blivit betald kommer <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> att ge dig följande meddelande när du skapar sista **kvartalsmomsen**.\\n\\n> Enligt Kontantmetoden ska du bokföra dina obetalda kundfordringar och leverantörsskulder vid räkenskapsårets slut och redovisa momsen. Detta kommer nu att göras automatiskt. Du bör kontrollera att alla inbetalningar och utbetalningar på året är avstämda innan du fortsätter. Vill du fortsätta?\\n\\nDet är viktigt att ta en extra titt på de sista transaktionerna innan räkenskapsåret är slut så att det inte dykt upp något i sista sekunden. I mitt fall hade Danskebank skapat en faktura, och dragit pengarna, för bankavgifter sista dagen i december.\\n\\nNär räkenskapsåret är slut ska **kontrolluppgifter** för anställda skickas till Skatteverket. Det här gäller 2017 men är på gång att ändras så att man skickar samma uppgifter i arbetsgivardeklarationen istället. Men i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> finns en funktion under *lön* i huvudmenyn som kan användas om du skapat lönespecar.\\n\\n* Klicka på *Ny kontrolluppgift*.\\n* Fyll i dina uppgifter.\\n* Spara.\\n* Nu kan man ladda ner en *xml-fil* som kan laddas upp hos Skatteverket för att fylla i *KU10*-blanketten.\\n\\nVisma har en väldigt bra [gratis broschyr om bokföring](http://download.vismaspcs.se/pdf/bokf%C3%B6ring-f%C3%B6r-nyb%C3%B6rjare.pdf?utm_source=Eloqua&utm_medium=email&utm_content=SE_SP_SI_Onboarding-eEko-Std-4&utm_campaign=SE_SP_SI_Onboarding-eA-Std-1704).\\n\\n# Övrigt\\n\\nOm du **veckopendlar** som jag så kanske du har en **lägenhet på arbetsorten**. Då kan man tänka sig att man [låter bolaget betala den](http://www.skatter.se/?q=node/2619). Jag blev rekommenderad att ta den privat och betala ut mer lön från bolaget istället. Då räknar jag på detta vid utbetalning av lön så att lön efter avdrag hamnar på gränsen för statlig skatt. Avdraget gör jag alltså i min privata deklaration. Väljer man att låta bolaget direkt betala lägenheten blir det lite krångligare med bokföring och deklaration. Det kan också vara så att Skatteverket ser det som en förmån. Så det här kändes enklast.\\n\\nDet finns bolag som har som **affärsidé att bara ha underkonsulter**. De åker på konferensresor och har gemensam kontorslokal precis som vilket annat bolag som helst. Skillnaden är bara att alla är underkonsulter. Detta bolag hjälper dig att starta ditt eget bolag och hitta kunder. De har ramavtal med stora konsultköpare och kan ordna bra timpriser. Ett sånt bolag är [Kvadrat](http://www.kvadrat.se/).\\n\\nJag valde först att ta över kontraktet jag var på och då gå från anställd konsult till underkonsult. Senare bytte jag uppdrag och då anslöt jag mig till [Kvadrat](http://www.kvadrat.se/).\\n\\nHar du frågor om skatter så är det smidigt att man kan [maila Skatteverket](https://www.skatteverket.se/omoss/kontaktaoss/mejla/).\\n\\nJag har skapat en facebook-grupp för att diskutera det här, går gärna med! https://www.facebook.com/groups/791240028009906/\"}]],\"markups\":[],\"sections\":[[10,0]]}',1577958375314,'2020-01-02 09:46:15');
/*!40000 ALTER TABLE `mobiledoc_revisions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `permissions`
--

DROP TABLE IF EXISTS `permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `permissions` (
  `id` varchar(24) NOT NULL,
  `name` varchar(50) NOT NULL,
  `object_type` varchar(50) NOT NULL,
  `action_type` varchar(50) NOT NULL,
  `object_id` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `permissions_name_unique` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `permissions`
--

LOCK TABLES `permissions` WRITE;
/*!40000 ALTER TABLE `permissions` DISABLE KEYS */;
INSERT INTO `permissions` VALUES ('597b43301ffc934f6c0277c0','Export database','db','exportContent',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c1','Import database','db','importContent',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c2','Delete all content','db','deleteAllContent',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c3','Send mail','mail','send',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c4','Browse notifications','notification','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c5','Add notifications','notification','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c6','Delete notifications','notification','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c7','Browse posts','post','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c8','Read posts','post','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277c9','Edit posts','post','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ca','Add posts','post','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277cb','Delete posts','post','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277cc','Browse settings','setting','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277cd','Read settings','setting','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ce','Edit settings','setting','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277cf','Generate slugs','slug','generate',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d0','Browse tags','tag','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d1','Read tags','tag','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d2','Edit tags','tag','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d3','Add tags','tag','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d4','Delete tags','tag','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d5','Browse themes','theme','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d6','Edit themes','theme','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d7','Activate themes','theme','activate',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d8','Upload themes','theme','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277d9','Download themes','theme','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277da','Delete themes','theme','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277db','Browse users','user','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277dc','Read users','user','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277dd','Edit users','user','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277de','Add users','user','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277df','Delete users','user','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e0','Assign a role','role','assign',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e1','Browse roles','role','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e2','Browse clients','client','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e3','Read clients','client','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e4','Edit clients','client','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e5','Add clients','client','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e6','Delete clients','client','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e7','Browse subscribers','subscriber','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e8','Read subscribers','subscriber','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277e9','Edit subscribers','subscriber','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ea','Add subscribers','subscriber','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277eb','Delete subscribers','subscriber','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ec','Browse invites','invite','browse',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ed','Read invites','invite','read',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ee','Edit invites','invite','edit',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277ef','Add invites','invite','add',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277f0','Delete invites','invite','destroy',NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('59e30f44c4d2c41252c4c292','Download redirects','redirect','download',NULL,'2017-10-15 07:33:24','1','2017-10-15 07:33:24','1'),('59e30f44c4d2c41252c4c293','Upload redirects','redirect','upload',NULL,'2017-10-15 07:33:24','1','2017-10-15 07:33:24','1'),('5a3a00cceddac110132b6dac','Add webhooks','webhook','add',NULL,'2017-12-20 06:18:52','1','2017-12-20 06:18:52','1'),('5a3a00cceddac110132b6dad','Delete webhooks','webhook','destroy',NULL,'2017-12-20 06:18:52','1','2017-12-20 06:18:52','1'),('5bd215b976568b252315b9e2','Browse integrations','integration','browse',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9e3','Browse API keys','api_key','browse',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9e4','Read integrations','integration','read',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9e5','Read API keys','api_key','read',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9e6','Edit integrations','integration','edit',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9e7','Edit API keys','api_key','edit',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9e8','Add integrations','integration','add',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9e9','Add API keys','api_key','add',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9ea','Delete integrations','integration','destroy',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215b976568b252315b9eb','Delete API keys','api_key','destroy',NULL,'2018-10-25 19:12:57','1','2018-10-25 19:12:57','1'),('5bd215bb76568b252315b9f6','Edit webhooks','webhook','edit',NULL,'2018-10-25 19:12:59','1','2018-10-25 19:12:59','1');
/*!40000 ALTER TABLE `permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `permissions_apps`
--

DROP TABLE IF EXISTS `permissions_apps`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `permissions_apps` (
  `id` varchar(24) NOT NULL,
  `app_id` varchar(24) NOT NULL,
  `permission_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `permissions_apps`
--

LOCK TABLES `permissions_apps` WRITE;
/*!40000 ALTER TABLE `permissions_apps` DISABLE KEYS */;
/*!40000 ALTER TABLE `permissions_apps` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `permissions_roles`
--

DROP TABLE IF EXISTS `permissions_roles`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `permissions_roles` (
  `id` varchar(24) NOT NULL,
  `role_id` varchar(24) NOT NULL,
  `permission_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `permissions_roles`
--

LOCK TABLES `permissions_roles` WRITE;
/*!40000 ALTER TABLE `permissions_roles` DISABLE KEYS */;
INSERT INTO `permissions_roles` VALUES ('597b43311ffc934f6c0277f2','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c0'),('597b43311ffc934f6c0277f3','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c1'),('597b43311ffc934f6c0277f4','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c2'),('597b43311ffc934f6c0277f5','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c3'),('597b43311ffc934f6c0277f6','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c4'),('597b43311ffc934f6c0277f7','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c5'),('597b43311ffc934f6c0277f8','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c6'),('597b43311ffc934f6c0277f9','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c7'),('597b43311ffc934f6c0277fa','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c8'),('597b43311ffc934f6c0277fb','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277c9'),('597b43311ffc934f6c0277fc','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277ca'),('597b43311ffc934f6c0277fd','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277cb'),('597b43311ffc934f6c0277fe','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277cc'),('597b43311ffc934f6c0277ff','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277cd'),('597b43311ffc934f6c027800','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277ce'),('597b43311ffc934f6c027801','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277cf'),('597b43311ffc934f6c027802','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d0'),('597b43311ffc934f6c027803','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d1'),('597b43311ffc934f6c027804','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d2'),('597b43311ffc934f6c027805','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d3'),('597b43311ffc934f6c027806','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d4'),('597b43311ffc934f6c027807','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d5'),('597b43311ffc934f6c027808','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d6'),('597b43311ffc934f6c027809','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d7'),('597b43311ffc934f6c02780a','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d8'),('597b43311ffc934f6c02780b','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277d9'),('597b43311ffc934f6c02780c','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277da'),('597b43311ffc934f6c02780d','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277db'),('597b43311ffc934f6c02780e','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277dc'),('597b43311ffc934f6c02780f','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277dd'),('597b43311ffc934f6c027810','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277de'),('597b43311ffc934f6c027811','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277df'),('597b43311ffc934f6c027812','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e0'),('597b43311ffc934f6c027813','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e1'),('597b43311ffc934f6c027814','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e2'),('597b43311ffc934f6c027815','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e3'),('597b43311ffc934f6c027816','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e4'),('597b43311ffc934f6c027817','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e5'),('597b43311ffc934f6c027818','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e6'),('597b43311ffc934f6c027819','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e7'),('597b43311ffc934f6c02781a','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e8'),('597b43311ffc934f6c02781b','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277e9'),('597b43311ffc934f6c02781c','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277ea'),('597b43311ffc934f6c02781d','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277eb'),('597b43311ffc934f6c02781e','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277ec'),('597b43311ffc934f6c02781f','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277ed'),('597b43311ffc934f6c027820','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277ee'),('597b43311ffc934f6c027821','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277ef'),('597b43311ffc934f6c027822','597b43301ffc934f6c0277bc','597b43301ffc934f6c0277f0'),('597b43311ffc934f6c027823','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277c7'),('597b43311ffc934f6c027824','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277c8'),('597b43311ffc934f6c027825','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277c9'),('597b43311ffc934f6c027826','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277ca'),('597b43311ffc934f6c027827','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277cb'),('597b43311ffc934f6c027828','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277cc'),('597b43311ffc934f6c027829','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277cd'),('597b43311ffc934f6c02782a','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277cf'),('597b43311ffc934f6c02782b','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277d0'),('597b43311ffc934f6c02782c','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277d1'),('597b43311ffc934f6c02782d','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277d2'),('597b43311ffc934f6c02782e','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277d3'),('597b43311ffc934f6c02782f','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277d4'),('597b43311ffc934f6c027830','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277db'),('597b43311ffc934f6c027831','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277dc'),('597b43311ffc934f6c027832','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277dd'),('597b43311ffc934f6c027833','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277de'),('597b43311ffc934f6c027834','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277df'),('597b43311ffc934f6c027835','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277e0'),('597b43311ffc934f6c027836','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277e1'),('597b43311ffc934f6c027837','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277e2'),('597b43311ffc934f6c027838','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277e3'),('597b43311ffc934f6c027839','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277e4'),('597b43311ffc934f6c02783a','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277e5'),('597b43311ffc934f6c02783b','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277e6'),('597b43311ffc934f6c02783c','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277ea'),('597b43311ffc934f6c02783d','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277ec'),('597b43311ffc934f6c02783e','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277ed'),('597b43311ffc934f6c02783f','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277ee'),('597b43311ffc934f6c027840','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277ef'),('597b43311ffc934f6c027841','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277f0'),('597b43311ffc934f6c027842','597b43301ffc934f6c0277be','597b43301ffc934f6c0277c7'),('597b43311ffc934f6c027843','597b43301ffc934f6c0277be','597b43301ffc934f6c0277c8'),('597b43311ffc934f6c027844','597b43301ffc934f6c0277be','597b43301ffc934f6c0277ca'),('597b43311ffc934f6c027845','597b43301ffc934f6c0277be','597b43301ffc934f6c0277cc'),('597b43311ffc934f6c027846','597b43301ffc934f6c0277be','597b43301ffc934f6c0277cd'),('597b43311ffc934f6c027847','597b43301ffc934f6c0277be','597b43301ffc934f6c0277cf'),('597b43311ffc934f6c027848','597b43301ffc934f6c0277be','597b43301ffc934f6c0277d0'),('597b43311ffc934f6c027849','597b43301ffc934f6c0277be','597b43301ffc934f6c0277d1'),('597b43311ffc934f6c02784a','597b43301ffc934f6c0277be','597b43301ffc934f6c0277d3'),('597b43311ffc934f6c02784b','597b43301ffc934f6c0277be','597b43301ffc934f6c0277db'),('597b43311ffc934f6c02784c','597b43301ffc934f6c0277be','597b43301ffc934f6c0277dc'),('597b43311ffc934f6c02784d','597b43301ffc934f6c0277be','597b43301ffc934f6c0277e1'),('597b43311ffc934f6c02784e','597b43301ffc934f6c0277be','597b43301ffc934f6c0277e2'),('597b43311ffc934f6c02784f','597b43301ffc934f6c0277be','597b43301ffc934f6c0277e3'),('597b43311ffc934f6c027850','597b43301ffc934f6c0277be','597b43301ffc934f6c0277e4'),('597b43311ffc934f6c027851','597b43301ffc934f6c0277be','597b43301ffc934f6c0277e5'),('597b43311ffc934f6c027852','597b43301ffc934f6c0277be','597b43301ffc934f6c0277e6'),('597b43311ffc934f6c027853','597b43301ffc934f6c0277be','597b43301ffc934f6c0277ea'),('59e30f44c4d2c41252c4c290','597b43301ffc934f6c0277bd','597b43301ffc934f6c0277d5'),('59e30f44c4d2c41252c4c291','597b43301ffc934f6c0277be','597b43301ffc934f6c0277d5'),('59e30f44c4d2c41252c4c294','597b43301ffc934f6c0277bc','59e30f44c4d2c41252c4c292'),('59e30f44c4d2c41252c4c295','597b43301ffc934f6c0277bc','59e30f44c4d2c41252c4c293'),('5a3a00cceddac110132b6dae','597b43301ffc934f6c0277bc','5a3a00cceddac110132b6dac'),('5a3a00cceddac110132b6daf','597b43301ffc934f6c0277bc','5a3a00cceddac110132b6dad'),('5a9845ce6b6eb71021b13f89','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277c7'),('5a9845ce6b6eb71021b13f8a','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277c8'),('5a9845ce6b6eb71021b13f8b','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277ca'),('5a9845cf6b6eb71021b13f8c','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277cc'),('5a9845cf6b6eb71021b13f8d','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277cd'),('5a9845cf6b6eb71021b13f8e','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277cf'),('5a9845cf6b6eb71021b13f8f','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277d0'),('5a9845cf6b6eb71021b13f90','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277d1'),('5a9845cf6b6eb71021b13f91','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277db'),('5a9845cf6b6eb71021b13f92','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277dc'),('5a9845cf6b6eb71021b13f93','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277e1'),('5a9845cf6b6eb71021b13f94','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277e2'),('5a9845cf6b6eb71021b13f95','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277e3'),('5a9845cf6b6eb71021b13f96','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277e4'),('5a9845cf6b6eb71021b13f97','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277e5'),('5a9845cf6b6eb71021b13f98','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277e6'),('5a9845cf6b6eb71021b13f99','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277ea'),('5a9845cf6b6eb71021b13f9a','5a9845ce6b6eb71021b13f88','597b43301ffc934f6c0277d5'),('5bd215b976568b252315b9b0','5bd215b976568b252315b9af','597b43301ffc934f6c0277c3'),('5bd215b976568b252315b9b1','5bd215b976568b252315b9af','597b43301ffc934f6c0277c4'),('5bd215b976568b252315b9b2','5bd215b976568b252315b9af','597b43301ffc934f6c0277c5'),('5bd215b976568b252315b9b3','5bd215b976568b252315b9af','597b43301ffc934f6c0277c6'),('5bd215b976568b252315b9b4','5bd215b976568b252315b9af','597b43301ffc934f6c0277c7'),('5bd215b976568b252315b9b5','5bd215b976568b252315b9af','597b43301ffc934f6c0277c8'),('5bd215b976568b252315b9b6','5bd215b976568b252315b9af','597b43301ffc934f6c0277c9'),('5bd215b976568b252315b9b7','5bd215b976568b252315b9af','597b43301ffc934f6c0277ca'),('5bd215b976568b252315b9b8','5bd215b976568b252315b9af','597b43301ffc934f6c0277cb'),('5bd215b976568b252315b9b9','5bd215b976568b252315b9af','597b43301ffc934f6c0277cc'),('5bd215b976568b252315b9ba','5bd215b976568b252315b9af','597b43301ffc934f6c0277cd'),('5bd215b976568b252315b9bb','5bd215b976568b252315b9af','597b43301ffc934f6c0277ce'),('5bd215b976568b252315b9bc','5bd215b976568b252315b9af','597b43301ffc934f6c0277cf'),('5bd215b976568b252315b9bd','5bd215b976568b252315b9af','597b43301ffc934f6c0277d0'),('5bd215b976568b252315b9be','5bd215b976568b252315b9af','597b43301ffc934f6c0277d1'),('5bd215b976568b252315b9bf','5bd215b976568b252315b9af','597b43301ffc934f6c0277d2'),('5bd215b976568b252315b9c0','5bd215b976568b252315b9af','597b43301ffc934f6c0277d3'),('5bd215b976568b252315b9c1','5bd215b976568b252315b9af','597b43301ffc934f6c0277d4'),('5bd215b976568b252315b9c2','5bd215b976568b252315b9af','597b43301ffc934f6c0277d5'),('5bd215b976568b252315b9c3','5bd215b976568b252315b9af','597b43301ffc934f6c0277d6'),('5bd215b976568b252315b9c4','5bd215b976568b252315b9af','597b43301ffc934f6c0277d7'),('5bd215b976568b252315b9c5','5bd215b976568b252315b9af','597b43301ffc934f6c0277d8'),('5bd215b976568b252315b9c6','5bd215b976568b252315b9af','597b43301ffc934f6c0277d9'),('5bd215b976568b252315b9c7','5bd215b976568b252315b9af','597b43301ffc934f6c0277da'),('5bd215b976568b252315b9c8','5bd215b976568b252315b9af','597b43301ffc934f6c0277db'),('5bd215b976568b252315b9c9','5bd215b976568b252315b9af','597b43301ffc934f6c0277dc'),('5bd215b976568b252315b9ca','5bd215b976568b252315b9af','597b43301ffc934f6c0277dd'),('5bd215b976568b252315b9cb','5bd215b976568b252315b9af','597b43301ffc934f6c0277de'),('5bd215b976568b252315b9cc','5bd215b976568b252315b9af','597b43301ffc934f6c0277df'),('5bd215b976568b252315b9cd','5bd215b976568b252315b9af','597b43301ffc934f6c0277e0'),('5bd215b976568b252315b9ce','5bd215b976568b252315b9af','597b43301ffc934f6c0277e1'),('5bd215b976568b252315b9cf','5bd215b976568b252315b9af','597b43301ffc934f6c0277e2'),('5bd215b976568b252315b9d0','5bd215b976568b252315b9af','597b43301ffc934f6c0277e3'),('5bd215b976568b252315b9d1','5bd215b976568b252315b9af','597b43301ffc934f6c0277e4'),('5bd215b976568b252315b9d2','5bd215b976568b252315b9af','597b43301ffc934f6c0277e5'),('5bd215b976568b252315b9d3','5bd215b976568b252315b9af','597b43301ffc934f6c0277e6'),('5bd215b976568b252315b9d4','5bd215b976568b252315b9af','597b43301ffc934f6c0277e7'),('5bd215b976568b252315b9d5','5bd215b976568b252315b9af','597b43301ffc934f6c0277e8'),('5bd215b976568b252315b9d6','5bd215b976568b252315b9af','597b43301ffc934f6c0277e9'),('5bd215b976568b252315b9d7','5bd215b976568b252315b9af','597b43301ffc934f6c0277ea'),('5bd215b976568b252315b9d8','5bd215b976568b252315b9af','597b43301ffc934f6c0277eb'),('5bd215b976568b252315b9d9','5bd215b976568b252315b9af','597b43301ffc934f6c0277ec'),('5bd215b976568b252315b9da','5bd215b976568b252315b9af','597b43301ffc934f6c0277ed'),('5bd215b976568b252315b9db','5bd215b976568b252315b9af','597b43301ffc934f6c0277ee'),('5bd215b976568b252315b9dc','5bd215b976568b252315b9af','597b43301ffc934f6c0277ef'),('5bd215b976568b252315b9dd','5bd215b976568b252315b9af','597b43301ffc934f6c0277f0'),('5bd215b976568b252315b9de','5bd215b976568b252315b9af','59e30f44c4d2c41252c4c292'),('5bd215b976568b252315b9df','5bd215b976568b252315b9af','59e30f44c4d2c41252c4c293'),('5bd215b976568b252315b9e0','5bd215b976568b252315b9af','5a3a00cceddac110132b6dac'),('5bd215b976568b252315b9e1','5bd215b976568b252315b9af','5a3a00cceddac110132b6dad'),('5bd215b976568b252315b9ec','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e2'),('5bd215b976568b252315b9ed','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e4'),('5bd215b976568b252315b9ee','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e6'),('5bd215b976568b252315b9ef','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e8'),('5bd215b976568b252315b9f0','597b43301ffc934f6c0277bc','5bd215b976568b252315b9ea'),('5bd215b976568b252315b9f1','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e3'),('5bd215b976568b252315b9f2','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e5'),('5bd215b976568b252315b9f3','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e7'),('5bd215b976568b252315b9f4','597b43301ffc934f6c0277bc','5bd215b976568b252315b9e9'),('5bd215b976568b252315b9f5','597b43301ffc934f6c0277bc','5bd215b976568b252315b9eb'),('5c41e2bcf581f72431885ba2','597b43301ffc934f6c0277bc','5bd215bb76568b252315b9f6'),('5c41e2bcf581f72431885ba3','5bd215b976568b252315b9af','5bd215bb76568b252315b9f6');
/*!40000 ALTER TABLE `permissions_roles` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `permissions_users`
--

DROP TABLE IF EXISTS `permissions_users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `permissions_users` (
  `id` varchar(24) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  `permission_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `permissions_users`
--

LOCK TABLES `permissions_users` WRITE;
/*!40000 ALTER TABLE `permissions_users` DISABLE KEYS */;
/*!40000 ALTER TABLE `permissions_users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts`
--

DROP TABLE IF EXISTS `posts`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `posts` (
  `id` varchar(24) NOT NULL,
  `uuid` varchar(36) NOT NULL,
  `title` varchar(2000) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `mobiledoc` longtext,
  `html` longtext,
  `comment_id` longtext,
  `plaintext` longtext,
  `feature_image` varchar(2000) DEFAULT NULL,
  `featured` tinyint(1) NOT NULL DEFAULT '0',
  `page` tinyint(1) NOT NULL DEFAULT '0',
  `status` varchar(50) NOT NULL DEFAULT 'draft',
  `locale` varchar(6) DEFAULT NULL,
  `visibility` varchar(50) NOT NULL DEFAULT 'public',
  `meta_title` varchar(2000) DEFAULT NULL,
  `meta_description` varchar(2000) DEFAULT NULL,
  `author_id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  `published_at` datetime DEFAULT NULL,
  `published_by` varchar(24) DEFAULT NULL,
  `custom_excerpt` varchar(2000) DEFAULT NULL,
  `codeinjection_head` text,
  `codeinjection_foot` text,
  `og_image` varchar(2000) DEFAULT NULL,
  `og_title` varchar(300) DEFAULT NULL,
  `og_description` varchar(500) DEFAULT NULL,
  `twitter_image` varchar(2000) DEFAULT NULL,
  `twitter_title` varchar(300) DEFAULT NULL,
  `twitter_description` varchar(500) DEFAULT NULL,
  `custom_template` varchar(100) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `posts_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts`
--

LOCK TABLES `posts` WRITE;
/*!40000 ALTER TABLE `posts` DISABLE KEYS */;
INSERT INTO `posts` VALUES ('597b4433e521cb4fbd918692','cf6d402a-0dbb-4872-a1f2-6f7d2d1571dc','JDBC problem: ResultSet.getString returns nothing','jdbc-problem-resultset-getstring-returns-nothing','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently did some database development in Java using Eclipse with JDBC and MySQL. I came across something very weird that I would like to share.\\n\\nAt first I thought the getString method of my ResultSet instance returned empty Strings but as I started analyzing the returned content I found out that it was actually null bytes. Exactly as many as the length of the expected string.\\n\\nAs always I Googled the phenomena and found people with similar problem, but no solution. I found that this is actually an Eclipse thing and when I tried to run my code outside Eclipse it worked perfectly.\\n\\nI finally came up with a solution, a replacement for getString that uses getAsciiStream insted.\\n```\\npublic String getString(ResultSet result, String column) {\\nString str = new String();\\ntry {\\nInputStreamReader in = new InputStreamReader(result\\n.getAsciiStream(column));\\nwhile (in.ready())\\nstr = str + (char) in.read();\\n} catch (SQLException e) {\\ne.printStackTrace();\\n} catch (IOException e) {\\ne.printStackTrace();\\n} finally {\\nreturn str;\\n}\\n}\\n```\"}]],\"sections\":[[10,0]]}','<p>I recently did some database development in Java using Eclipse with JDBC and MySQL. I came across something very weird that I would like to share.</p>\n<p>At first I thought the getString method of my ResultSet instance returned empty Strings but as I started analyzing the returned content I found out that it was actually null bytes. Exactly as many as the length of the expected string.</p>\n<p>As always I Googled the phenomena and found people with similar problem, but no solution. I found that this is actually an Eclipse thing and when I tried to run my code outside Eclipse it worked perfectly.</p>\n<p>I finally came up with a solution, a replacement for getString that uses getAsciiStream insted.</p>\n<pre><code>public String getString(ResultSet result, String column) {\nString str = new String();\ntry {\nInputStreamReader in = new InputStreamReader(result\n.getAsciiStream(column));\nwhile (in.ready())\nstr = str + (char) in.read();\n} catch (SQLException e) {\ne.printStackTrace();\n} catch (IOException e) {\ne.printStackTrace();\n} finally {\nreturn str;\n}\n}\n</code></pre>\n','5','I recently did some database development in Java using Eclipse with JDBC and\nMySQL. I came across something very weird that I would like to share.\n\nAt first I thought the getString method of my ResultSet instance returned empty\nStrings but as I started analyzing the returned content I found out that it was\nactually null bytes. Exactly as many as the length of the expected string.\n\nAs always I Googled the phenomena and found people with similar problem, but no\nsolution. I found that this is actually an Eclipse thing and when I tried to run\nmy code outside Eclipse it worked perfectly.\n\nI finally came up with a solution, a replacement for getString that uses\ngetAsciiStream insted.\n\npublic String getString(ResultSet result, String column) {\nString str = new String();\ntry {\nInputStreamReader in = new InputStreamReader(result\n.getAsciiStream(column));\nwhile (in.ready())\nstr = str + (char) in.read();\n} catch (SQLException e) {\ne.printStackTrace();\n} catch (IOException e) {\ne.printStackTrace();\n} finally {\nreturn str;\n}\n}',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2013-10-19 09:00:52','2','2014-10-04 14:06:25','1','2013-10-19 09:00:52','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd918693','d98a892f-fb6e-4276-b733-b48b467f2ba9','Runtime.exec hangs, a complete solution','runtime-exec-hangs-a-complete-solution','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"If you Google \\\"Runtime.exec hangs\\\" you will get alot of results. Executing external commands from Java seems very easy but it is actually very complicated. I was stuck at this problem for a long time before I finally got it. I never found a complete solution so I tought a post like this may boost my user count =)\\n\\nIf you execute a command using Runtime.exec on Windows and wait for it to complete, like follows, your program may hang on waitFor.\\n```\\nProcess p = Runtime.getRuntime().exec(\\\"myCommand\\\");\\np.waitFor();\\n```\\n\\nThe reason for the hanging is the communication between Java and the external operating system process. And more specifically the communication buffers. The buffers for STDERR and STDOUT has to be flushed for the program not to hang.\\n\\nFlushing these buffers are very easy, but! You can not flush them one by one, you have to do it simultaniously for the program not to hang while flushing! So you have to start at least one new thread to succeed.\\n\\nI have made a complete class that executes a command and serves the content of STDOUT and STDERR. It also provides functionality for sending data to STDIN.\\n```\\nimport java.io.BufferedReader;\\nimport java.io.IOException;\\nimport java.io.InputStreamReader;\\nimport java.io.PrintWriter;\\nimport java.util.ArrayList;\\nimport java.util.concurrent.Semaphore;\\n\\npublic class ExecCommand {\\nprivate Semaphore outputSem;\\nprivate String output;\\nprivate Semaphore errorSem;\\nprivate String error;\\nprivate Process p;\\n\\nprivate class InputWriter extends Thread {\\nprivate String input;\\n\\npublic InputWriter(String input) {\\nthis.input = input;\\n}\\n\\npublic void run() {\\nPrintWriter pw = new PrintWriter(p.getOutputStream());\\npw.println(input);\\npw.flush();\\n}\\n}\\n\\nprivate class OutputReader extends Thread {\\npublic OutputReader() {\\ntry {\\noutputSem = new Semaphore(1);\\noutputSem.acquire();\\n} catch (InterruptedException e) {\\ne.printStackTrace();\\n}\\n}\\n\\npublic void run() {\\ntry {\\nStringBuffer readBuffer = new StringBuffer();\\nBufferedReader isr = new BufferedReader(new InputStreamReader(p\\n.getInputStream()));\\nString buff = new String();\\nwhile ((buff = isr.readLine()) != null) {\\nreadBuffer.append(buff);\\nSystem.out.println(buff);\\n}\\noutput = readBuffer.toString();\\noutputSem.release();\\n} catch (IOException e) {\\ne.printStackTrace();\\n}\\n}\\n}\\n\\nprivate class ErrorReader extends Thread {\\npublic ErrorReader() {\\ntry {\\nerrorSem = new Semaphore(1);\\nerrorSem.acquire();\\n} catch (InterruptedException e) {\\ne.printStackTrace();\\n}\\n}\\n\\npublic void run() {\\ntry {\\nStringBuffer readBuffer = new StringBuffer();\\nBufferedReader isr = new BufferedReader(new InputStreamReader(p\\n.getErrorStream()));\\nString buff = new String();\\nwhile ((buff = isr.readLine()) != null) {\\nreadBuffer.append(buff);\\n}\\nerror = readBuffer.toString();\\nerrorSem.release();\\n} catch (IOException e) {\\ne.printStackTrace();\\n}\\nif (error.length() > 0)\\nSystem.out.println(error);\\n}\\n}\\n\\npublic ExecCommand(String command, String input) {\\ntry {\\np = Runtime.getRuntime().exec(makeArray(command));\\nnew InputWriter(input).start();\\nnew OutputReader().start();\\nnew ErrorReader().start();\\np.waitFor();\\n} catch (IOException e) {\\ne.printStackTrace();\\n} catch (InterruptedException e) {\\ne.printStackTrace();\\n}\\n}\\n\\npublic ExecCommand(String command) {\\ntry {\\np = Runtime.getRuntime().exec(makeArray(command));\\nnew OutputReader().start();\\nnew ErrorReader().start();\\np.waitFor();\\n} catch (IOException e) {\\ne.printStackTrace();\\n} catch (InterruptedException e) {\\ne.printStackTrace();\\n}\\n}\\n\\npublic String getOutput() {\\ntry {\\noutputSem.acquire();\\n} catch (InterruptedException e) {\\ne.printStackTrace();\\n}\\nString value = output;\\noutputSem.release();\\nreturn value;\\n}\\n\\npublic String getError() {\\ntry {\\nerrorSem.acquire();\\n} catch (InterruptedException e) {\\ne.printStackTrace();\\n}\\nString value = error;\\nerrorSem.release();\\nreturn value;\\n}\\n\\nprivate String[] makeArray(String command) {\\nArrayList<String> commandArray = new ArrayList<String>();\\nString buff = \\\"\\\";\\nboolean lookForEnd = false;\\nfor (int i = 0; i < command.length(); i++) {\\nif (lookForEnd) {\\nif (command.charAt(i) == \'\\\\\\\"\') {\\nif (buff.length() > 0)\\ncommandArray.add(buff);\\nbuff = \\\"\\\";\\nlookForEnd = false;\\n} else {\\nbuff += command.charAt(i);\\n}\\n} else {\\nif (command.charAt(i) == \'\\\\\\\"\') {\\nlookForEnd = true;\\n} else if (command.charAt(i) == \' \') {\\nif (buff.length() > 0)\\ncommandArray.add(buff);\\nbuff = \\\"\\\";\\n} else {\\nbuff += command.charAt(i);\\n}\\n}\\n}\\nif (buff.length() > 0)\\ncommandArray.add(buff);\\n\\nString[] array = new String[commandArray.size()];\\nfor (int i = 0; i < commandArray.size(); i++) {\\narray[i] = commandArray.get(i);\\n}\\n\\nreturn array;\\n}\\n}\\n```\"}]],\"sections\":[[10,0]]}','<p>If you Google &quot;Runtime.exec hangs&quot; you will get alot of results. Executing external commands from Java seems very easy but it is actually very complicated. I was stuck at this problem for a long time before I finally got it. I never found a complete solution so I tought a post like this may boost my user count =)</p>\n<p>If you execute a command using Runtime.exec on Windows and wait for it to complete, like follows, your program may hang on waitFor.</p>\n<pre><code>Process p = Runtime.getRuntime().exec(&quot;myCommand&quot;);\np.waitFor();\n</code></pre>\n<p>The reason for the hanging is the communication between Java and the external operating system process. And more specifically the communication buffers. The buffers for STDERR and STDOUT has to be flushed for the program not to hang.</p>\n<p>Flushing these buffers are very easy, but! You can not flush them one by one, you have to do it simultaniously for the program not to hang while flushing! So you have to start at least one new thread to succeed.</p>\n<p>I have made a complete class that executes a command and serves the content of STDOUT and STDERR. It also provides functionality for sending data to STDIN.</p>\n<pre><code>import java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.io.PrintWriter;\nimport java.util.ArrayList;\nimport java.util.concurrent.Semaphore;\n\npublic class ExecCommand {\nprivate Semaphore outputSem;\nprivate String output;\nprivate Semaphore errorSem;\nprivate String error;\nprivate Process p;\n\nprivate class InputWriter extends Thread {\nprivate String input;\n\npublic InputWriter(String input) {\nthis.input = input;\n}\n\npublic void run() {\nPrintWriter pw = new PrintWriter(p.getOutputStream());\npw.println(input);\npw.flush();\n}\n}\n\nprivate class OutputReader extends Thread {\npublic OutputReader() {\ntry {\noutputSem = new Semaphore(1);\noutputSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic void run() {\ntry {\nStringBuffer readBuffer = new StringBuffer();\nBufferedReader isr = new BufferedReader(new InputStreamReader(p\n.getInputStream()));\nString buff = new String();\nwhile ((buff = isr.readLine()) != null) {\nreadBuffer.append(buff);\nSystem.out.println(buff);\n}\noutput = readBuffer.toString();\noutputSem.release();\n} catch (IOException e) {\ne.printStackTrace();\n}\n}\n}\n\nprivate class ErrorReader extends Thread {\npublic ErrorReader() {\ntry {\nerrorSem = new Semaphore(1);\nerrorSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic void run() {\ntry {\nStringBuffer readBuffer = new StringBuffer();\nBufferedReader isr = new BufferedReader(new InputStreamReader(p\n.getErrorStream()));\nString buff = new String();\nwhile ((buff = isr.readLine()) != null) {\nreadBuffer.append(buff);\n}\nerror = readBuffer.toString();\nerrorSem.release();\n} catch (IOException e) {\ne.printStackTrace();\n}\nif (error.length() &gt; 0)\nSystem.out.println(error);\n}\n}\n\npublic ExecCommand(String command, String input) {\ntry {\np = Runtime.getRuntime().exec(makeArray(command));\nnew InputWriter(input).start();\nnew OutputReader().start();\nnew ErrorReader().start();\np.waitFor();\n} catch (IOException e) {\ne.printStackTrace();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic ExecCommand(String command) {\ntry {\np = Runtime.getRuntime().exec(makeArray(command));\nnew OutputReader().start();\nnew ErrorReader().start();\np.waitFor();\n} catch (IOException e) {\ne.printStackTrace();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic String getOutput() {\ntry {\noutputSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\nString value = output;\noutputSem.release();\nreturn value;\n}\n\npublic String getError() {\ntry {\nerrorSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\nString value = error;\nerrorSem.release();\nreturn value;\n}\n\nprivate String[] makeArray(String command) {\nArrayList&lt;String&gt; commandArray = new ArrayList&lt;String&gt;();\nString buff = &quot;&quot;;\nboolean lookForEnd = false;\nfor (int i = 0; i &lt; command.length(); i++) {\nif (lookForEnd) {\nif (command.charAt(i) == \'\\&quot;\') {\nif (buff.length() &gt; 0)\ncommandArray.add(buff);\nbuff = &quot;&quot;;\nlookForEnd = false;\n} else {\nbuff += command.charAt(i);\n}\n} else {\nif (command.charAt(i) == \'\\&quot;\') {\nlookForEnd = true;\n} else if (command.charAt(i) == \' \') {\nif (buff.length() &gt; 0)\ncommandArray.add(buff);\nbuff = &quot;&quot;;\n} else {\nbuff += command.charAt(i);\n}\n}\n}\nif (buff.length() &gt; 0)\ncommandArray.add(buff);\n\nString[] array = new String[commandArray.size()];\nfor (int i = 0; i &lt; commandArray.size(); i++) {\narray[i] = commandArray.get(i);\n}\n\nreturn array;\n}\n}\n</code></pre>\n','6','If you Google \"Runtime.exec hangs\" you will get alot of results. Executing\nexternal commands from Java seems very easy but it is actually very complicated.\nI was stuck at this problem for a long time before I finally got it. I never\nfound a complete solution so I tought a post like this may boost my user count\n=)\n\nIf you execute a command using Runtime.exec on Windows and wait for it to\ncomplete, like follows, your program may hang on waitFor.\n\nProcess p = Runtime.getRuntime().exec(\"myCommand\");\np.waitFor();\n\n\nThe reason for the hanging is the communication between Java and the external\noperating system process. And more specifically the communication buffers. The\nbuffers for STDERR and STDOUT has to be flushed for the program not to hang.\n\nFlushing these buffers are very easy, but! You can not flush them one by one,\nyou have to do it simultaniously for the program not to hang while flushing! So\nyou have to start at least one new thread to succeed.\n\nI have made a complete class that executes a command and serves the content of\nSTDOUT and STDERR. It also provides functionality for sending data to STDIN.\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.io.PrintWriter;\nimport java.util.ArrayList;\nimport java.util.concurrent.Semaphore;\n\npublic class ExecCommand {\nprivate Semaphore outputSem;\nprivate String output;\nprivate Semaphore errorSem;\nprivate String error;\nprivate Process p;\n\nprivate class InputWriter extends Thread {\nprivate String input;\n\npublic InputWriter(String input) {\nthis.input = input;\n}\n\npublic void run() {\nPrintWriter pw = new PrintWriter(p.getOutputStream());\npw.println(input);\npw.flush();\n}\n}\n\nprivate class OutputReader extends Thread {\npublic OutputReader() {\ntry {\noutputSem = new Semaphore(1);\noutputSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic void run() {\ntry {\nStringBuffer readBuffer = new StringBuffer();\nBufferedReader isr = new BufferedReader(new InputStreamReader(p\n.getInputStream()));\nString buff = new String();\nwhile ((buff = isr.readLine()) != null) {\nreadBuffer.append(buff);\nSystem.out.println(buff);\n}\noutput = readBuffer.toString();\noutputSem.release();\n} catch (IOException e) {\ne.printStackTrace();\n}\n}\n}\n\nprivate class ErrorReader extends Thread {\npublic ErrorReader() {\ntry {\nerrorSem = new Semaphore(1);\nerrorSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic void run() {\ntry {\nStringBuffer readBuffer = new StringBuffer();\nBufferedReader isr = new BufferedReader(new InputStreamReader(p\n.getErrorStream()));\nString buff = new String();\nwhile ((buff = isr.readLine()) != null) {\nreadBuffer.append(buff);\n}\nerror = readBuffer.toString();\nerrorSem.release();\n} catch (IOException e) {\ne.printStackTrace();\n}\nif (error.length() > 0)\nSystem.out.println(error);\n}\n}\n\npublic ExecCommand(String command, String input) {\ntry {\np = Runtime.getRuntime().exec(makeArray(command));\nnew InputWriter(input).start();\nnew OutputReader().start();\nnew ErrorReader().start();\np.waitFor();\n} catch (IOException e) {\ne.printStackTrace();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic ExecCommand(String command) {\ntry {\np = Runtime.getRuntime().exec(makeArray(command));\nnew OutputReader().start();\nnew ErrorReader().start();\np.waitFor();\n} catch (IOException e) {\ne.printStackTrace();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\n}\n\npublic String getOutput() {\ntry {\noutputSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\nString value = output;\noutputSem.release();\nreturn value;\n}\n\npublic String getError() {\ntry {\nerrorSem.acquire();\n} catch (InterruptedException e) {\ne.printStackTrace();\n}\nString value = error;\nerrorSem.release();\nreturn value;\n}\n\nprivate String[] makeArray(String command) {\nArrayList<String> commandArray = new ArrayList<String>();\nString buff = \"\";\nboolean lookForEnd = false;\nfor (int i = 0; i < command.length(); i++) {\nif (lookForEnd) {\nif (command.charAt(i) == \'\\\"\') {\nif (buff.length() > 0)\ncommandArray.add(buff);\nbuff = \"\";\nlookForEnd = false;\n} else {\nbuff += command.charAt(i);\n}\n} else {\nif (command.charAt(i) == \'\\\"\') {\nlookForEnd = true;\n} else if (command.charAt(i) == \' \') {\nif (buff.length() > 0)\ncommandArray.add(buff);\nbuff = \"\";\n} else {\nbuff += command.charAt(i);\n}\n}\n}\nif (buff.length() > 0)\ncommandArray.add(buff);\n\nString[] array = new String[commandArray.size()];\nfor (int i = 0; i < commandArray.size(); i++) {\narray[i] = commandArray.get(i);\n}\n\nreturn array;\n}\n}',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2013-10-19 09:02:28','2','2014-10-04 14:06:09','1','2013-10-19 09:02:28','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd918694','46517bb8-933d-4c44-a99d-0dfe8081301a','Introducing HTMLUnitGenerator','introducing-htmlunitgenerator','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I strongly support software testing. If you don\'t produce test cases that cover the requirements you implement, then you are not developing anything. It is impossible to reach sustainability, produce new functionality fast and with high quality or even cooperate with other developers, unless you work with automated test cases!\\n\\nBeing a web developer I\'ve been faced with problem of testing front end of web applications. When it comes to GUI and making sure its look and feel is as it should, the testing is done by a human resource. But when it comes to making sure flows can be executed without JavaScripts, CSS or backend crashes, or just making sure a text actually appears on a webpage, I want automated testing.\\nHTMLUnit\\n\\nHTMLUnit is a really nice tool. It is a head less web browser. This is perfect for creating test cases that clicks on different elements and does asserts on, for example, different attributes and values.\\n\\nThe problem is, we don\'t want to write those test cases in Java. Java is a general purpose programming language. The test cases will get messy and hard to read. They don\'t have to but the chances are they will.\\n\\nAnother problem is documentation of test cases. If we write test cases in Java we need to document them to make sure what flows are covered. This is extra important with front end testing, since these test can be very time consuming when testing a large web site.\\nHTMLUnitGenrerator\\n\\nI created an open source project to deal with these problems. I have created a domain specific language, DSL. You may say the test cases are automatically generated from the documentation, since the DSL is so easy to read and understand, it could just as well be documentation to a test case written in Java.\\n\\nHere is a sneak peak, check out GitHub for more examples.\\n```\\nSee testcases/BBBSeePaths.flow\\nSee testcases/BBBSeeUrls.flow\\nGo to baspaket and wait 2 seconds\\nFind a with attribute href set to /servlet/orderflow/search/search-flow?Id=tcm:142-23371 in campaignmodule\\nClick on campaignModuleChoose and wait 10 seconds\\nFind input with attribute id set to _eventId_search in searchpopup\\nFill in locationForm with _eventId as search and phoneNumber.fullNumber as 0768966787\\nClick on _eventId_search and wait 10 seconds\\nFind input with attribute id set to _eventId_search in searchpopup\\nFill in locationForm with address.floor as 3\\nClick on _eventId_search and wait 10 seconds\\nFind a with attribute href set to /orderflow/index.html?Id=tcm:142-23381&fromSearch&page=new in searchpopup\\n```\"}]],\"sections\":[[10,0]]}','<p>I strongly support software testing. If you don\'t produce test cases that cover the requirements you implement, then you are not developing anything. It is impossible to reach sustainability, produce new functionality fast and with high quality or even cooperate with other developers, unless you work with automated test cases!</p>\n<p>Being a web developer I\'ve been faced with problem of testing front end of web applications. When it comes to GUI and making sure its look and feel is as it should, the testing is done by a human resource. But when it comes to making sure flows can be executed without JavaScripts, CSS or backend crashes, or just making sure a text actually appears on a webpage, I want automated testing.<br>\nHTMLUnit</p>\n<p>HTMLUnit is a really nice tool. It is a head less web browser. This is perfect for creating test cases that clicks on different elements and does asserts on, for example, different attributes and values.</p>\n<p>The problem is, we don\'t want to write those test cases in Java. Java is a general purpose programming language. The test cases will get messy and hard to read. They don\'t have to but the chances are they will.</p>\n<p>Another problem is documentation of test cases. If we write test cases in Java we need to document them to make sure what flows are covered. This is extra important with front end testing, since these test can be very time consuming when testing a large web site.<br>\nHTMLUnitGenrerator</p>\n<p>I created an open source project to deal with these problems. I have created a domain specific language, DSL. You may say the test cases are automatically generated from the documentation, since the DSL is so easy to read and understand, it could just as well be documentation to a test case written in Java.</p>\n<p>Here is a sneak peak, check out GitHub for more examples.</p>\n<pre><code>See testcases/BBBSeePaths.flow\nSee testcases/BBBSeeUrls.flow\nGo to baspaket and wait 2 seconds\nFind a with attribute href set to /servlet/orderflow/search/search-flow?Id=tcm:142-23371 in campaignmodule\nClick on campaignModuleChoose and wait 10 seconds\nFind input with attribute id set to _eventId_search in searchpopup\nFill in locationForm with _eventId as search and phoneNumber.fullNumber as 0768966787\nClick on _eventId_search and wait 10 seconds\nFind input with attribute id set to _eventId_search in searchpopup\nFill in locationForm with address.floor as 3\nClick on _eventId_search and wait 10 seconds\nFind a with attribute href set to /orderflow/index.html?Id=tcm:142-23381&amp;fromSearch&amp;page=new in searchpopup\n</code></pre>\n','7','I strongly support software testing. If you don\'t produce test cases that cover\nthe requirements you implement, then you are not developing anything. It is\nimpossible to reach sustainability, produce new functionality fast and with high\nquality or even cooperate with other developers, unless you work with automated\ntest cases!\n\nBeing a web developer I\'ve been faced with problem of testing front end of web\napplications. When it comes to GUI and making sure its look and feel is as it\nshould, the testing is done by a human resource. But when it comes to making\nsure flows can be executed without JavaScripts, CSS or backend crashes, or just\nmaking sure a text actually appears on a webpage, I want automated testing.\nHTMLUnit\n\nHTMLUnit is a really nice tool. It is a head less web browser. This is perfect\nfor creating test cases that clicks on different elements and does asserts on,\nfor example, different attributes and values.\n\nThe problem is, we don\'t want to write those test cases in Java. Java is a\ngeneral purpose programming language. The test cases will get messy and hard to\nread. They don\'t have to but the chances are they will.\n\nAnother problem is documentation of test cases. If we write test cases in Java\nwe need to document them to make sure what flows are covered. This is extra\nimportant with front end testing, since these test can be very time consuming\nwhen testing a large web site.\nHTMLUnitGenrerator\n\nI created an open source project to deal with these problems. I have created a\ndomain specific language, DSL. You may say the test cases are automatically\ngenerated from the documentation, since the DSL is so easy to read and\nunderstand, it could just as well be documentation to a test case written in\nJava.\n\nHere is a sneak peak, check out GitHub for more examples.\n\nSee testcases/BBBSeePaths.flow\nSee testcases/BBBSeeUrls.flow\nGo to baspaket and wait 2 seconds\nFind a with attribute href set to /servlet/orderflow/search/search-flow?Id=tcm:142-23371 in campaignmodule\nClick on campaignModuleChoose and wait 10 seconds\nFind input with attribute id set to _eventId_search in searchpopup\nFill in locationForm with _eventId as search and phoneNumber.fullNumber as 0768966787\nClick on _eventId_search and wait 10 seconds\nFind input with attribute id set to _eventId_search in searchpopup\nFill in locationForm with address.floor as 3\nClick on _eventId_search and wait 10 seconds\nFind a with attribute href set to /orderflow/index.html?Id=tcm:142-23381&fromSearch&page=new in searchpopup',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2013-10-19 09:03:22','2','2014-10-04 14:05:47','1','2013-10-19 09:03:22','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd918696','88f69eb1-256c-40a9-8eab-8133b22f0fa9','Owncloud 5 Server with Lighttpd, Sqlite on ArchLinux','owncloud-5-server-with-lighttpd-sqlite-on-archlinux','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I just installed Owncloud Server on my Raspberry PI. It was a bit harder then I had expected so I\'ll summarize how I did it and the issues I had. I\'m using Raspberry PI (512Mb model), ArchLinux, Lighttpd 1.4.32, SQLite 3.7.17.\\n##Lighttpd\\n\\nFirst, install it.\\n```\\npacman -S lighttpd\\n```\\n\\nThe Lighttpd config file exists in `/etc/lighttpd/lighttpd.conf`. I made alot of changes here it\'s probably easiest just to show the whole file.\\n```\\nserver.port = 80\\nserver.username = \\\"http\\\"\\nserver.groupname = \\\"http\\\"\\nserver.document-root = \\\"/srv/http\\\"\\nserver.errorlog = \\\"/var/log/lighttpd/error.log\\\"\\ndir-listing.activate = \\\"enable\\\"\\nindex-file.names = ( \\\"index.html\\\", \\\"index.php\\\" )\\nstatic-file.exclude-extensions = (\\\".php\\\", \\\".pl\\\", \\\".fcgi\\\")\\nmimetype.assign = ( \\\".html\\\" => \\\"text/html\\\", \\\".txt\\\" => \\\"text/plain\\\", \\\".jpg\\\" => \\\"image/jpeg\\\", \\\".png\\\" => \\\"image/png\\\", \\\"\\\" => \\\"application/octet-stream\\\" )\\n#ssl.engine = \\\"enable\\\"\\n\\nssl.pemfile = \\\"/etc/ssl/owncloud/server.pem\\\"\\n\\nserver.modules = (\\n\\\"mod_access\\\",\\n\\\"mod_fastcgi\\\",\\n\\\"mod_cgi\\\"\\n)\\n\\n$HTTP[\\\"url\\\"] =~ \\\"^/owncloud/data/\\\" {\\nurl.access-deny = (\\\"\\\")\\n}\\n$HTTP[\\\"url\\\"] =~ \\\"^/owncloud($|/)\\\" {\\ndir-listing.activate = \\\"disable\\\"\\n}\\n\\nfastcgi.server = (\\n\\\".php\\\" => ((\\n\\\"bin-path\\\" => \\\"/usr/bin/php-cgi\\\",\\n\\\"socket\\\" => \\\"/tmp/php.socket\\\",\\n\\\"max-procs\\\" => 4\\n))\\n)\\n```\\n\\nYou can test the Lighttpd configuration like this.\\n```\\nlighttpd -t -f /etc/lighttpd/lighttpd.conf\\n```\\n##PHP\\n\\nInstall like this.\\n```\\npacman -S php-cgi\\n```\\n\\nThe configuration exists in `/etc/php/php.ini`. There are a number of settings to change here. Here are the extensions I found was neccessary for Owncloud.\\n```\\nextension=curl.so\\nextension=gd.so\\nextension=iconv.so\\nextension=openssl.so\\nextension=pdo_sqlite.so\\nextension=sqlite3.so\\nextension=zip.so\\n```\\n\\nThe document root has to be set.\\n```\\ndoc_root = /srv/http\\n```\\n\\nI don\'t know exactly what this is, but it is needed =)\\n```\\ncgi.fix_pathinfo=1\\n```\\n##OpenSSL\\n```\\npacman -S openssl\\nmkdir /etc/ssl/owncloud\\ncd /etc/ssl/owncloud\\nopenssl req -new -x509 -keyout server.pem -out server.pem -days 365 -nodes\\nchmod 0600 /etc/ssl/owncloud\\n```\\n##Install Owncloud\\n\\nInstall and link the application to make it available in webserver. I like this solution better then copying the files. Now it\'s easier to keep it updated with Pacman.\\n```\\npacman -S owncloud\\nln -s /usr/share/webapps/owncloud /srv/http/owncloud\\n```\\n\\nTime to start the webserver!\\n```\\nlighttpd -t -f /etc/lighttpd/lighttpd.conf #This test can save you some time!\\nsystemctl restart lighttpd.service\\n```\\n\\nNow you can surf into /owncloud and the application should be running. You will most likely get a response similar to this.\\nNo database drivers (sqlite, mysql, or postgresql) installed.\\nPHP module zip not installed.\\n\\n```\\nPlease ask your server administrator to install the module.\\nPHP module GD is not installed.\\n\\nPlease ask your server administrator to install the module.\\nPHP module iconv is not installed.\\n\\nPlease ask your server administrator to install the module.\\n```\\n\\nI installed them one by one, and then restarted Lighttpd to see that they got installed correctly. Like php-sqlite for example.\\n```\\npacman -Ss sqlite #search for sqlite\\npacman -S extra/php-sqlite #install php-sqlite\\n```\\n\\nIt may be that they are just not enabled in `php.ini` extensions section, and/or that they are not even intalled. Like I had PHP-GD installed but had missed `;extension=gd.so` in `php.ini`.\\n\\nWhen all extensions are in place, you will see two input fields where you input admin user and password. Once entered it will take a very long time to submit the form! I actually gave up and did a restart of Lighttpd. After that it loaded faster.\\nIssues\\n\\nIf you have issues, there are helpful logs.\\n```\\ntail -f /srv/http/owncloud/data/owncloud.log\\ntail -f /var/log/lighttpd/error.log\\n```\\n\\nFirst issue I had was that the first page got stuck hanging on \\\"Upgrading filesystem cache\\\". Looking at the log I found.\\n```\\n2013-07-22 18:28:35: (mod_fastcgi.c.2676) FastCGI-stderr: PHP Fatal error: Call to a member function raiseError() on a non-object in /usr/share/webapps/owncloud/lib/MDB2/Driver/sqlite3.php on line 898\\n```\\n\\nThis is very easy to solve, just uncomment \\n```\\n\\\"extension=pdo_sqlite.so\\\" in php.ini.\\nextension=pdo_sqlite.so\\n```\\n\\nSecond problem was admin got stuck. I got a tip that it could be PHP not being able to lookup `apps.owncloud.com` so I tried this.\\n```\\necho \\\"<?php echo gethostbyname(\'apps.owncloud.com\'); ?&rt;\\\" > /srv/http/dns.php\\n```\\n\\nAnd the dns.php page also got stuck. I added the entry in `/etc/hosts` like this, not nice but it works for now.\\n```\\n188.138.118.86 apps.owncloud.com apps.owncloud.com\\n```\\n\\nHowever, the admin page was not accessble, it also hang. In the `owncloud.log` I saw.\\n```\\n{\\\"app\\\":\\\"PHP\\\",\\\"message\\\":\\\"curl_setopt_array(): CURLOPT_FOLLOWLOCATION cannot be activated when an open_basedir is set at \\\\/usr\\\\/share\\\\/webapps\\\\/owncloud\\\\/3rdparty\\\\/Sabre\\\\/DAV\\\\/Client.php#462\\\",\\\"level\\\":2,\\\"time\\\":814}\\n```\\n\\nI fixed that by commenting out open_basedir in php.ini.\\n```\\n;open_basedir = ...\\n```\\n\\nThe third thing, that definitly finally did the trick, was `max-procs` parameter in Lighttpd config. I had it set to `1` and I changed it to `4`.\\n\\nLast I recommend looking at https://forum.owncloud.org/viewtopic.php?f=8&t=10692 for performance tips.\\n\\n\"}]],\"sections\":[[10,0]]}','<p>I just installed Owncloud Server on my Raspberry PI. It was a bit harder then I had expected so I\'ll summarize how I did it and the issues I had. I\'m using Raspberry PI (512Mb model), ArchLinux, Lighttpd 1.4.32, SQLite 3.7.17.</p>\n<h2 id=\"lighttpd\">Lighttpd</h2>\n<p>First, install it.</p>\n<pre><code>pacman -S lighttpd\n</code></pre>\n<p>The Lighttpd config file exists in <code>/etc/lighttpd/lighttpd.conf</code>. I made alot of changes here it\'s probably easiest just to show the whole file.</p>\n<pre><code>server.port = 80\nserver.username = &quot;http&quot;\nserver.groupname = &quot;http&quot;\nserver.document-root = &quot;/srv/http&quot;\nserver.errorlog = &quot;/var/log/lighttpd/error.log&quot;\ndir-listing.activate = &quot;enable&quot;\nindex-file.names = ( &quot;index.html&quot;, &quot;index.php&quot; )\nstatic-file.exclude-extensions = (&quot;.php&quot;, &quot;.pl&quot;, &quot;.fcgi&quot;)\nmimetype.assign = ( &quot;.html&quot; =&gt; &quot;text/html&quot;, &quot;.txt&quot; =&gt; &quot;text/plain&quot;, &quot;.jpg&quot; =&gt; &quot;image/jpeg&quot;, &quot;.png&quot; =&gt; &quot;image/png&quot;, &quot;&quot; =&gt; &quot;application/octet-stream&quot; )\n#ssl.engine = &quot;enable&quot;\n\nssl.pemfile = &quot;/etc/ssl/owncloud/server.pem&quot;\n\nserver.modules = (\n&quot;mod_access&quot;,\n&quot;mod_fastcgi&quot;,\n&quot;mod_cgi&quot;\n)\n\n$HTTP[&quot;url&quot;] =~ &quot;^/owncloud/data/&quot; {\nurl.access-deny = (&quot;&quot;)\n}\n$HTTP[&quot;url&quot;] =~ &quot;^/owncloud($|/)&quot; {\ndir-listing.activate = &quot;disable&quot;\n}\n\nfastcgi.server = (\n&quot;.php&quot; =&gt; ((\n&quot;bin-path&quot; =&gt; &quot;/usr/bin/php-cgi&quot;,\n&quot;socket&quot; =&gt; &quot;/tmp/php.socket&quot;,\n&quot;max-procs&quot; =&gt; 4\n))\n)\n</code></pre>\n<p>You can test the Lighttpd configuration like this.</p>\n<pre><code>lighttpd -t -f /etc/lighttpd/lighttpd.conf\n</code></pre>\n<h2 id=\"php\">PHP</h2>\n<p>Install like this.</p>\n<pre><code>pacman -S php-cgi\n</code></pre>\n<p>The configuration exists in <code>/etc/php/php.ini</code>. There are a number of settings to change here. Here are the extensions I found was neccessary for Owncloud.</p>\n<pre><code>extension=curl.so\nextension=gd.so\nextension=iconv.so\nextension=openssl.so\nextension=pdo_sqlite.so\nextension=sqlite3.so\nextension=zip.so\n</code></pre>\n<p>The document root has to be set.</p>\n<pre><code>doc_root = /srv/http\n</code></pre>\n<p>I don\'t know exactly what this is, but it is needed =)</p>\n<pre><code>cgi.fix_pathinfo=1\n</code></pre>\n<h2 id=\"openssl\">OpenSSL</h2>\n<pre><code>pacman -S openssl\nmkdir /etc/ssl/owncloud\ncd /etc/ssl/owncloud\nopenssl req -new -x509 -keyout server.pem -out server.pem -days 365 -nodes\nchmod 0600 /etc/ssl/owncloud\n</code></pre>\n<h2 id=\"installowncloud\">Install Owncloud</h2>\n<p>Install and link the application to make it available in webserver. I like this solution better then copying the files. Now it\'s easier to keep it updated with Pacman.</p>\n<pre><code>pacman -S owncloud\nln -s /usr/share/webapps/owncloud /srv/http/owncloud\n</code></pre>\n<p>Time to start the webserver!</p>\n<pre><code>lighttpd -t -f /etc/lighttpd/lighttpd.conf #This test can save you some time!\nsystemctl restart lighttpd.service\n</code></pre>\n<p>Now you can surf into /owncloud and the application should be running. You will most likely get a response similar to this.<br>\nNo database drivers (sqlite, mysql, or postgresql) installed.<br>\nPHP module zip not installed.</p>\n<pre><code>Please ask your server administrator to install the module.\nPHP module GD is not installed.\n\nPlease ask your server administrator to install the module.\nPHP module iconv is not installed.\n\nPlease ask your server administrator to install the module.\n</code></pre>\n<p>I installed them one by one, and then restarted Lighttpd to see that they got installed correctly. Like php-sqlite for example.</p>\n<pre><code>pacman -Ss sqlite #search for sqlite\npacman -S extra/php-sqlite #install php-sqlite\n</code></pre>\n<p>It may be that they are just not enabled in <code>php.ini</code> extensions section, and/or that they are not even intalled. Like I had PHP-GD installed but had missed <code>;extension=gd.so</code> in <code>php.ini</code>.</p>\n<p>When all extensions are in place, you will see two input fields where you input admin user and password. Once entered it will take a very long time to submit the form! I actually gave up and did a restart of Lighttpd. After that it loaded faster.<br>\nIssues</p>\n<p>If you have issues, there are helpful logs.</p>\n<pre><code>tail -f /srv/http/owncloud/data/owncloud.log\ntail -f /var/log/lighttpd/error.log\n</code></pre>\n<p>First issue I had was that the first page got stuck hanging on &quot;Upgrading filesystem cache&quot;. Looking at the log I found.</p>\n<pre><code>2013-07-22 18:28:35: (mod_fastcgi.c.2676) FastCGI-stderr: PHP Fatal error: Call to a member function raiseError() on a non-object in /usr/share/webapps/owncloud/lib/MDB2/Driver/sqlite3.php on line 898\n</code></pre>\n<p>This is very easy to solve, just uncomment</p>\n<pre><code>&quot;extension=pdo_sqlite.so&quot; in php.ini.\nextension=pdo_sqlite.so\n</code></pre>\n<p>Second problem was admin got stuck. I got a tip that it could be PHP not being able to lookup <code>apps.owncloud.com</code> so I tried this.</p>\n<pre><code>echo &quot;&lt;?php echo gethostbyname(\'apps.owncloud.com\'); ?&amp;rt;&quot; &gt; /srv/http/dns.php\n</code></pre>\n<p>And the dns.php page also got stuck. I added the entry in <code>/etc/hosts</code> like this, not nice but it works for now.</p>\n<pre><code>188.138.118.86 apps.owncloud.com apps.owncloud.com\n</code></pre>\n<p>However, the admin page was not accessble, it also hang. In the <code>owncloud.log</code> I saw.</p>\n<pre><code>{&quot;app&quot;:&quot;PHP&quot;,&quot;message&quot;:&quot;curl_setopt_array(): CURLOPT_FOLLOWLOCATION cannot be activated when an open_basedir is set at \\/usr\\/share\\/webapps\\/owncloud\\/3rdparty\\/Sabre\\/DAV\\/Client.php#462&quot;,&quot;level&quot;:2,&quot;time&quot;:814}\n</code></pre>\n<p>I fixed that by commenting out open_basedir in php.ini.</p>\n<pre><code>;open_basedir = ...\n</code></pre>\n<p>The third thing, that definitly finally did the trick, was <code>max-procs</code> parameter in Lighttpd config. I had it set to <code>1</code> and I changed it to <code>4</code>.</p>\n<p>Last I recommend looking at <a href=\"https://forum.owncloud.org/viewtopic.php?f=8&amp;t=10692\">https://forum.owncloud.org/viewtopic.php?f=8&amp;t=10692</a> for performance tips.</p>\n','9','I just installed Owncloud Server on my Raspberry PI. It was a bit harder then I\nhad expected so I\'ll summarize how I did it and the issues I had. I\'m using\nRaspberry PI (512Mb model), ArchLinux, Lighttpd 1.4.32, SQLite 3.7.17.\n\nLighttpd\nFirst, install it.\n\npacman -S lighttpd\n\n\nThe Lighttpd config file exists in /etc/lighttpd/lighttpd.conf. I made alot of\nchanges here it\'s probably easiest just to show the whole file.\n\nserver.port = 80\nserver.username = \"http\"\nserver.groupname = \"http\"\nserver.document-root = \"/srv/http\"\nserver.errorlog = \"/var/log/lighttpd/error.log\"\ndir-listing.activate = \"enable\"\nindex-file.names = ( \"index.html\", \"index.php\" )\nstatic-file.exclude-extensions = (\".php\", \".pl\", \".fcgi\")\nmimetype.assign = ( \".html\" => \"text/html\", \".txt\" => \"text/plain\", \".jpg\" => \"image/jpeg\", \".png\" => \"image/png\", \"\" => \"application/octet-stream\" )\n#ssl.engine = \"enable\"\n\nssl.pemfile = \"/etc/ssl/owncloud/server.pem\"\n\nserver.modules = (\n\"mod_access\",\n\"mod_fastcgi\",\n\"mod_cgi\"\n)\n\n$HTTP[\"url\"] =~ \"^/owncloud/data/\" {\nurl.access-deny = (\"\")\n}\n$HTTP[\"url\"] =~ \"^/owncloud($|/)\" {\ndir-listing.activate = \"disable\"\n}\n\nfastcgi.server = (\n\".php\" => ((\n\"bin-path\" => \"/usr/bin/php-cgi\",\n\"socket\" => \"/tmp/php.socket\",\n\"max-procs\" => 4\n))\n)\n\n\nYou can test the Lighttpd configuration like this.\n\nlighttpd -t -f /etc/lighttpd/lighttpd.conf\n\n\nPHP\nInstall like this.\n\npacman -S php-cgi\n\n\nThe configuration exists in /etc/php/php.ini. There are a number of settings to\nchange here. Here are the extensions I found was neccessary for Owncloud.\n\nextension=curl.so\nextension=gd.so\nextension=iconv.so\nextension=openssl.so\nextension=pdo_sqlite.so\nextension=sqlite3.so\nextension=zip.so\n\n\nThe document root has to be set.\n\ndoc_root = /srv/http\n\n\nI don\'t know exactly what this is, but it is needed =)\n\ncgi.fix_pathinfo=1\n\n\nOpenSSL\npacman -S openssl\nmkdir /etc/ssl/owncloud\ncd /etc/ssl/owncloud\nopenssl req -new -x509 -keyout server.pem -out server.pem -days 365 -nodes\nchmod 0600 /etc/ssl/owncloud\n\n\nInstall Owncloud\nInstall and link the application to make it available in webserver. I like this\nsolution better then copying the files. Now it\'s easier to keep it updated with\nPacman.\n\npacman -S owncloud\nln -s /usr/share/webapps/owncloud /srv/http/owncloud\n\n\nTime to start the webserver!\n\nlighttpd -t -f /etc/lighttpd/lighttpd.conf #This test can save you some time!\nsystemctl restart lighttpd.service\n\n\nNow you can surf into /owncloud and the application should be running. You will\nmost likely get a response similar to this.\nNo database drivers (sqlite, mysql, or postgresql) installed.\nPHP module zip not installed.\n\nPlease ask your server administrator to install the module.\nPHP module GD is not installed.\n\nPlease ask your server administrator to install the module.\nPHP module iconv is not installed.\n\nPlease ask your server administrator to install the module.\n\n\nI installed them one by one, and then restarted Lighttpd to see that they got\ninstalled correctly. Like php-sqlite for example.\n\npacman -Ss sqlite #search for sqlite\npacman -S extra/php-sqlite #install php-sqlite\n\n\nIt may be that they are just not enabled in php.ini  extensions section, and/or\nthat they are not even intalled. Like I had PHP-GD installed but had missed \n;extension=gd.so  in php.ini.\n\nWhen all extensions are in place, you will see two input fields where you input\nadmin user and password. Once entered it will take a very long time to submit\nthe form! I actually gave up and did a restart of Lighttpd. After that it loaded\nfaster.\nIssues\n\nIf you have issues, there are helpful logs.\n\ntail -f /srv/http/owncloud/data/owncloud.log\ntail -f /var/log/lighttpd/error.log\n\n\nFirst issue I had was that the first page got stuck hanging on \"Upgrading\nfilesystem cache\". Looking at the log I found.\n\n2013-07-22 18:28:35: (mod_fastcgi.c.2676) FastCGI-stderr: PHP Fatal error: Call to a member function raiseError() on a non-object in /usr/share/webapps/owncloud/lib/MDB2/Driver/sqlite3.php on line 898\n\n\nThis is very easy to solve, just uncomment\n\n\"extension=pdo_sqlite.so\" in php.ini.\nextension=pdo_sqlite.so\n\n\nSecond problem was admin got stuck. I got a tip that it could be PHP not being\nable to lookup apps.owncloud.com  so I tried this.\n\necho \"<?php echo gethostbyname(\'apps.owncloud.com\'); ?&rt;\" > /srv/http/dns.php\n\n\nAnd the dns.php page also got stuck. I added the entry in /etc/hosts  like this,\nnot nice but it works for now.\n\n188.138.118.86 apps.owncloud.com apps.owncloud.com\n\n\nHowever, the admin page was not accessble, it also hang. In the owncloud.log  I\nsaw.\n\n{\"app\":\"PHP\",\"message\":\"curl_setopt_array(): CURLOPT_FOLLOWLOCATION cannot be activated when an open_basedir is set at \\/usr\\/share\\/webapps\\/owncloud\\/3rdparty\\/Sabre\\/DAV\\/Client.php#462\",\"level\":2,\"time\":814}\n\n\nI fixed that by commenting out open_basedir in php.ini.\n\n;open_basedir = ...\n\n\nThe third thing, that definitly finally did the trick, was max-procs  parameter\nin Lighttpd config. I had it set to 1  and I changed it to 4.\n\nLast I recommend looking at https://forum.owncloud.org/viewtopic.php?f=8&t=10692\n[https://forum.owncloud.org/viewtopic.php?f=8&t=10692]  for performance tips.',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2013-10-19 09:09:54','2','2014-10-04 14:04:45','1','2013-10-19 09:09:54','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd918697','be4ce0c2-9ae0-489e-a6fd-fb2d6586fa7d','Raspberry PI with Camera Module and ArchLinux','raspberry-pi-with-camera-module-and-archlinux','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I reccently bought a Raspberry PI. I am new to ArchLinux so I thought I\'d make some notes of the initial setup. This is the hardware used to build the camera:\\n\\n* Raspberry Pi Model B 512MB RAM\\n* Camera Module for Raspberry Pi\\n* WiFi USB Nano\\n* OpenBox Sky Case\\n* 16GB SD\\n\\n##Partitioning SD card\\n\\nI added the ArchLinux image to a 16GB card. The images creates a 2GB parition so you need to either extend it before booting or create a new partition after boot. I choose to create a new partition after boot, and use it for `/home`.\\n```\\nfdisk /dev/disk/by-id/mmc-*\\nn add partition\\nw save and exit\\n```\\n\\n##Create file system.\\n```\\nmkfs.ext4 /dev/mmcblk0p3\\n```\\n\\nRemove everything in `/home`. I am assuming you have nothing important here yet.\\n```\\nrm -rf /home/*\\n```\\n\\nMount the device at boot.\\n```\\n/dev/mmcblk0p3 /home ext4 defaults 0 2\\n```\\n##Package manager\\n\\nI had never heard of Pacman package manager before. But its just as easy. To search for packages do this.\\n```\\npacman -Ss partOfPackageName\\n```\\n\\nTo install a package do this.\\n```\\npacman -S packageName\\n```\\n\\nUpgrade all packages.\\n```\\npacman -Syu\\n```\\n\\nInitiallly I had some problems when installing packages. I got this.\\n```\\n\\\"from mirror.archlinuxarm.org : The requested URL returned error: 404 Not Found\\\"\\n```\\n\\nJust perform a full uppgrade and it should be resolved.\\n##Network setup\\n\\nTo list available devices do.\\n```\\nip link\\n```\\n\\nTo get the wireless network running I installed these.\\n```\\npacman -S wpa_supplicant\\npacman -S wpa_actiond\\npacman -S ifplugd\\npacman -S dhclient\\npacman -S openntpd\\n```\\n\\nThere is a very nice setup wizard, just do this.\\n```\\nwifi-menu -o\\n```\\n\\nNetwork configuration exists in profile files. I had some problems with the DHCP client not setting IP:s after network loss. I added `DHPClient` to the profiles to make it reconnect.\\n```\\n#/etc/netctl/ethernet-dhcp\\nDescription=\'A basic dhcp ethernet connection\'\\nInterface=eth0\\nConnection=ethernet\\nIP=dhcp\\nDHCPClient=\'dhclient\'\\nTimeoutDHCP=30\\nExecUpPost=\\\"ntpd -s &\\\"\\n```\\n\\n```\\n#/etc/netctl/wlan0-tallefjantlinksys\\nDescription=\'Automatically generated profile by wifi-menu\'\\nInterface=wlan0\\nConnection=wireless\\nSecurity=wpa\\nESSID=MYNETWORKNAME\\nIP=dhcp\\nKey=12312312312313123132\\nDHCPClient=\'dhclient\'\\nTimeoutDHCP=30\\nExecUpPost=\\\"ntpd -s &\\\"\\n```\\n\\nTo make WLAN connect when available, and Ethernet connect when plugged, do this.\\n```\\nsystemctl enable netctl-auto@wlan0.service\\nsystemctl enable netctl-ifplugd@eth00.service\\n```\\n\\nTo make WLAN and Ethernet use DHCP at boot, do this.\\n```\\nsystemctl enable dhcpcd@eth0\\nsystemctl enable dhcpcd@wlan0\\n```\\n\\nI noted that the network setup was not working after the first initual upgrade with Pacman. To solve it I just disabled the enabled services and enabled them again.\\n```\\nsystemctl status --failed\\nsystemctl disable FAILEDSERVICE\\n```\\nI had problems with DHCP timeouts so I added the `TimeoutDHCP` parameter and set it to 30. Default is 10 seconds. Also the `ExecUpPost` will set time from NTP when connected.\\n##Time and date\\n\\nIn my case, Sweden.\\n```\\nrm /etc/timezone\\nln -s /usr/share/zoneinfo/Europe/Stockholm /etc/timezone\\n```\\nAlso, the Raspberry Pi has no battery. The time will be 1970 on every reboot. You can use NTP to set the date from a time server on startup.\\n```\\npacman -S openntpd\\n```\\n\\nTo start ntpd when a network interface is connected, add `ExecUpPost` to your interface profile. Here is an example of my eth0.\\n```\\nDescription=\'A basic dhcp ethernet connection\'\\nInterface=eth0\\nConnection=ethernet\\nIP=dhcp\\nExecUpPost=\\\"ntpd -s &\\\"\\nRaspberry Camera\\n```\\n\\n##Camera module software in PATH\\nIn `~/.bashrc` I added.\\n```\\nexport PATH=$PATH:/opt/vc/bin\\n```\\n\\n##Tweak camera memory usage\\nIn `/boot/config.txt` I added.\\n```\\ngpu_mem=128\\nstart_file=start_x.elf\\nfixup_file=fixup_x.dat\\n```\\n\\n##Speed up camera\\nThe camera may seem slow. There is a default delay of 5 seconds before it takes the picture. You can change this with `-t 0`.\\nI\'ve noticed the device hangs when storing larger videos. It\'s a good idea to record video directly to RAM, that works much better for me. By default `/tmp` is mounted as `tmpfs`.\\n```\\n/opt/vc/bin/raspistill -t 0 -o /tmp/test.png\\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 5000\\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 20000\\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 60000\\n```\"}]],\"sections\":[[10,0]]}','<p>I reccently bought a Raspberry PI. I am new to ArchLinux so I thought I\'d make some notes of the initial setup. This is the hardware used to build the camera:</p>\n<ul>\n<li>Raspberry Pi Model B 512MB RAM</li>\n<li>Camera Module for Raspberry Pi</li>\n<li>WiFi USB Nano</li>\n<li>OpenBox Sky Case</li>\n<li>16GB SD</li>\n</ul>\n<h2 id=\"partitioningsdcard\">Partitioning SD card</h2>\n<p>I added the ArchLinux image to a 16GB card. The images creates a 2GB parition so you need to either extend it before booting or create a new partition after boot. I choose to create a new partition after boot, and use it for <code>/home</code>.</p>\n<pre><code>fdisk /dev/disk/by-id/mmc-*\nn add partition\nw save and exit\n</code></pre>\n<h2 id=\"createfilesystem\">Create file system.</h2>\n<pre><code>mkfs.ext4 /dev/mmcblk0p3\n</code></pre>\n<p>Remove everything in <code>/home</code>. I am assuming you have nothing important here yet.</p>\n<pre><code>rm -rf /home/*\n</code></pre>\n<p>Mount the device at boot.</p>\n<pre><code>/dev/mmcblk0p3 /home ext4 defaults 0 2\n</code></pre>\n<h2 id=\"packagemanager\">Package manager</h2>\n<p>I had never heard of Pacman package manager before. But its just as easy. To search for packages do this.</p>\n<pre><code>pacman -Ss partOfPackageName\n</code></pre>\n<p>To install a package do this.</p>\n<pre><code>pacman -S packageName\n</code></pre>\n<p>Upgrade all packages.</p>\n<pre><code>pacman -Syu\n</code></pre>\n<p>Initiallly I had some problems when installing packages. I got this.</p>\n<pre><code>&quot;from mirror.archlinuxarm.org : The requested URL returned error: 404 Not Found&quot;\n</code></pre>\n<p>Just perform a full uppgrade and it should be resolved.</p>\n<h2 id=\"networksetup\">Network setup</h2>\n<p>To list available devices do.</p>\n<pre><code>ip link\n</code></pre>\n<p>To get the wireless network running I installed these.</p>\n<pre><code>pacman -S wpa_supplicant\npacman -S wpa_actiond\npacman -S ifplugd\npacman -S dhclient\npacman -S openntpd\n</code></pre>\n<p>There is a very nice setup wizard, just do this.</p>\n<pre><code>wifi-menu -o\n</code></pre>\n<p>Network configuration exists in profile files. I had some problems with the DHCP client not setting IP:s after network loss. I added <code>DHPClient</code> to the profiles to make it reconnect.</p>\n<pre><code>#/etc/netctl/ethernet-dhcp\nDescription=\'A basic dhcp ethernet connection\'\nInterface=eth0\nConnection=ethernet\nIP=dhcp\nDHCPClient=\'dhclient\'\nTimeoutDHCP=30\nExecUpPost=&quot;ntpd -s &amp;&quot;\n</code></pre>\n<pre><code>#/etc/netctl/wlan0-tallefjantlinksys\nDescription=\'Automatically generated profile by wifi-menu\'\nInterface=wlan0\nConnection=wireless\nSecurity=wpa\nESSID=MYNETWORKNAME\nIP=dhcp\nKey=12312312312313123132\nDHCPClient=\'dhclient\'\nTimeoutDHCP=30\nExecUpPost=&quot;ntpd -s &amp;&quot;\n</code></pre>\n<p>To make WLAN connect when available, and Ethernet connect when plugged, do this.</p>\n<pre><code>systemctl enable netctl-auto@wlan0.service\nsystemctl enable netctl-ifplugd@eth00.service\n</code></pre>\n<p>To make WLAN and Ethernet use DHCP at boot, do this.</p>\n<pre><code>systemctl enable dhcpcd@eth0\nsystemctl enable dhcpcd@wlan0\n</code></pre>\n<p>I noted that the network setup was not working after the first initual upgrade with Pacman. To solve it I just disabled the enabled services and enabled them again.</p>\n<pre><code>systemctl status --failed\nsystemctl disable FAILEDSERVICE\n</code></pre>\n<p>I had problems with DHCP timeouts so I added the <code>TimeoutDHCP</code> parameter and set it to 30. Default is 10 seconds. Also the <code>ExecUpPost</code> will set time from NTP when connected.</p>\n<h2 id=\"timeanddate\">Time and date</h2>\n<p>In my case, Sweden.</p>\n<pre><code>rm /etc/timezone\nln -s /usr/share/zoneinfo/Europe/Stockholm /etc/timezone\n</code></pre>\n<p>Also, the Raspberry Pi has no battery. The time will be 1970 on every reboot. You can use NTP to set the date from a time server on startup.</p>\n<pre><code>pacman -S openntpd\n</code></pre>\n<p>To start ntpd when a network interface is connected, add <code>ExecUpPost</code> to your interface profile. Here is an example of my eth0.</p>\n<pre><code>Description=\'A basic dhcp ethernet connection\'\nInterface=eth0\nConnection=ethernet\nIP=dhcp\nExecUpPost=&quot;ntpd -s &amp;&quot;\nRaspberry Camera\n</code></pre>\n<h2 id=\"cameramodulesoftwareinpath\">Camera module software in PATH</h2>\n<p>In <code>~/.bashrc</code> I added.</p>\n<pre><code>export PATH=$PATH:/opt/vc/bin\n</code></pre>\n<h2 id=\"tweakcameramemoryusage\">Tweak camera memory usage</h2>\n<p>In <code>/boot/config.txt</code> I added.</p>\n<pre><code>gpu_mem=128\nstart_file=start_x.elf\nfixup_file=fixup_x.dat\n</code></pre>\n<h2 id=\"speedupcamera\">Speed up camera</h2>\n<p>The camera may seem slow. There is a default delay of 5 seconds before it takes the picture. You can change this with <code>-t 0</code>.<br>\nI\'ve noticed the device hangs when storing larger videos. It\'s a good idea to record video directly to RAM, that works much better for me. By default <code>/tmp</code> is mounted as <code>tmpfs</code>.</p>\n<pre><code>/opt/vc/bin/raspistill -t 0 -o /tmp/test.png\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 5000\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 20000\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 60000\n</code></pre>\n','10','I reccently bought a Raspberry PI. I am new to ArchLinux so I thought I\'d make\nsome notes of the initial setup. This is the hardware used to build the camera:\n\n * Raspberry Pi Model B 512MB RAM\n * Camera Module for Raspberry Pi\n * WiFi USB Nano\n * OpenBox Sky Case\n * 16GB SD\n\nPartitioning SD card\nI added the ArchLinux image to a 16GB card. The images creates a 2GB parition so\nyou need to either extend it before booting or create a new partition after\nboot. I choose to create a new partition after boot, and use it for /home.\n\nfdisk /dev/disk/by-id/mmc-*\nn add partition\nw save and exit\n\n\nCreate file system.\nmkfs.ext4 /dev/mmcblk0p3\n\n\nRemove everything in /home. I am assuming you have nothing important here yet.\n\nrm -rf /home/*\n\n\nMount the device at boot.\n\n/dev/mmcblk0p3 /home ext4 defaults 0 2\n\n\nPackage manager\nI had never heard of Pacman package manager before. But its just as easy. To\nsearch for packages do this.\n\npacman -Ss partOfPackageName\n\n\nTo install a package do this.\n\npacman -S packageName\n\n\nUpgrade all packages.\n\npacman -Syu\n\n\nInitiallly I had some problems when installing packages. I got this.\n\n\"from mirror.archlinuxarm.org : The requested URL returned error: 404 Not Found\"\n\n\nJust perform a full uppgrade and it should be resolved.\n\nNetwork setup\nTo list available devices do.\n\nip link\n\n\nTo get the wireless network running I installed these.\n\npacman -S wpa_supplicant\npacman -S wpa_actiond\npacman -S ifplugd\npacman -S dhclient\npacman -S openntpd\n\n\nThere is a very nice setup wizard, just do this.\n\nwifi-menu -o\n\n\nNetwork configuration exists in profile files. I had some problems with the DHCP\nclient not setting IP:s after network loss. I added DHPClient  to the profiles\nto make it reconnect.\n\n#/etc/netctl/ethernet-dhcp\nDescription=\'A basic dhcp ethernet connection\'\nInterface=eth0\nConnection=ethernet\nIP=dhcp\nDHCPClient=\'dhclient\'\nTimeoutDHCP=30\nExecUpPost=\"ntpd -s &\"\n\n\n#/etc/netctl/wlan0-tallefjantlinksys\nDescription=\'Automatically generated profile by wifi-menu\'\nInterface=wlan0\nConnection=wireless\nSecurity=wpa\nESSID=MYNETWORKNAME\nIP=dhcp\nKey=12312312312313123132\nDHCPClient=\'dhclient\'\nTimeoutDHCP=30\nExecUpPost=\"ntpd -s &\"\n\n\nTo make WLAN connect when available, and Ethernet connect when plugged, do this.\n\nsystemctl enable netctl-auto@wlan0.service\nsystemctl enable netctl-ifplugd@eth00.service\n\n\nTo make WLAN and Ethernet use DHCP at boot, do this.\n\nsystemctl enable dhcpcd@eth0\nsystemctl enable dhcpcd@wlan0\n\n\nI noted that the network setup was not working after the first initual upgrade\nwith Pacman. To solve it I just disabled the enabled services and enabled them\nagain.\n\nsystemctl status --failed\nsystemctl disable FAILEDSERVICE\n\n\nI had problems with DHCP timeouts so I added the TimeoutDHCP  parameter and set\nit to 30. Default is 10 seconds. Also the ExecUpPost  will set time from NTP\nwhen connected.\n\nTime and date\nIn my case, Sweden.\n\nrm /etc/timezone\nln -s /usr/share/zoneinfo/Europe/Stockholm /etc/timezone\n\n\nAlso, the Raspberry Pi has no battery. The time will be 1970 on every reboot.\nYou can use NTP to set the date from a time server on startup.\n\npacman -S openntpd\n\n\nTo start ntpd when a network interface is connected, add ExecUpPost  to your\ninterface profile. Here is an example of my eth0.\n\nDescription=\'A basic dhcp ethernet connection\'\nInterface=eth0\nConnection=ethernet\nIP=dhcp\nExecUpPost=\"ntpd -s &\"\nRaspberry Camera\n\n\nCamera module software in PATH\nIn ~/.bashrc  I added.\n\nexport PATH=$PATH:/opt/vc/bin\n\n\nTweak camera memory usage\nIn /boot/config.txt  I added.\n\ngpu_mem=128\nstart_file=start_x.elf\nfixup_file=fixup_x.dat\n\n\nSpeed up camera\nThe camera may seem slow. There is a default delay of 5 seconds before it takes\nthe picture. You can change this with -t 0.\nI\'ve noticed the device hangs when storing larger videos. It\'s a good idea to\nrecord video directly to RAM, that works much better for me. By default /tmp  is\nmounted as tmpfs.\n\n/opt/vc/bin/raspistill -t 0 -o /tmp/test.png\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 5000\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 20000\n/opt/vc/bin/raspivid -o /tmp/out.h264 -t 60000',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2013-10-19 09:14:09','2','2014-10-04 14:04:18','1','2013-10-19 09:14:09','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd918698','9967d65f-d8fd-403e-82b2-c564aabfc6c3','Raspberry Surveillance','raspberry-surveillance','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I reccently bought a Raspberry PI. The main reason was that I had an idea of making some video surveillance software. I have made 2 previous blog posts regarding the Raspberry Pi and both of them are sort of related this one. One where I wrote some notes on the initial setup of the hardware and basic software. One where I wrote about how to setup Owncloud. This post is about https://github.com/tomasbjerre/RaspberrySurveillance. It is project where I\'ve developed, mainly, a [web interface](http://files.bjurr.se/blog/rbs.png) that can:\\n\\n* Show snapshot of camera\\n* Setup motion triggering\\n* Start and stop motion triggering\\n* Save captured videos to disk\\n* Save captured videos to Webdav (Supported by Owncloud)\\n\\nThis means you can start and stop motion triggering from a web interface. Whenever a video is captured you can have it uploaded to Owncloud and have it automatically synced to you Windows PC.\\n\\nWhen camera triggers it will store pictures and/or vieos, it is configurable. Here is a very small example of a captured event.\\n\\n* [Picture that detected change](http://files.bjurr.se/blog/00003-image.jpg)\\n* [Video recorded on trigger](http://files.bjurr.se/blog/00003-2013-08-06_10-03-30-video.h264)\\n* [Picture showing what was changed in picture](http://files.bjurr.se/blog/00003-image-diff.jpg)\\n\"}]],\"sections\":[[10,0]]}','<p>I reccently bought a Raspberry PI. The main reason was that I had an idea of making some video surveillance software. I have made 2 previous blog posts regarding the Raspberry Pi and both of them are sort of related this one. One where I wrote some notes on the initial setup of the hardware and basic software. One where I wrote about how to setup Owncloud. This post is about <a href=\"https://github.com/tomasbjerre/RaspberrySurveillance\">https://github.com/tomasbjerre/RaspberrySurveillance</a>. It is project where I\'ve developed, mainly, a <a href=\"http://files.bjurr.se/blog/rbs.png\">web interface</a> that can:</p>\n<ul>\n<li>Show snapshot of camera</li>\n<li>Setup motion triggering</li>\n<li>Start and stop motion triggering</li>\n<li>Save captured videos to disk</li>\n<li>Save captured videos to Webdav (Supported by Owncloud)</li>\n</ul>\n<p>This means you can start and stop motion triggering from a web interface. Whenever a video is captured you can have it uploaded to Owncloud and have it automatically synced to you Windows PC.</p>\n<p>When camera triggers it will store pictures and/or vieos, it is configurable. Here is a very small example of a captured event.</p>\n<ul>\n<li><a href=\"http://files.bjurr.se/blog/00003-image.jpg\">Picture that detected change</a></li>\n<li><a href=\"http://files.bjurr.se/blog/00003-2013-08-06_10-03-30-video.h264\">Video recorded on trigger</a></li>\n<li><a href=\"http://files.bjurr.se/blog/00003-image-diff.jpg\">Picture showing what was changed in picture</a></li>\n</ul>\n','11','I reccently bought a Raspberry PI. The main reason was that I had an idea of\nmaking some video surveillance software. I have made 2 previous blog posts\nregarding the Raspberry Pi and both of them are sort of related this one. One\nwhere I wrote some notes on the initial setup of the hardware and basic\nsoftware. One where I wrote about how to setup Owncloud. This post is about \nhttps://github.com/tomasbjerre/RaspberrySurveillance. It is project where I\'ve\ndeveloped, mainly, a web interface [http://files.bjurr.se/blog/rbs.png]  that\ncan:\n\n * Show snapshot of camera\n * Setup motion triggering\n * Start and stop motion triggering\n * Save captured videos to disk\n * Save captured videos to Webdav (Supported by Owncloud)\n\nThis means you can start and stop motion triggering from a web interface.\nWhenever a video is captured you can have it uploaded to Owncloud and have it\nautomatically synced to you Windows PC.\n\nWhen camera triggers it will store pictures and/or vieos, it is configurable.\nHere is a very small example of a captured event.\n\n * Picture that detected change [http://files.bjurr.se/blog/00003-image.jpg]\n * Video recorded on trigger\n   [http://files.bjurr.se/blog/00003-2013-08-06_10-03-30-video.h264]\n * Picture showing what was changed in picture\n   [http://files.bjurr.se/blog/00003-image-diff.jpg]',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2013-10-19 09:15:05','2','2014-10-04 14:03:54','1','2013-10-19 09:15:09','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd918699','5420692e-0deb-47ce-ad83-c1d7c168b1c4','Dummy camera made smart with Raspberry Pi','dummy-camera-made-smart-with-raspberry-pi','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I\'d like to share how I build my camera using Raspberry Pi. I used a dummy camera and turned into a smart camera! The total cost is about £100 / 1000 SEK / $150 =)\\n\\nI have also developed software to help use the Raspberry Camera for surveillance. Check out these blog posts for further information.\\n\\n* [Raspberry Surveillance](/raspberry-surveillance)\\n* [Raspberry PI with Camera Module and ArchLinux](/raspberry-pi-with-camera-module-and-archlinux)\\n* [Owncloud 5 Server with Lighttpd, Sqlite on ArchLinux](/owncloud-5-server-with-lighttpd-sqlite-on-archlinux)\\n\\n##Camera\\n\\nI found these parts on Amazon.co.uk.\\n\\n* Raspberry PI Starter Kit - Power, SD-card, Raspberry 512MB\\n* USB Wifi Adapter\\n* Byron - CS11D Dummy CCTV Camera Unit\\n\"}]],\"sections\":[[10,0]]}','<p>I\'d like to share how I build my camera using Raspberry Pi. I used a dummy camera and turned into a smart camera! The total cost is about £100 / 1000 SEK / $150 =)</p>\n<p>I have also developed software to help use the Raspberry Camera for surveillance. Check out these blog posts for further information.</p>\n<ul>\n<li><a href=\"/raspberry-surveillance\">Raspberry Surveillance</a></li>\n<li><a href=\"/raspberry-pi-with-camera-module-and-archlinux\">Raspberry PI with Camera Module and ArchLinux</a></li>\n<li><a href=\"/owncloud-5-server-with-lighttpd-sqlite-on-archlinux\">Owncloud 5 Server with Lighttpd, Sqlite on ArchLinux</a></li>\n</ul>\n<h2 id=\"camera\">Camera</h2>\n<p>I found these parts on Amazon.co.uk.</p>\n<ul>\n<li>Raspberry PI Starter Kit - Power, SD-card, Raspberry 512MB</li>\n<li>USB Wifi Adapter</li>\n<li>Byron - CS11D Dummy CCTV Camera Unit</li>\n</ul>\n','12','I\'d like to share how I build my camera using Raspberry Pi. I used a dummy\ncamera and turned into a smart camera! The total cost is about £100 / 1000 SEK /\n$150 =)\n\nI have also developed software to help use the Raspberry Camera for\nsurveillance. Check out these blog posts for further information.\n\n * Raspberry Surveillance [/raspberry-surveillance]\n * Raspberry PI with Camera Module and ArchLinux\n   [/raspberry-pi-with-camera-module-and-archlinux]\n * Owncloud 5 Server with Lighttpd, Sqlite on ArchLinux\n   [/owncloud-5-server-with-lighttpd-sqlite-on-archlinux]\n\nCamera\nI found these parts on Amazon.co.uk.\n\n * Raspberry PI Starter Kit - Power, SD-card, Raspberry 512MB\n * USB Wifi Adapter\n * Byron - CS11D Dummy CCTV Camera Unit',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2013-10-19 09:15:39','2','2014-10-05 20:09:01','1','2013-10-19 09:15:39','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd91869b','9366cb09-05cb-4207-806a-f0651b83dde1','FindFilm.se -  Netflix from A to Z, and more =)','findfilm-se-netflix-from-a-to-z-and-more','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I grew tired of the way Netflix is categorizing movies. I wanted to browse them alphabetically. I also wanted to find the movies that were recently added to Netflix. So I created [Findfilm.se](http://findfilm.se/), it has these features and it also includes movies from HeadWeb and ViaPlay. Enjoy =) And by the way: det är bara på Svenska =)\"}]],\"sections\":[[10,0]]}','<p>I grew tired of the way Netflix is categorizing movies. I wanted to browse them alphabetically. I also wanted to find the movies that were recently added to Netflix. So I created <a href=\"http://findfilm.se/\">Findfilm.se</a>, it has these features and it also includes movies from HeadWeb and ViaPlay. Enjoy =) And by the way: det är bara på Svenska =)</p>\n','14','I grew tired of the way Netflix is categorizing movies. I wanted to browse them\nalphabetically. I also wanted to find the movies that were recently added to\nNetflix. So I created Findfilm.se [http://findfilm.se/], it has these features\nand it also includes movies from HeadWeb and ViaPlay. Enjoy =) And by the way:\ndet är bara på Svenska =)',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2014-10-04 13:38:48','2','2014-10-05 23:58:28','1','2014-10-04 13:38:48','2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd91869e','1086957f-b0ba-40eb-bac3-978cec5a4923','Building Atlassian Stash pull requests in Jenkins','building-atlassian-stash-pull-requests-in-jenkins','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Update 160912: There is a [new post here](http://bjurr.com/continuous-integration-with-bitbucket-server-and-jenkins/).\\n\\nWe are just about to introduce pull requests at my current position. We are using Git with Atlassian Stash and Jenkins. We want to verify that the pull requests:\\n\\n* Compile\\n* Does not break any test cases\\n* Can be merged to target branch\\n* Compiles after merge\\n* Does not break test cases after merge\\n\\nAfter some Googling around the issue I found no solution, so I tought I\'d make a post about how I solved it.\\n\\n# Verifying source of the pull request\\nThere is a really nice plugin for Jenkins [Stash Notifier Plugin](https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin) that can be used to notify Stash of the status of a build. Enable it on any Jenkins job that builds the branch you are merging from. It will add an icon and a link to Jenkins in the pull request view of Stash.\\n\\n# Discovering new pull requests\\nI initially solved this with a Jenkins job that is polling Stash for new pull requests. But polling is never good so I created a Stash plugin that will notify Jenkins about new pull requests.\\n\\n## Pull Request Notifier Plugin for Stash\\nThe plugin is available in [Atlassian Marketplace](https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash) and at [GitHub](https://github.com/tomasbjerre/pull-request-notifier-for-stash). When installed, you will have this configuration GUI.\\n\\n![pull-request-notifier-for-stash](/content/images/2015/04/variables.png)\\n\\nThe features include:\\n\\n* Trigger on one, or several, event(s) regarding pull requests.\\n* Invoke one, or several, URL(s) when event(s) are triggered.\\n * Optionally with *basic authentication* headers.\\n * Completely custom URL supporting variable parameters\\n     * ${PULL\\\\_REQUEST\\\\_ID} Example: 1\\n     * ${PULL\\\\_REQUEST\\\\_ACTION} Example: OPENED\\n     * ${PULL\\\\_REQUEST\\\\_AUTHOR\\\\_DISPLAY\\\\_NAME} Example: Administrator\\n     * ${PULL\\\\_REQUEST\\\\_AUTHOR\\\\_EMAIL} Example: admin@example.com\\n     * ${PULL\\\\_REQUEST\\\\_AUTHOR\\\\_ID} Example: 1\\n     * ${PULL\\\\_REQUEST\\\\_AUTHOR\\\\_NAME} Example: admin\\n     * ${PULL\\\\_REQUEST\\\\_AUTHOR\\\\_SLUG} Example: admin\\n     * ${PULL\\\\_REQUEST\\\\_FROM\\\\_HASH} Example: 6053a1eaa1c009dd11092d09a72f3c41af1b59ad\\n     * ${PULL\\\\_REQUEST\\\\_FROM\\\\_ID} Example: refs/heads/branch_mod_merge\\n     * ${PULL\\\\_REQUEST\\\\_FROM\\\\_REPO\\\\_ID} Example: 1\\n     * ${PULL\\\\_REQUEST\\\\_FROM\\\\_REPO\\\\_NAME} Example: rep\\\\_1\\n     * ${PULL\\\\_REQUEST\\\\_FROM\\\\_REPO\\\\_PROJECT\\\\_ID} Example: 1\\n     * ${PULL\\\\_REQUEST\\\\_FROM\\\\_REPO\\\\_PROJECT\\\\_KEY} Example: PROJECT\\\\_1\\n     * ${PULL\\\\_REQUEST\\\\_FROM\\\\_REPO\\\\_SLUG} Example: rep\\\\_1\\n     * And same variables for TO, like: ${PULL\\\\_REQUEST\\\\_TO\\\\_HASH}\\n\\nYou can have several notifications and have them trigger different URL:s. If you trigger Jenkins builds, you may want each repo to have its own build job in Jenkins. The filtering functionality is highly configurable. Create a string with the variables and then a regexp that should match that string.\\n![pull-request-notifier-for-stash-filter](/content/images/2015/04/filter_branch_crop.png)\\n\\n## Polling Jenkins with Groovy script\\nNote that you should only do it this way if you cannot use the plugin described above! For example, uou may not have enaugh access to Stash to install plugins.\\n\\nStash has really nice [REST API:s](https://developer.atlassian.com/static/rest/stash/3.6.1/stash-rest.html). I created a scheduled job in Jenkins that runs every 5 minutes. I implemented it in Groovy using the [Groovy plugin](https://wiki.jenkins-ci.org/display/JENKINS/Groovy+plugin).\\n\\n* Find all repos: http://stash/rest/api/1.0/projects/PROJECTS/repos/\\n* Find all pull requests in a repo: http://stash/rest/api/1.0/projects/PROJECTS/repos/\\\"+repo.slug+\\\"/pull-requests?base&details&filterText&orderBy\\n\\n```\\nString summary = \\\"\\\"\\nint newPullRequests = 0;\\nFile previousPullRequests = new File(\\\"/ci/lib/jenkins/workspace/Pull Request Poller/previousPullRequests.txt\\\")\\n\\nString getJson(String addr) {\\n manager.listener.logger.println(\\\"Getting URL: \\\"+addr)\\n def authString = \\\"user:pass\\\".getBytes().encodeBase64().toString()\\n java.net.URLConnection conn = addr.toURL().openConnection()\\n conn.setRequestProperty( \\\"Authorization\\\", \\\"Basic ${authString}\\\" )\\n conn.connect()\\n def reader = new BufferedReader(new InputStreamReader(conn.getInputStream()))\\n def stringBuilder = new StringBuilder()\\n String line = null\\n while ((line = reader.readLine()) != null) {\\n  stringBuilder.append(line + \\\"\\\\n\\\")\\n }\\n String json = groovy.json.JsonOutput.prettyPrint(stringBuilder.toString())\\n manager.listener.logger.println(\\\"Got response:\\\\n\\\"+json)\\n return json\\n}\\n\\nnew groovy.json.JsonSlurper().parseText(getJson(\\\"http://stash/rest/api/1.0/projects/PROJECTS/repos/\\\")).values.each { repo ->\\n manager.listener.logger.println(\\\"Repo: \\\"+repo.slug)\\n\\n String prettyJSON = getJson(\\\"http://stash/rest/api/1.0/projects/PROJECTS/repos/\\\"+repo.slug+\\\"/pull-requests?base&details&filterText&orderBy\\\")\\n def jsonData = new groovy.json.JsonSlurper().parseText(prettyJSON);\\n jsonData.values.each { value ->\\n  String title = value.title\\n  String from = value.fromRef.latestChangeset\\n  String fromRepo = value.fromRef.repository.links.clone.find { it.name == \\\"ssh\\\" }.href\\n  String to = value.toRef.latestChangeset\\n  String toRepo = value.toRef.repository.links.clone.find { it.name == \\\"ssh\\\" }.href\\n  String repositorySlug = repo.slug\\n  String pullRequestId = value.id\\n  String requestUrl =  \\\"http://stash/projects/PROJECTS/repos/\\\"+repositorySlug+\\\"/pull-requests/\\\"+pullRequestId+\\\"/overview\\\"\\n\\n  //Remember that this request has been triggered, and avoid triggering it again\\n  String identifier = from+\\\" \\\"+to\\n  if (previousPullRequests.text.contains(identifier)) {\\n   manager.listener.logger.println(\\\"Ignoring: \\\"+identifier)\\n   return;\\n  }\\n  previousPullRequests.append(identifier+\\\"\\\\n\\\")\\n\\n  //Trigger a jenkins job that will verify the pull request\\n  String invokeBuildUrl = \\\"http://jenkins/job/Pull%20Request%20Builder/buildWithParameters?token=SECRET_CONFIGURED_IN_BUILD&FROM=\\\"+from+\\\"&TO=\\\"+to+\\\"&FROMREPO=\\\"+fromRepo+\\\"&TOREPO=\\\"+toRepo+\\\"&REPOSITORY_SLUG=\\\"+repositorySlug+\\\"&PULL_REQUEST_ID=\\\"+pullRequestId\\n  manager.listener.logger.println(invokeBuildUrl)\\n  new URL(invokeBuildUrl).getText()\\n\\n  summary += \\\"<h1>\\\"+title+\\\"</h1><br><a href=\'\\\"+requestUrl+\\\"\'>\\\"+requestUrl+\\\"</a><br>From: \\\"+jsonData.values[0].fromRef.id+\\\" (\\\"+from+\\\") in \\\"+fromRepo+\\\"<br>To: \\\"+jsonData.values[0].toRef.id+\\\" (\\\"+to+\\\") in \\\"+toRepo+\\\"<br><a href=\'\\\"+invokeBuildUrl+\\\"\'>\\\"+invokeBuildUrl+\\\"</a><hr>\\\"\\n  newPullRequests++;\\n }\\n}\\n\\n//Add some info to the build\\nif (newPullRequests == 0) {\\n manager.createSummary(\\\"gear2.gif\\\").appendText(\\\"<h1>No new pull requests found!</h1>\\\" , false)\\n} else {\\n manager.addShortText(\\\"+\\\"+newPullRequests, \\\"grey\\\", \\\"white\\\", \\\"0px\\\", \\\"white\\\")\\n manager.createSummary(\\\"gear2.gif\\\").appendText(summary , false)\\n}\\n```\\n\\n# Merging and building the pull request\\nI created a parameterized job to merge the pull request from source branch to target branch. It takes *FROM_HASH*, *FROM_REPO*, *TO_HASH*, *TO_REPO*, *REPOSITORY_SLUG* and *PULL_REQUEST_ID* as parameters.\\n\\nThe job has a build step *execute shell* that does the actual verification.\\n\\n```\\ngit clone $TO_REPO\\ncd *\\ngit reset --hard $TO_HASH\\ngit status\\ngit remote add from $FROM_REPO\\ngit fetch from\\ngit merge $FROM_HASH\\ngit --no-pager log --max-count=10 --graph --abbrev-commit\\n\\n#compile command here ...\\n```\\n\\nThe job uses the [Stash Notifier Plugin](https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin) to record result in the pull request in Stash. Use the ${FROM_HASH} variable to get the build status reported correctly in the pull request in Stash.\\n\\nIt adds a comment to the pull request, like this.\\n\\n```\\ncurl -D- -u user:pass -X POST -H \\\"Content-Type: application/json\\\"  --data \\\"{ \\\\\\\"text\\\\\\\": \\\\\\\"Looking good :) http://jenkins/job/Pull%20Request%20Builder/${BUILD_NUMBER}/\\\\\\\" }\\\" http://stash/rest/api/1.0/projects/PROJECT/repos/$REPOSITORY_SLUG/pull-requests/$PULL_REQUEST_ID/comments\\n```\\n\\n# Static code analyzers\\nIf you are using static code analyzers you may want to have a look at [Jenkins Violation Comments to Stash Plugin](https://github.com/jenkinsci/jenkins-violation-comments-to-stash-plugin) for Jenkins.\\n\\nIt is configured like this.\\n\\n![alt](/content/images/2015/05/screenshot-config.png)\\n\\nAnd will comment the pull requests like this.\\n\\n![alt](/content/images/2015/05/screenshot-stash.png)\\n\"}]],\"sections\":[[10,0]]}','<p>Update 160912: There is a <a href=\"http://bjurr.com/continuous-integration-with-bitbucket-server-and-jenkins/\">new post here</a>.</p>\n<p>We are just about to introduce pull requests at my current position. We are using Git with Atlassian Stash and Jenkins. We want to verify that the pull requests:</p>\n<ul>\n<li>Compile</li>\n<li>Does not break any test cases</li>\n<li>Can be merged to target branch</li>\n<li>Compiles after merge</li>\n<li>Does not break test cases after merge</li>\n</ul>\n<p>After some Googling around the issue I found no solution, so I tought I\'d make a post about how I solved it.</p>\n<h1 id=\"verifyingsourceofthepullrequest\">Verifying source of the pull request</h1>\n<p>There is a really nice plugin for Jenkins <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin\">Stash Notifier Plugin</a> that can be used to notify Stash of the status of a build. Enable it on any Jenkins job that builds the branch you are merging from. It will add an icon and a link to Jenkins in the pull request view of Stash.</p>\n<h1 id=\"discoveringnewpullrequests\">Discovering new pull requests</h1>\n<p>I initially solved this with a Jenkins job that is polling Stash for new pull requests. But polling is never good so I created a Stash plugin that will notify Jenkins about new pull requests.</p>\n<h2 id=\"pullrequestnotifierpluginforstash\">Pull Request Notifier Plugin for Stash</h2>\n<p>The plugin is available in <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash\">Atlassian Marketplace</a> and at <a href=\"https://github.com/tomasbjerre/pull-request-notifier-for-stash\">GitHub</a>. When installed, you will have this configuration GUI.</p>\n<p><img src=\"/content/images/2015/04/variables.png\" alt=\"pull-request-notifier-for-stash\"></p>\n<p>The features include:</p>\n<ul>\n<li>Trigger on one, or several, event(s) regarding pull requests.</li>\n<li>Invoke one, or several, URL(s) when event(s) are triggered.</li>\n<li>Optionally with <em>basic authentication</em> headers.</li>\n<li>Completely custom URL supporting variable parameters\n<ul>\n<li>${PULL_REQUEST_ID} Example: 1</li>\n<li>${PULL_REQUEST_ACTION} Example: OPENED</li>\n<li>${PULL_REQUEST_AUTHOR_DISPLAY_NAME} Example: Administrator</li>\n<li>${PULL_REQUEST_AUTHOR_EMAIL} Example: <a href=\"mailto:admin@example.com\">admin@example.com</a></li>\n<li>${PULL_REQUEST_AUTHOR_ID} Example: 1</li>\n<li>${PULL_REQUEST_AUTHOR_NAME} Example: admin</li>\n<li>${PULL_REQUEST_AUTHOR_SLUG} Example: admin</li>\n<li>${PULL_REQUEST_FROM_HASH} Example: 6053a1eaa1c009dd11092d09a72f3c41af1b59ad</li>\n<li>${PULL_REQUEST_FROM_ID} Example: refs/heads/branch_mod_merge</li>\n<li>${PULL_REQUEST_FROM_REPO_ID} Example: 1</li>\n<li>${PULL_REQUEST_FROM_REPO_NAME} Example: rep_1</li>\n<li>${PULL_REQUEST_FROM_REPO_PROJECT_ID} Example: 1</li>\n<li>${PULL_REQUEST_FROM_REPO_PROJECT_KEY} Example: PROJECT_1</li>\n<li>${PULL_REQUEST_FROM_REPO_SLUG} Example: rep_1</li>\n<li>And same variables for TO, like: ${PULL_REQUEST_TO_HASH}</li>\n</ul>\n</li>\n</ul>\n<p>You can have several notifications and have them trigger different URL:s. If you trigger Jenkins builds, you may want each repo to have its own build job in Jenkins. The filtering functionality is highly configurable. Create a string with the variables and then a regexp that should match that string.<br>\n<img src=\"/content/images/2015/04/filter_branch_crop.png\" alt=\"pull-request-notifier-for-stash-filter\"></p>\n<h2 id=\"pollingjenkinswithgroovyscript\">Polling Jenkins with Groovy script</h2>\n<p>Note that you should only do it this way if you cannot use the plugin described above! For example, uou may not have enaugh access to Stash to install plugins.</p>\n<p>Stash has really nice <a href=\"https://developer.atlassian.com/static/rest/stash/3.6.1/stash-rest.html\">REST API:s</a>. I created a scheduled job in Jenkins that runs every 5 minutes. I implemented it in Groovy using the <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Groovy+plugin\">Groovy plugin</a>.</p>\n<ul>\n<li>Find all repos: <a href=\"http://stash/rest/api/1.0/projects/PROJECTS/repos/\">http://stash/rest/api/1.0/projects/PROJECTS/repos/</a></li>\n<li>Find all pull requests in a repo: <a href=\"http://stash/rest/api/1.0/projects/PROJECTS/repos/%22+repo.slug+%22/pull-requests?base&amp;details&amp;filterText&amp;orderBy\">http://stash/rest/api/1.0/projects/PROJECTS/repos/&quot;+repo.slug+&quot;/pull-requests?base&amp;details&amp;filterText&amp;orderBy</a></li>\n</ul>\n<pre><code>String summary = &quot;&quot;\nint newPullRequests = 0;\nFile previousPullRequests = new File(&quot;/ci/lib/jenkins/workspace/Pull Request Poller/previousPullRequests.txt&quot;)\n\nString getJson(String addr) {\n manager.listener.logger.println(&quot;Getting URL: &quot;+addr)\n def authString = &quot;user:pass&quot;.getBytes().encodeBase64().toString()\n java.net.URLConnection conn = addr.toURL().openConnection()\n conn.setRequestProperty( &quot;Authorization&quot;, &quot;Basic ${authString}&quot; )\n conn.connect()\n def reader = new BufferedReader(new InputStreamReader(conn.getInputStream()))\n def stringBuilder = new StringBuilder()\n String line = null\n while ((line = reader.readLine()) != null) {\n  stringBuilder.append(line + &quot;\\n&quot;)\n }\n String json = groovy.json.JsonOutput.prettyPrint(stringBuilder.toString())\n manager.listener.logger.println(&quot;Got response:\\n&quot;+json)\n return json\n}\n\nnew groovy.json.JsonSlurper().parseText(getJson(&quot;http://stash/rest/api/1.0/projects/PROJECTS/repos/&quot;)).values.each { repo -&gt;\n manager.listener.logger.println(&quot;Repo: &quot;+repo.slug)\n\n String prettyJSON = getJson(&quot;http://stash/rest/api/1.0/projects/PROJECTS/repos/&quot;+repo.slug+&quot;/pull-requests?base&amp;details&amp;filterText&amp;orderBy&quot;)\n def jsonData = new groovy.json.JsonSlurper().parseText(prettyJSON);\n jsonData.values.each { value -&gt;\n  String title = value.title\n  String from = value.fromRef.latestChangeset\n  String fromRepo = value.fromRef.repository.links.clone.find { it.name == &quot;ssh&quot; }.href\n  String to = value.toRef.latestChangeset\n  String toRepo = value.toRef.repository.links.clone.find { it.name == &quot;ssh&quot; }.href\n  String repositorySlug = repo.slug\n  String pullRequestId = value.id\n  String requestUrl =  &quot;http://stash/projects/PROJECTS/repos/&quot;+repositorySlug+&quot;/pull-requests/&quot;+pullRequestId+&quot;/overview&quot;\n\n  //Remember that this request has been triggered, and avoid triggering it again\n  String identifier = from+&quot; &quot;+to\n  if (previousPullRequests.text.contains(identifier)) {\n   manager.listener.logger.println(&quot;Ignoring: &quot;+identifier)\n   return;\n  }\n  previousPullRequests.append(identifier+&quot;\\n&quot;)\n\n  //Trigger a jenkins job that will verify the pull request\n  String invokeBuildUrl = &quot;http://jenkins/job/Pull%20Request%20Builder/buildWithParameters?token=SECRET_CONFIGURED_IN_BUILD&amp;FROM=&quot;+from+&quot;&amp;TO=&quot;+to+&quot;&amp;FROMREPO=&quot;+fromRepo+&quot;&amp;TOREPO=&quot;+toRepo+&quot;&amp;REPOSITORY_SLUG=&quot;+repositorySlug+&quot;&amp;PULL_REQUEST_ID=&quot;+pullRequestId\n  manager.listener.logger.println(invokeBuildUrl)\n  new URL(invokeBuildUrl).getText()\n\n  summary += &quot;&lt;h1&gt;&quot;+title+&quot;&lt;/h1&gt;&lt;br&gt;&lt;a href=\'&quot;+requestUrl+&quot;\'&gt;&quot;+requestUrl+&quot;&lt;/a&gt;&lt;br&gt;From: &quot;+jsonData.values[0].fromRef.id+&quot; (&quot;+from+&quot;) in &quot;+fromRepo+&quot;&lt;br&gt;To: &quot;+jsonData.values[0].toRef.id+&quot; (&quot;+to+&quot;) in &quot;+toRepo+&quot;&lt;br&gt;&lt;a href=\'&quot;+invokeBuildUrl+&quot;\'&gt;&quot;+invokeBuildUrl+&quot;&lt;/a&gt;&lt;hr&gt;&quot;\n  newPullRequests++;\n }\n}\n\n//Add some info to the build\nif (newPullRequests == 0) {\n manager.createSummary(&quot;gear2.gif&quot;).appendText(&quot;&lt;h1&gt;No new pull requests found!&lt;/h1&gt;&quot; , false)\n} else {\n manager.addShortText(&quot;+&quot;+newPullRequests, &quot;grey&quot;, &quot;white&quot;, &quot;0px&quot;, &quot;white&quot;)\n manager.createSummary(&quot;gear2.gif&quot;).appendText(summary , false)\n}\n</code></pre>\n<h1 id=\"mergingandbuildingthepullrequest\">Merging and building the pull request</h1>\n<p>I created a parameterized job to merge the pull request from source branch to target branch. It takes <em>FROM_HASH</em>, <em>FROM_REPO</em>, <em>TO_HASH</em>, <em>TO_REPO</em>, <em>REPOSITORY_SLUG</em> and <em>PULL_REQUEST_ID</em> as parameters.</p>\n<p>The job has a build step <em>execute shell</em> that does the actual verification.</p>\n<pre><code>git clone $TO_REPO\ncd *\ngit reset --hard $TO_HASH\ngit status\ngit remote add from $FROM_REPO\ngit fetch from\ngit merge $FROM_HASH\ngit --no-pager log --max-count=10 --graph --abbrev-commit\n\n#compile command here ...\n</code></pre>\n<p>The job uses the <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin\">Stash Notifier Plugin</a> to record result in the pull request in Stash. Use the ${FROM_HASH} variable to get the build status reported correctly in the pull request in Stash.</p>\n<p>It adds a comment to the pull request, like this.</p>\n<pre><code>curl -D- -u user:pass -X POST -H &quot;Content-Type: application/json&quot;  --data &quot;{ \\&quot;text\\&quot;: \\&quot;Looking good :) http://jenkins/job/Pull%20Request%20Builder/${BUILD_NUMBER}/\\&quot; }&quot; http://stash/rest/api/1.0/projects/PROJECT/repos/$REPOSITORY_SLUG/pull-requests/$PULL_REQUEST_ID/comments\n</code></pre>\n<h1 id=\"staticcodeanalyzers\">Static code analyzers</h1>\n<p>If you are using static code analyzers you may want to have a look at <a href=\"https://github.com/jenkinsci/jenkins-violation-comments-to-stash-plugin\">Jenkins Violation Comments to Stash Plugin</a> for Jenkins.</p>\n<p>It is configured like this.</p>\n<p><img src=\"/content/images/2015/05/screenshot-config.png\" alt=\"alt\"></p>\n<p>And will comment the pull requests like this.</p>\n<p><img src=\"/content/images/2015/05/screenshot-stash.png\" alt=\"alt\"></p>\n','18','Update 160912: There is a new post here\n[http://bjurr.com/continuous-integration-with-bitbucket-server-and-jenkins/].\n\nWe are just about to introduce pull requests at my current position. We are\nusing Git with Atlassian Stash and Jenkins. We want to verify that the pull\nrequests:\n\n * Compile\n * Does not break any test cases\n * Can be merged to target branch\n * Compiles after merge\n * Does not break test cases after merge\n\nAfter some Googling around the issue I found no solution, so I tought I\'d make a\npost about how I solved it.\n\nVerifying source of the pull request\nThere is a really nice plugin for Jenkins Stash Notifier Plugin\n[https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin]  that can be\nused to notify Stash of the status of a build. Enable it on any Jenkins job that\nbuilds the branch you are merging from. It will add an icon and a link to\nJenkins in the pull request view of Stash.\n\nDiscovering new pull requests\nI initially solved this with a Jenkins job that is polling Stash for new pull\nrequests. But polling is never good so I created a Stash plugin that will notify\nJenkins about new pull requests.\n\nPull Request Notifier Plugin for Stash\nThe plugin is available in Atlassian Marketplace\n[https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash] \n and at GitHub [https://github.com/tomasbjerre/pull-request-notifier-for-stash].\nWhen installed, you will have this configuration GUI.\n\n\n\nThe features include:\n\n * Trigger on one, or several, event(s) regarding pull requests.\n * Invoke one, or several, URL(s) when event(s) are triggered.\n * Optionally with basic authentication  headers.\n * Completely custom URL supporting variable parameters * ${PULL_REQUEST_ID}\n      Example: 1\n    * ${PULL_REQUEST_ACTION}\n      Example: OPENED\n    * \n      ${PULL_REQUEST_AUTHOR_DISPLAY_NAME} Example: Administrator\n    * \n      ${PULL_REQUEST_AUTHOR_EMAIL} Example: admin@example.com\n    * \n      ${PULL_REQUEST_AUTHOR_ID} Example: 1\n    * \n      ${PULL_REQUEST_AUTHOR_NAME} Example: admin\n    * \n      ${PULL_REQUEST_AUTHOR_SLUG} Example: admin\n    * \n      ${PULL_REQUEST_FROM_HASH} Example:\n      6053a1eaa1c009dd11092d09a72f3c41af1b59ad\n    * \n      ${PULL_REQUEST_FROM_ID} Example: refs/heads/branch_mod_merge\n    * \n      ${PULL_REQUEST_FROM_REPO_ID} Example: 1\n    * \n      ${PULL_REQUEST_FROM_REPO_NAME} Example: rep_1\n    * \n      ${PULL_REQUEST_FROM_REPO_PROJECT_ID} Example: 1\n    * \n      ${PULL_REQUEST_FROM_REPO_PROJECT_KEY} Example: PROJECT_1\n    * \n      ${PULL_REQUEST_FROM_REPO_SLUG} Example: rep_1\n    * And same variables for\n      TO, like: ${PULL_REQUEST_TO_HASH}\n   \n   \n\nYou can have several notifications and have them trigger different URL:s. If you\ntrigger Jenkins builds, you may want each repo to have its own build job in\nJenkins. The filtering functionality is highly configurable. Create a string\nwith the variables and then a regexp that should match that string.\n\n\nPolling Jenkins with Groovy script\nNote that you should only do it this way if you cannot use the plugin described\nabove! For example, uou may not have enaugh access to Stash to install plugins.\n\nStash has really nice REST API:s\n[https://developer.atlassian.com/static/rest/stash/3.6.1/stash-rest.html]. I\ncreated a scheduled job in Jenkins that runs every 5 minutes. I implemented it\nin Groovy using the Groovy plugin\n[https://wiki.jenkins-ci.org/display/JENKINS/Groovy+plugin].\n\n * Find all repos: http://stash/rest/api/1.0/projects/PROJECTS/repos/\n * Find all pull requests in a repo: \n   http://stash/rest/api/1.0/projects/PROJECTS/repos/\"+repo.slug+\"/pull-requests?base&details&filterText&orderBy\n   [http://stash/rest/api/1.0/projects/PROJECTS/repos/%22+repo.slug+%22/pull-requests?base&details&filterText&orderBy]\n\nString summary = \"\"\nint newPullRequests = 0;\nFile previousPullRequests = new File(\"/ci/lib/jenkins/workspace/Pull Request Poller/previousPullRequests.txt\")\n\nString getJson(String addr) {\n manager.listener.logger.println(\"Getting URL: \"+addr)\n def authString = \"user:pass\".getBytes().encodeBase64().toString()\n java.net.URLConnection conn = addr.toURL().openConnection()\n conn.setRequestProperty( \"Authorization\", \"Basic ${authString}\" )\n conn.connect()\n def reader = new BufferedReader(new InputStreamReader(conn.getInputStream()))\n def stringBuilder = new StringBuilder()\n String line = null\n while ((line = reader.readLine()) != null) {\n  stringBuilder.append(line + \"\\n\")\n }\n String json = groovy.json.JsonOutput.prettyPrint(stringBuilder.toString())\n manager.listener.logger.println(\"Got response:\\n\"+json)\n return json\n}\n\nnew groovy.json.JsonSlurper().parseText(getJson(\"http://stash/rest/api/1.0/projects/PROJECTS/repos/\")).values.each { repo ->\n manager.listener.logger.println(\"Repo: \"+repo.slug)\n\n String prettyJSON = getJson(\"http://stash/rest/api/1.0/projects/PROJECTS/repos/\"+repo.slug+\"/pull-requests?base&details&filterText&orderBy\")\n def jsonData = new groovy.json.JsonSlurper().parseText(prettyJSON);\n jsonData.values.each { value ->\n  String title = value.title\n  String from = value.fromRef.latestChangeset\n  String fromRepo = value.fromRef.repository.links.clone.find { it.name == \"ssh\" }.href\n  String to = value.toRef.latestChangeset\n  String toRepo = value.toRef.repository.links.clone.find { it.name == \"ssh\" }.href\n  String repositorySlug = repo.slug\n  String pullRequestId = value.id\n  String requestUrl =  \"http://stash/projects/PROJECTS/repos/\"+repositorySlug+\"/pull-requests/\"+pullRequestId+\"/overview\"\n\n  //Remember that this request has been triggered, and avoid triggering it again\n  String identifier = from+\" \"+to\n  if (previousPullRequests.text.contains(identifier)) {\n   manager.listener.logger.println(\"Ignoring: \"+identifier)\n   return;\n  }\n  previousPullRequests.append(identifier+\"\\n\")\n\n  //Trigger a jenkins job that will verify the pull request\n  String invokeBuildUrl = \"http://jenkins/job/Pull%20Request%20Builder/buildWithParameters?token=SECRET_CONFIGURED_IN_BUILD&FROM=\"+from+\"&TO=\"+to+\"&FROMREPO=\"+fromRepo+\"&TOREPO=\"+toRepo+\"&REPOSITORY_SLUG=\"+repositorySlug+\"&PULL_REQUEST_ID=\"+pullRequestId\n  manager.listener.logger.println(invokeBuildUrl)\n  new URL(invokeBuildUrl).getText()\n\n  summary += \"<h1>\"+title+\"</h1><br><a href=\'\"+requestUrl+\"\'>\"+requestUrl+\"</a><br>From: \"+jsonData.values[0].fromRef.id+\" (\"+from+\") in \"+fromRepo+\"<br>To: \"+jsonData.values[0].toRef.id+\" (\"+to+\") in \"+toRepo+\"<br><a href=\'\"+invokeBuildUrl+\"\'>\"+invokeBuildUrl+\"</a><hr>\"\n  newPullRequests++;\n }\n}\n\n//Add some info to the build\nif (newPullRequests == 0) {\n manager.createSummary(\"gear2.gif\").appendText(\"<h1>No new pull requests found!</h1>\" , false)\n} else {\n manager.addShortText(\"+\"+newPullRequests, \"grey\", \"white\", \"0px\", \"white\")\n manager.createSummary(\"gear2.gif\").appendText(summary , false)\n}\n\n\nMerging and building the pull request\nI created a parameterized job to merge the pull request from source branch to\ntarget branch. It takes FROM_HASH, FROM_REPO, TO_HASH, TO_REPO, REPOSITORY_SLUG \nand PULL_REQUEST_ID  as parameters.\n\nThe job has a build step execute shell  that does the actual verification.\n\ngit clone $TO_REPO\ncd *\ngit reset --hard $TO_HASH\ngit status\ngit remote add from $FROM_REPO\ngit fetch from\ngit merge $FROM_HASH\ngit --no-pager log --max-count=10 --graph --abbrev-commit\n\n#compile command here ...\n\n\nThe job uses the Stash Notifier Plugin\n[https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin]  to record\nresult in the pull request in Stash. Use the ${FROM_HASH} variable to get the\nbuild status reported correctly in the pull request in Stash.\n\nIt adds a comment to the pull request, like this.\n\ncurl -D- -u user:pass -X POST -H \"Content-Type: application/json\"  --data \"{ \\\"text\\\": \\\"Looking good :) http://jenkins/job/Pull%20Request%20Builder/${BUILD_NUMBER}/\\\" }\" http://stash/rest/api/1.0/projects/PROJECT/repos/$REPOSITORY_SLUG/pull-requests/$PULL_REQUEST_ID/comments\n\n\nStatic code analyzers\nIf you are using static code analyzers you may want to have a look at Jenkins\nViolation Comments to Stash Plugin\n[https://github.com/jenkinsci/jenkins-violation-comments-to-stash-plugin]  for\nJenkins.\n\nIt is configured like this.\n\n\n\nAnd will comment the pull requests like this.',NULL,0,0,'published',NULL,'public',NULL,NULL,'1','2015-02-18 16:25:34','1','2017-03-18 19:18:18','1','2015-02-18 16:25:34','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd91869f','a29e716c-e305-4ad2-9a10-00bb44a0a545','Simple Stash Commit Checker','simple-stash-commit-checker','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Update 160912: There is a [new post here](http://bjurr.com/continuous-integration-with-bitbucket-server-and-jenkins/).\\n\\nI\'m currently working with a customer that has around 30 comitters working on the same code base. We are using Git with Atlassian Stash.\\n\\nReccently some code were released to production earlier then planned. Because of a faulty merge about 10 weeks earlier. This brings up the subject of commit comments. I started looking around for a good commit checker for Stash. I found mainly one problem with the existing hooks. It was not possible to customize messages shown to comitters when rejected.\\n\\nSo I created [Simple Stash Commit Checker](https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc). The code is available [here](https://github.com/tomasbjerre/simple-stash-commit-checker). It is highly configurable. For any rejection reason, there is a configurable response.\\n\\n![Config Email](/content/images/2015/03/config_and_reject.png)\\n\\nEmail and author can be checked against corresponding value in Stash.\\n\\n![Config Email](/content/images/2015/03/config.png)\\n\\nIf you enable this kind of plugin, the developers may need some time to adjust. Thats why I added a *dry run* mode. Where rejection messages are shown but no commits are actually rejected.\\n![Dry run](/content/images/2015/03/dry.png)\\n\\nIt is possible to add groups of restrictions. A group could be *Issues*. And rules in the groups. A rule could be, for example, *Jira* and/or *Incident*. And the group could state that at least one issue should be mentioned in the commit.\\n\\n![Groups](/content/images/2015/03/rules.png)\\n\\nThe group could also accept commits that does not contain some specific word. Some people like to write *fixing review comments* or *fixing sonar errors*. There could be a rule rejecting commits containing *sonar* or *review*. With a rejection reason like: *It is not relevant to mention that the changes were suggested by Sonar or from a review. It is relevant to mention what is actually changed and how that improves the code.*\"}]],\"sections\":[[10,0]]}','<p>Update 160912: There is a <a href=\"http://bjurr.com/continuous-integration-with-bitbucket-server-and-jenkins/\">new post here</a>.</p>\n<p>I\'m currently working with a customer that has around 30 comitters working on the same code base. We are using Git with Atlassian Stash.</p>\n<p>Reccently some code were released to production earlier then planned. Because of a faulty merge about 10 weeks earlier. This brings up the subject of commit comments. I started looking around for a good commit checker for Stash. I found mainly one problem with the existing hooks. It was not possible to customize messages shown to comitters when rejected.</p>\n<p>So I created <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc\">Simple Stash Commit Checker</a>. The code is available <a href=\"https://github.com/tomasbjerre/simple-stash-commit-checker\">here</a>. It is highly configurable. For any rejection reason, there is a configurable response.</p>\n<p><img src=\"/content/images/2015/03/config_and_reject.png\" alt=\"Config Email\"></p>\n<p>Email and author can be checked against corresponding value in Stash.</p>\n<p><img src=\"/content/images/2015/03/config.png\" alt=\"Config Email\"></p>\n<p>If you enable this kind of plugin, the developers may need some time to adjust. Thats why I added a <em>dry run</em> mode. Where rejection messages are shown but no commits are actually rejected.<br>\n<img src=\"/content/images/2015/03/dry.png\" alt=\"Dry run\"></p>\n<p>It is possible to add groups of restrictions. A group could be <em>Issues</em>. And rules in the groups. A rule could be, for example, <em>Jira</em> and/or <em>Incident</em>. And the group could state that at least one issue should be mentioned in the commit.</p>\n<p><img src=\"/content/images/2015/03/rules.png\" alt=\"Groups\"></p>\n<p>The group could also accept commits that does not contain some specific word. Some people like to write <em>fixing review comments</em> or <em>fixing sonar errors</em>. There could be a rule rejecting commits containing <em>sonar</em> or <em>review</em>. With a rejection reason like: <em>It is not relevant to mention that the changes were suggested by Sonar or from a review. It is relevant to mention what is actually changed and how that improves the code.</em></p>\n','19','Update 160912: There is a new post here\n[http://bjurr.com/continuous-integration-with-bitbucket-server-and-jenkins/].\n\nI\'m currently working with a customer that has around 30 comitters working on\nthe same code base. We are using Git with Atlassian Stash.\n\nReccently some code were released to production earlier then planned. Because of\na faulty merge about 10 weeks earlier. This brings up the subject of commit\ncomments. I started looking around for a good commit checker for Stash. I found\nmainly one problem with the existing hooks. It was not possible to customize\nmessages shown to comitters when rejected.\n\nSo I created Simple Stash Commit Checker\n[https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc]. The code is\navailable here [https://github.com/tomasbjerre/simple-stash-commit-checker]. It\nis highly configurable. For any rejection reason, there is a configurable\nresponse.\n\n\n\nEmail and author can be checked against corresponding value in Stash.\n\n\n\nIf you enable this kind of plugin, the developers may need some time to adjust.\nThats why I added a dry run  mode. Where rejection messages are shown but no\ncommits are actually rejected.\n\n\nIt is possible to add groups of restrictions. A group could be Issues. And rules\nin the groups. A rule could be, for example, Jira  and/or Incident. And the\ngroup could state that at least one issue should be mentioned in the commit.\n\n\n\nThe group could also accept commits that does not contain some specific word.\nSome people like to write fixing review comments  or fixing sonar errors. There\ncould be a rule rejecting commits containing sonar  or review. With a rejection\nreason like: It is not relevant to mention that the changes were suggested by\nSonar or from a review. It is relevant to mention what is actually changed and\nhow that improves the code.','/content/images/2018/02/0722.sdt-atlassian-1.png',0,0,'published',NULL,'public',NULL,NULL,'1','2015-03-01 21:35:13','1','2018-02-24 09:12:56','1','2015-03-01 21:35:29','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a1','d5ae9f56-dcc8-41d7-9b44-f46607fb0875','Git Changelog Lib - Changelog, or releasenotes, from template','git-changelog','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I define a **changelog** as a document that, for every release, contains a section describing the changes in that release. I define **releasenotes** as the changes made in a specific release. Most notes below can be applied to both, but I\'m just going to talk about changelogs. To keep it simple.\\n\\n#The problem\\n\\nChangelogs are important! But there are some problems that comes with having a changelog.\\n\\n * **Boring** Keeping the changelog updated is boring!\\n * **Availability** You may be required to make the changelog available to people who are not developers.\\n * **Reliability** Some change may not be in the changelog, or some change may be there but not in the actual release.\\n * **Formatting** Very few developers enjoy fiddling with text formatting!\\n\\n#Other solutions\\n\\nThere are some solutions, if you [google it](http://lmgtfy.com/?q=git+changelog). What I was missing in the solutions that I found was:\\n\\n * **Templating** Companies may have very specific requirements on the formatting of the changelog.\\n * **Wiki integration** I needed to publish the changelog in MediaWiki.\\n\\n#My solution\\n\\nIs [Git Changelog Lib](https://github.com/tomasbjerre/git-changelog-lib). A library, Java, that create a changelog for you, using the information that is already in the GIT repository. The lib can be used:\\n\\n * With a [Gradle plugin](https://github.com/tomasbjerre/git-changelog-gradle-plugin).\\n * With a [Maven plugin](https://github.com/tomasbjerre/git-changelog-maven-plugin).\\n * With a [Jenkins plugin](https://github.com/jenkinsci/git-changelog-plugin).\\n * With a [Bitbucket Server plugin](https://github.com/tomasbjerre/git-changelog-bitbucket-plugin).\\n * From [command line](https://github.com/tomasbjerre/git-changelog-command-line).\\n\\nThe basic idea is to have library that gathers a data structure and supplies it to a Mustache template so that the changelog becomes completely customizable.\\n\\n```\\n# Git Changelog changelog\\n\\nChangelog of Git Changelog.\\n{{#tags}}\\n## {{name}}\\n {{#issues}}\\n  {{#hasLink}}\\n### {{name}} [{{issue}}]({{link}}) {{title}}\\n  {{/hasLink}}\\n  {{^hasLink}}\\n### {{name}} {{title}}\\n  {{/hasLink}}\\n  {{#authors}}\\n* {{authorName}}\\n   {{#commits}}\\n[{{hash}}](https://server/{{hash}}) *{{commitTime}}*\\n{{{message}}}\\n\\n   {{/commits}}\\n\\n  {{/authors}}\\n {{/issues}}\\n{{/tags}}\\n```\\n\\nSome of the features included are:\\n\\n * **Templating** Using Mustache templates.\\n * **MediaWiki** Integrated with MediaWiki to publish changelogs.\\n * **[Jenkins plugin](https://github.com/jenkinsci/git-changelog-plugin)** To publish the changelog on Jenkins summary page. Or let Jenkins create MediaWiki page or file in workspace.\\n * **[Bitbucket Server plugin](https://github.com/tomasbjerre/git-changelog-bitbucket-plugin)** Adds a changelog in the repository page.\\n * **[Gradle plugin](https://github.com/tomasbjerre/git-changelog-gradle-plugin)** To add a changelog task to the Gradle build.\\n * **[Maven plugin](https://github.com/tomasbjerre/git-changelog-maven-plugin)** To add a changelog task to the Maven build.\"}]],\"sections\":[[10,0]]}','<p>I define a <strong>changelog</strong> as a document that, for every release, contains a section describing the changes in that release. I define <strong>releasenotes</strong> as the changes made in a specific release. Most notes below can be applied to both, but I\'m just going to talk about changelogs. To keep it simple.</p>\n<h1 id=\"theproblem\">The problem</h1>\n<p>Changelogs are important! But there are some problems that comes with having a changelog.</p>\n<ul>\n<li><strong>Boring</strong> Keeping the changelog updated is boring!</li>\n<li><strong>Availability</strong> You may be required to make the changelog available to people who are not developers.</li>\n<li><strong>Reliability</strong> Some change may not be in the changelog, or some change may be there but not in the actual release.</li>\n<li><strong>Formatting</strong> Very few developers enjoy fiddling with text formatting!</li>\n</ul>\n<h1 id=\"othersolutions\">Other solutions</h1>\n<p>There are some solutions, if you <a href=\"http://lmgtfy.com/?q=git+changelog\">google it</a>. What I was missing in the solutions that I found was:</p>\n<ul>\n<li><strong>Templating</strong> Companies may have very specific requirements on the formatting of the changelog.</li>\n<li><strong>Wiki integration</strong> I needed to publish the changelog in MediaWiki.</li>\n</ul>\n<h1 id=\"mysolution\">My solution</h1>\n<p>Is <a href=\"https://github.com/tomasbjerre/git-changelog-lib\">Git Changelog Lib</a>. A library, Java, that create a changelog for you, using the information that is already in the GIT repository. The lib can be used:</p>\n<ul>\n<li>With a <a href=\"https://github.com/tomasbjerre/git-changelog-gradle-plugin\">Gradle plugin</a>.</li>\n<li>With a <a href=\"https://github.com/tomasbjerre/git-changelog-maven-plugin\">Maven plugin</a>.</li>\n<li>With a <a href=\"https://github.com/jenkinsci/git-changelog-plugin\">Jenkins plugin</a>.</li>\n<li>With a <a href=\"https://github.com/tomasbjerre/git-changelog-bitbucket-plugin\">Bitbucket Server plugin</a>.</li>\n<li>From <a href=\"https://github.com/tomasbjerre/git-changelog-command-line\">command line</a>.</li>\n</ul>\n<p>The basic idea is to have library that gathers a data structure and supplies it to a Mustache template so that the changelog becomes completely customizable.</p>\n<pre><code># Git Changelog changelog\n\nChangelog of Git Changelog.\n{{#tags}}\n## {{name}}\n {{#issues}}\n  {{#hasLink}}\n### {{name}} [{{issue}}]({{link}}) {{title}}\n  {{/hasLink}}\n  {{^hasLink}}\n### {{name}} {{title}}\n  {{/hasLink}}\n  {{#authors}}\n* {{authorName}}\n   {{#commits}}\n[{{hash}}](https://server/{{hash}}) *{{commitTime}}*\n{{{message}}}\n\n   {{/commits}}\n\n  {{/authors}}\n {{/issues}}\n{{/tags}}\n</code></pre>\n<p>Some of the features included are:</p>\n<ul>\n<li><strong>Templating</strong> Using Mustache templates.</li>\n<li><strong>MediaWiki</strong> Integrated with MediaWiki to publish changelogs.</li>\n<li><strong><a href=\"https://github.com/jenkinsci/git-changelog-plugin\">Jenkins plugin</a></strong> To publish the changelog on Jenkins summary page. Or let Jenkins create MediaWiki page or file in workspace.</li>\n<li><strong><a href=\"https://github.com/tomasbjerre/git-changelog-bitbucket-plugin\">Bitbucket Server plugin</a></strong> Adds a changelog in the repository page.</li>\n<li><strong><a href=\"https://github.com/tomasbjerre/git-changelog-gradle-plugin\">Gradle plugin</a></strong> To add a changelog task to the Gradle build.</li>\n<li><strong><a href=\"https://github.com/tomasbjerre/git-changelog-maven-plugin\">Maven plugin</a></strong> To add a changelog task to the Maven build.</li>\n</ul>\n','23','I define a changelog  as a document that, for every release, contains a section\ndescribing the changes in that release. I define releasenotes  as the changes\nmade in a specific release. Most notes below can be applied to both, but I\'m\njust going to talk about changelogs. To keep it simple.\n\nThe problem\nChangelogs are important! But there are some problems that comes with having a\nchangelog.\n\n * Boring  Keeping the changelog updated is boring!\n * Availability  You may be required to make the changelog available to people\n   who are not developers.\n * Reliability  Some change may not be in the changelog, or some change may be\n   there but not in the actual release.\n * Formatting  Very few developers enjoy fiddling with text formatting!\n\nOther solutions\nThere are some solutions, if you google it [http://lmgtfy.com/?q=git+changelog].\nWhat I was missing in the solutions that I found was:\n\n * Templating  Companies may have very specific requirements on the formatting\n   of the changelog.\n * Wiki integration  I needed to publish the changelog in MediaWiki.\n\nMy solution\nIs Git Changelog Lib [https://github.com/tomasbjerre/git-changelog-lib]. A\nlibrary, Java, that create a changelog for you, using the information that is\nalready in the GIT repository. The lib can be used:\n\n * With a Gradle plugin\n   [https://github.com/tomasbjerre/git-changelog-gradle-plugin].\n * With a Maven plugin\n   [https://github.com/tomasbjerre/git-changelog-maven-plugin].\n * With a Jenkins plugin [https://github.com/jenkinsci/git-changelog-plugin].\n * With a Bitbucket Server plugin\n   [https://github.com/tomasbjerre/git-changelog-bitbucket-plugin].\n * From command line [https://github.com/tomasbjerre/git-changelog-command-line]\n   .\n\nThe basic idea is to have library that gathers a data structure and supplies it\nto a Mustache template so that the changelog becomes completely customizable.\n\n# Git Changelog changelog\n\nChangelog of Git Changelog.\n{{#tags}}\n## {{name}}\n {{#issues}}\n  {{#hasLink}}\n### {{name}} [{{issue}}]({{link}}) {{title}}\n  {{/hasLink}}\n  {{^hasLink}}\n### {{name}} {{title}}\n  {{/hasLink}}\n  {{#authors}}\n* {{authorName}}\n   {{#commits}}\n[{{hash}}](https://server/{{hash}}) *{{commitTime}}*\n{{{message}}}\n\n   {{/commits}}\n\n  {{/authors}}\n {{/issues}}\n{{/tags}}\n\n\nSome of the features included are:\n\n * Templating  Using Mustache templates.\n * MediaWiki  Integrated with MediaWiki to publish changelogs.\n * Jenkins plugin [https://github.com/jenkinsci/git-changelog-plugin]  To\n   publish the changelog on Jenkins summary page. Or let Jenkins create\n   MediaWiki page or file in workspace.\n * Bitbucket Server plugin\n   [https://github.com/tomasbjerre/git-changelog-bitbucket-plugin]  Adds a\n   changelog in the repository page.\n * Gradle plugin [https://github.com/tomasbjerre/git-changelog-gradle-plugin] \n   To add a changelog task to the Gradle build.\n * Maven plugin [https://github.com/tomasbjerre/git-changelog-maven-plugin]  To\n   add a changelog task to the Maven build.','/content/images/2018/02/Git-logo-black.svg.png',0,0,'published',NULL,'public',NULL,NULL,'1','2015-12-19 09:00:06','1','2018-02-24 09:11:54','1','2015-12-19 09:24:55','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a2','120ed7f4-df28-4658-819f-f1e42b1b843c','Static code analysis with GitHub','static-code-analysis-with-github','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I previously wrote about [Violation Comments to Bitbucket Server](https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Bitbucket+Server+Plugin) plugin for Jenkins. I wanted to do the same thing with GitHub and Travis, here is the result.\\n\\n![Violation comment in GitHub pull request](/content/images/2016/03/findbugs-github-pr-file-comment.png)\\n\\nYou may also have a look at [violations-test](https://github.com/tomasbjerre/violations-test/pull/2) repo where I have a live demo of this.\\n\\nEvery time I push to a pull request, or its target branch, Travis will perform static code analysis and report back to GitHub. I created a [Maven plugin](https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin) and [Gradle plugin](https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin) to make this possible. I also created a [Jenkins plugin](https://github.com/jenkinsci/violation-comments-to-github-jenkins-plugin).\\n\\nIt supports same format as [violations-lib](https://github.com/tomasbjerre/violations-lib):\\n  \\n * [**Checkstyle**](http://checkstyle.sourceforge.net/)\\n * [**CPPLint**](https://github.com/theandrewdavis/cpplint)\\n * [**CPPCheck**](http://cppcheck.sourceforge.net/)\\n * [**CSSLint**](https://github.com/CSSLint/csslint)\\n * [**Findbugs**](http://findbugs.sourceforge.net/)\\n * [**Flake8**](http://flake8.readthedocs.org/en/latest/) ([_PyLint_](https://www.pylint.org/), [_Pep8_](https://github.com/PyCQA/pycodestyle))\\n * [**JSHint**](http://jshint.com/)\\n * **Lint** A common XML format, used by different linters.\\n * [**PerlCritic**](https://github.com/Perl-Critic)\\n * [**PMD**](https://pmd.github.io/)\\n * [**ReSharper**](https://www.jetbrains.com/resharper/)\\n * [**XMLLint**](http://xmlsoft.org/xmllint.html)\\n\\nMany more formats are planned and [pull requests](https://github.com/tomasbjerre/violations-lib) are very welcome!\\n\\nThis will **not work in Travis for pull requests from forked repositories**. But **will work great with Travis for internal pull requests**. But that is a **limitation in Travis, not in the plugins** used for reporting. So if you want to do this on pull requests from forked repos you can use a private hosted build server. I created a **[Jenkins plugin](https://github.com/tomasbjerre/violation-comments-to-github-jenkins-plugin) that can be used for building pull requests from forked repositories**. You may also use the [Maven plugin](https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin) or [Gradle plugin](https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin) for that.\\n\\nHere is how to set it up in Travis.\\n\\n# Travis setup\\nYou need to have a user that is allowed to post comments on the pull request. You may use the users username and password, or an OAuth2 token. I will use OAuth2 token in this example. Here is how to create it:\\n\\n`curl -u \'yourgithubuser\' -d \'{\\\"note\\\":\\\"Violation comments\\\"}\' https://api.github.com/authorizations\\n`\\n\\nIt will prompt for you password and give you back the token.\\n\\nYou need to add it to `.travis.yml`. Travis provides a nice tool for encrypting the token:\\n```\\nsudo apt-get install ruby-dev\\ngem install travis\\ntravis encrypt export GITHUB_OAUTH2TOKEN=YOUR TOKEN HERE\\n```\\nAdd the encrypted token to your `.travis.yml` under env. I\'m including the Gradle task here also:\\n```\\nsudo: false\\nlanguage: java\\nenv:\\n  - secure: \\\"YOUR ENCRYPTED TOKEN HERE\\\"\\njdk:\\n  - oraclejdk7\\nscript:\\n  - ./gradlew build violationCommentsToGitHub -DGITHUB_PULLREQUESTID=$TRAVIS_PULL_REQUEST -DGITHUB_OAUTH2TOKEN=$GITHUB_OAUTH2TOKEN -i --stacktrace\\nnotifications:\\n  email: false\\n```\\n\\nNow you need to edit your `build.gradle` to include the plugin. As mentioned above, there is also a [Maven plugin](https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin) with the exact same functionality. This blog post may not be up to date, so best is to check [Gradle plugin](https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin) for latest version and config. But here is an example:\\n\\n```\\n  buildscript {\\n    repositories {\\n      maven {\\n        url \\\"https://plugins.gradle.org/m2/\\\"\\n      }\\n    }\\n    dependencies {\\n      classpath \\\"se.bjurr.violations:violation-comments-to-github-gradle-plugin:1.1\\\"\\n    }\\n  }\\n\\n  apply plugin: \\\"se.bjurr.violations.violation-comments-to-github-gradle-plugin\\\"\\n\\n  task violationCommentsToGitHub(type: se.bjurr.violations.comments.github.plugin.gradle.ViolationCommentsToGitHubTask) {\\n   repositoryOwner = \\\"tomasbjerre\\\";\\n   repositoryName = \\\"violations-test\\\"\\n   pullRequestId = System.properties[\'GITHUB_PULLREQUESTID\']\\n   username = System.properties[\'GITHUB_USERNAME\']\\n   password = System.properties[\'GITHUB_PASSWORD\']\\n   oAuth2Token = System.properties[\'GITHUB_OAUTH2TOKEN\']\\n   gitHubUrl = \\\"https://api.github.com/\\\"\\n   createCommentWithAllSingleFileComments = false\\n   createSingleFileComments = true\\n   violations = [\\n    [\\\"FINDBUGS\\\",   \\\".\\\", \\\".*/findbugs/.*\\\\\\\\.xml\\\\$\\\"],\\n    [\\\"PMD\\\",        \\\".\\\", \\\".*/pmd/.*\\\\\\\\.xml\\\\$\\\"],\\n    [\\\"CHECKSTYLE\\\", \\\".\\\", \\\".*/checkstyle/.*\\\\\\\\.xml\\\\$\\\"],\\n    [\\\"JSHINT\\\",     \\\".\\\", \\\".*/jshint/.*\\\\\\\\.xml\\\\$\\\"],\\n    [\\\"CSSLINT\\\",    \\\".\\\", \\\".*/csslint/.*\\\\\\\\.xml\\\\$\\\"]\\n   ]\\n  }\\n```\\n\\nNow all you need to do is to add the task to the build script, as you saw above, you need this:\\n```\\n script:\\n  - ./gradlew build violationCommentsToGitHub -DGITHUB_PULLREQUESTID=$TRAVIS_PULL_REQUEST -DGITHUB_OAUTH2TOKEN=$GITHUB_OAUTH2TOKEN -i\\n```\"}]],\"sections\":[[10,0]]}','<p>I previously wrote about <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Bitbucket+Server+Plugin\">Violation Comments to Bitbucket Server</a> plugin for Jenkins. I wanted to do the same thing with GitHub and Travis, here is the result.</p>\n<p><img src=\"/content/images/2016/03/findbugs-github-pr-file-comment.png\" alt=\"Violation comment in GitHub pull request\"></p>\n<p>You may also have a look at <a href=\"https://github.com/tomasbjerre/violations-test/pull/2\">violations-test</a> repo where I have a live demo of this.</p>\n<p>Every time I push to a pull request, or its target branch, Travis will perform static code analysis and report back to GitHub. I created a <a href=\"https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin\">Maven plugin</a> and <a href=\"https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin\">Gradle plugin</a> to make this possible. I also created a <a href=\"https://github.com/jenkinsci/violation-comments-to-github-jenkins-plugin\">Jenkins plugin</a>.</p>\n<p>It supports same format as <a href=\"https://github.com/tomasbjerre/violations-lib\">violations-lib</a>:</p>\n<ul>\n<li><a href=\"http://checkstyle.sourceforge.net/\"><strong>Checkstyle</strong></a></li>\n<li><a href=\"https://github.com/theandrewdavis/cpplint\"><strong>CPPLint</strong></a></li>\n<li><a href=\"http://cppcheck.sourceforge.net/\"><strong>CPPCheck</strong></a></li>\n<li><a href=\"https://github.com/CSSLint/csslint\"><strong>CSSLint</strong></a></li>\n<li><a href=\"http://findbugs.sourceforge.net/\"><strong>Findbugs</strong></a></li>\n<li><a href=\"http://flake8.readthedocs.org/en/latest/\"><strong>Flake8</strong></a> (<a href=\"https://www.pylint.org/\"><em>PyLint</em></a>, <a href=\"https://github.com/PyCQA/pycodestyle\"><em>Pep8</em></a>)</li>\n<li><a href=\"http://jshint.com/\"><strong>JSHint</strong></a></li>\n<li><strong>Lint</strong> A common XML format, used by different linters.</li>\n<li><a href=\"https://github.com/Perl-Critic\"><strong>PerlCritic</strong></a></li>\n<li><a href=\"https://pmd.github.io/\"><strong>PMD</strong></a></li>\n<li><a href=\"https://www.jetbrains.com/resharper/\"><strong>ReSharper</strong></a></li>\n<li><a href=\"http://xmlsoft.org/xmllint.html\"><strong>XMLLint</strong></a></li>\n</ul>\n<p>Many more formats are planned and <a href=\"https://github.com/tomasbjerre/violations-lib\">pull requests</a> are very welcome!</p>\n<p>This will <strong>not work in Travis for pull requests from forked repositories</strong>. But <strong>will work great with Travis for internal pull requests</strong>. But that is a <strong>limitation in Travis, not in the plugins</strong> used for reporting. So if you want to do this on pull requests from forked repos you can use a private hosted build server. I created a <strong><a href=\"https://github.com/tomasbjerre/violation-comments-to-github-jenkins-plugin\">Jenkins plugin</a> that can be used for building pull requests from forked repositories</strong>. You may also use the <a href=\"https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin\">Maven plugin</a> or <a href=\"https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin\">Gradle plugin</a> for that.</p>\n<p>Here is how to set it up in Travis.</p>\n<h1 id=\"travissetup\">Travis setup</h1>\n<p>You need to have a user that is allowed to post comments on the pull request. You may use the users username and password, or an OAuth2 token. I will use OAuth2 token in this example. Here is how to create it:</p>\n<p><code>curl -u \'yourgithubuser\' -d \'{&quot;note&quot;:&quot;Violation comments&quot;}\' https://api.github.com/authorizations</code></p>\n<p>It will prompt for you password and give you back the token.</p>\n<p>You need to add it to <code>.travis.yml</code>. Travis provides a nice tool for encrypting the token:</p>\n<pre><code>sudo apt-get install ruby-dev\ngem install travis\ntravis encrypt export GITHUB_OAUTH2TOKEN=YOUR TOKEN HERE\n</code></pre>\n<p>Add the encrypted token to your <code>.travis.yml</code> under env. I\'m including the Gradle task here also:</p>\n<pre><code>sudo: false\nlanguage: java\nenv:\n  - secure: &quot;YOUR ENCRYPTED TOKEN HERE&quot;\njdk:\n  - oraclejdk7\nscript:\n  - ./gradlew build violationCommentsToGitHub -DGITHUB_PULLREQUESTID=$TRAVIS_PULL_REQUEST -DGITHUB_OAUTH2TOKEN=$GITHUB_OAUTH2TOKEN -i --stacktrace\nnotifications:\n  email: false\n</code></pre>\n<p>Now you need to edit your <code>build.gradle</code> to include the plugin. As mentioned above, there is also a <a href=\"https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin\">Maven plugin</a> with the exact same functionality. This blog post may not be up to date, so best is to check <a href=\"https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin\">Gradle plugin</a> for latest version and config. But here is an example:</p>\n<pre><code>  buildscript {\n    repositories {\n      maven {\n        url &quot;https://plugins.gradle.org/m2/&quot;\n      }\n    }\n    dependencies {\n      classpath &quot;se.bjurr.violations:violation-comments-to-github-gradle-plugin:1.1&quot;\n    }\n  }\n\n  apply plugin: &quot;se.bjurr.violations.violation-comments-to-github-gradle-plugin&quot;\n\n  task violationCommentsToGitHub(type: se.bjurr.violations.comments.github.plugin.gradle.ViolationCommentsToGitHubTask) {\n   repositoryOwner = &quot;tomasbjerre&quot;;\n   repositoryName = &quot;violations-test&quot;\n   pullRequestId = System.properties[\'GITHUB_PULLREQUESTID\']\n   username = System.properties[\'GITHUB_USERNAME\']\n   password = System.properties[\'GITHUB_PASSWORD\']\n   oAuth2Token = System.properties[\'GITHUB_OAUTH2TOKEN\']\n   gitHubUrl = &quot;https://api.github.com/&quot;\n   createCommentWithAllSingleFileComments = false\n   createSingleFileComments = true\n   violations = [\n    [&quot;FINDBUGS&quot;,   &quot;.&quot;, &quot;.*/findbugs/.*\\\\.xml\\$&quot;],\n    [&quot;PMD&quot;,        &quot;.&quot;, &quot;.*/pmd/.*\\\\.xml\\$&quot;],\n    [&quot;CHECKSTYLE&quot;, &quot;.&quot;, &quot;.*/checkstyle/.*\\\\.xml\\$&quot;],\n    [&quot;JSHINT&quot;,     &quot;.&quot;, &quot;.*/jshint/.*\\\\.xml\\$&quot;],\n    [&quot;CSSLINT&quot;,    &quot;.&quot;, &quot;.*/csslint/.*\\\\.xml\\$&quot;]\n   ]\n  }\n</code></pre>\n<p>Now all you need to do is to add the task to the build script, as you saw above, you need this:</p>\n<pre><code> script:\n  - ./gradlew build violationCommentsToGitHub -DGITHUB_PULLREQUESTID=$TRAVIS_PULL_REQUEST -DGITHUB_OAUTH2TOKEN=$GITHUB_OAUTH2TOKEN -i\n</code></pre>\n','24','I previously wrote about Violation Comments to Bitbucket Server\n[https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Bitbucket+Server+Plugin] \n plugin for Jenkins. I wanted to do the same thing with GitHub and Travis, here\nis the result.\n\n\n\nYou may also have a look at violations-test\n[https://github.com/tomasbjerre/violations-test/pull/2]  repo where I have a\nlive demo of this.\n\nEvery time I push to a pull request, or its target branch, Travis will perform\nstatic code analysis and report back to GitHub. I created a Maven plugin\n[https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin]  and \nGradle plugin\n[https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin]  to\nmake this possible. I also created a Jenkins plugin\n[https://github.com/jenkinsci/violation-comments-to-github-jenkins-plugin].\n\nIt supports same format as violations-lib\n[https://github.com/tomasbjerre/violations-lib]:\n\n * Checkstyle [http://checkstyle.sourceforge.net/]\n * CPPLint [https://github.com/theandrewdavis/cpplint]\n * CPPCheck [http://cppcheck.sourceforge.net/]\n * CSSLint [https://github.com/CSSLint/csslint]\n * Findbugs [http://findbugs.sourceforge.net/]\n * Flake8 [http://flake8.readthedocs.org/en/latest/]  (PyLint\n   [https://www.pylint.org/], Pep8 [https://github.com/PyCQA/pycodestyle])\n * JSHint [http://jshint.com/]\n * Lint  A common XML format, used by different linters.\n * PerlCritic [https://github.com/Perl-Critic]\n * PMD [https://pmd.github.io/]\n * ReSharper [https://www.jetbrains.com/resharper/]\n * XMLLint [http://xmlsoft.org/xmllint.html]\n\nMany more formats are planned and pull requests\n[https://github.com/tomasbjerre/violations-lib]  are very welcome!\n\nThis will not work in Travis for pull requests from forked repositories. But \nwill work great with Travis for internal pull requests. But that is a limitation\nin Travis, not in the plugins  used for reporting. So if you want to do this on\npull requests from forked repos you can use a private hosted build server. I\ncreated a Jenkins plugin\n[https://github.com/tomasbjerre/violation-comments-to-github-jenkins-plugin] \nthat can be used for building pull requests from forked repositories. You may\nalso use the Maven plugin\n[https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin]  or \nGradle plugin\n[https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin]  for\nthat.\n\nHere is how to set it up in Travis.\n\nTravis setup\nYou need to have a user that is allowed to post comments on the pull request.\nYou may use the users username and password, or an OAuth2 token. I will use\nOAuth2 token in this example. Here is how to create it:\n\ncurl -u \'yourgithubuser\' -d \'{\"note\":\"Violation comments\"}\'\nhttps://api.github.com/authorizations\n\nIt will prompt for you password and give you back the token.\n\nYou need to add it to .travis.yml. Travis provides a nice tool for encrypting\nthe token:\n\nsudo apt-get install ruby-dev\ngem install travis\ntravis encrypt export GITHUB_OAUTH2TOKEN=YOUR TOKEN HERE\n\n\nAdd the encrypted token to your .travis.yml  under env. I\'m including the Gradle\ntask here also:\n\nsudo: false\nlanguage: java\nenv:\n  - secure: \"YOUR ENCRYPTED TOKEN HERE\"\njdk:\n  - oraclejdk7\nscript:\n  - ./gradlew build violationCommentsToGitHub -DGITHUB_PULLREQUESTID=$TRAVIS_PULL_REQUEST -DGITHUB_OAUTH2TOKEN=$GITHUB_OAUTH2TOKEN -i --stacktrace\nnotifications:\n  email: false\n\n\nNow you need to edit your build.gradle  to include the plugin. As mentioned\nabove, there is also a Maven plugin\n[https://github.com/tomasbjerre/violation-comments-to-github-maven-plugin]  with\nthe exact same functionality. This blog post may not be up to date, so best is\nto check Gradle plugin\n[https://github.com/tomasbjerre/violation-comments-to-github-gradle-plugin]  for\nlatest version and config. But here is an example:\n\n  buildscript {\n    repositories {\n      maven {\n        url \"https://plugins.gradle.org/m2/\"\n      }\n    }\n    dependencies {\n      classpath \"se.bjurr.violations:violation-comments-to-github-gradle-plugin:1.1\"\n    }\n  }\n\n  apply plugin: \"se.bjurr.violations.violation-comments-to-github-gradle-plugin\"\n\n  task violationCommentsToGitHub(type: se.bjurr.violations.comments.github.plugin.gradle.ViolationCommentsToGitHubTask) {\n   repositoryOwner = \"tomasbjerre\";\n   repositoryName = \"violations-test\"\n   pullRequestId = System.properties[\'GITHUB_PULLREQUESTID\']\n   username = System.properties[\'GITHUB_USERNAME\']\n   password = System.properties[\'GITHUB_PASSWORD\']\n   oAuth2Token = System.properties[\'GITHUB_OAUTH2TOKEN\']\n   gitHubUrl = \"https://api.github.com/\"\n   createCommentWithAllSingleFileComments = false\n   createSingleFileComments = true\n   violations = [\n    [\"FINDBUGS\",   \".\", \".*/findbugs/.*\\\\.xml\\$\"],\n    [\"PMD\",        \".\", \".*/pmd/.*\\\\.xml\\$\"],\n    [\"CHECKSTYLE\", \".\", \".*/checkstyle/.*\\\\.xml\\$\"],\n    [\"JSHINT\",     \".\", \".*/jshint/.*\\\\.xml\\$\"],\n    [\"CSSLINT\",    \".\", \".*/csslint/.*\\\\.xml\\$\"]\n   ]\n  }\n\n\nNow all you need to do is to add the task to the build script, as you saw above,\nyou need this:\n\n script:\n  - ./gradlew build violationCommentsToGitHub -DGITHUB_PULLREQUESTID=$TRAVIS_PULL_REQUEST -DGITHUB_OAUTH2TOKEN=$GITHUB_OAUTH2TOKEN -i','/content/images/2018/02/GitHub-Mark.png',0,0,'published',NULL,'public',NULL,NULL,'1','2016-03-04 21:01:59','1','2018-02-24 09:10:30','1','2016-03-04 21:30:58','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a3','3cc02964-4f5d-417d-89f5-85ded3b9ba07','Clean invocations of dirty methods','clean-invocations-of-dirty-methods','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently started to implement a [client for a REST API](https://github.com/tomasbjerre/bitbucket-server-java-client). I was using [RetroFit](http://square.github.io/retrofit/) and here is one the resources used by the client.\\n```\\npublic interface BitBucketServerService {\\n @GET(\\\"/rest/api/1.0/projects/{projectkey}/repos/{repositoryslug}/pull-requests?direction={direction}&at={at}&state={state}&order={order}&withattributes={withattributes}&withproperties={withproperties}\\\")\\n Call<BitbucketServerResponse<BitBucketServerPullRequest>> pullRequests(//\\n   @Query(\\\"projectkey\\\") String projectKey,//\\n   @Query(\\\"repositoryslug\\\") String repositoryslug,//\\n   @Query(\\\"direction\\\") String direction,//\\n   @Query(\\\"at\\\") Integer at,//\\n   @Query(\\\"state\\\") String state,//\\n   @Query(\\\"order\\\") String order,//\\n   @Query(\\\"withattributes\\\") boolean withattributes,//\\n   @Query(\\\"withproperties\\\") boolean withproperties);\\n}\\n```\\n\\nThe problem here is that calls to this service will be dirty. Alot of parameters in each invocation. Alot of strings that can accidentally be added in the wrong order. So just for comparison, here is an invocation of that service.\\n```\\nbitBucketServerService //\\n .pullRequests(\\\"projectKey\\\", \\\"repositoryslug\\\", \\\"direction\\\", 1, \\\"state\\\", \\\"order\\\", true, true);\\n```\\n\\nI created [Java Method Invocation Builder](https://github.com/tomasbjerre/java-method-invocation-builder). It adds the `@GenerateMethodInvocationBuilder` and also `@Default`. They are added to an interface, or class. It enables default values of method parameters  in Java and is making the invocations readable.\\n\\n```\\n@GenerateMethodInvocationBuilder\\npublic interface BitBucketServerService {\\n @GET(\\\"/rest/api/1.0/projects/{projectkey}/repos/{repositoryslug}/pull-requests?direction={direction}&at={at}&state={state}&order={order}&withattributes={withattributes}&withproperties={withproperties}\\\")\\n Call<BitbucketServerResponse<BitBucketServerPullRequest>> pullRequests(//\\n   @Query(\\\"projectkey\\\") String projectKey,//\\n   @Query(\\\"repositoryslug\\\") String repositoryslug,//\\n   @Default(\\\"INCOMING\\\") @Query(\\\"direction\\\") String direction,//\\n   @Query(\\\"at\\\") Integer at,//\\n   @Default(\\\"OPEN\\\") @Query(\\\"state\\\") String state,//\\n   @Default(\\\"NEWEST\\\") @Query(\\\"order\\\") String order,//\\n   @Default(\\\"true\\\") @Query(\\\"withattributes\\\") boolean withattributes,//\\n   @Default(\\\"true\\\") @Query(\\\"withproperties\\\") boolean withproperties);\\n}\\n```\\n\\n[Java Method Invocation Builder](https://github.com/tomasbjerre/java-method-invocation-builder) will generate builders for invoking every method in the **interface**, **or class**. So that the invoking code can look like this instead.\\n\\n```\\n  BitBucketServerServicePullRequestsBuilder.pullRequests()//\\n    .withProjectKey(\\\"projectKey\\\")//\\n    .withRepositoryslug(\\\"repositoryslug\\\")//\\n    .withAt(123)//\\n    .invoke(bitBucketServerService);\\n```\\n\\nThe code is generated as Java files at compile time. I created [Maven](https://github.com/tomasbjerre/java-method-invocation-builder/tree/master/example-maven) and [Gradle](https://github.com/tomasbjerre/java-method-invocation-builder/tree/master/example-gradle) examples to show how to use it.\\n\\n![Example](/content/images/2016/06/example_usage.png)\\n\"}]],\"sections\":[[10,0]]}','<p>I recently started to implement a <a href=\"https://github.com/tomasbjerre/bitbucket-server-java-client\">client for a REST API</a>. I was using <a href=\"http://square.github.io/retrofit/\">RetroFit</a> and here is one the resources used by the client.</p>\n<pre><code>public interface BitBucketServerService {\n @GET(&quot;/rest/api/1.0/projects/{projectkey}/repos/{repositoryslug}/pull-requests?direction={direction}&amp;at={at}&amp;state={state}&amp;order={order}&amp;withattributes={withattributes}&amp;withproperties={withproperties}&quot;)\n Call&lt;BitbucketServerResponse&lt;BitBucketServerPullRequest&gt;&gt; pullRequests(//\n   @Query(&quot;projectkey&quot;) String projectKey,//\n   @Query(&quot;repositoryslug&quot;) String repositoryslug,//\n   @Query(&quot;direction&quot;) String direction,//\n   @Query(&quot;at&quot;) Integer at,//\n   @Query(&quot;state&quot;) String state,//\n   @Query(&quot;order&quot;) String order,//\n   @Query(&quot;withattributes&quot;) boolean withattributes,//\n   @Query(&quot;withproperties&quot;) boolean withproperties);\n}\n</code></pre>\n<p>The problem here is that calls to this service will be dirty. Alot of parameters in each invocation. Alot of strings that can accidentally be added in the wrong order. So just for comparison, here is an invocation of that service.</p>\n<pre><code>bitBucketServerService //\n .pullRequests(&quot;projectKey&quot;, &quot;repositoryslug&quot;, &quot;direction&quot;, 1, &quot;state&quot;, &quot;order&quot;, true, true);\n</code></pre>\n<p>I created <a href=\"https://github.com/tomasbjerre/java-method-invocation-builder\">Java Method Invocation Builder</a>. It adds the <code>@GenerateMethodInvocationBuilder</code> and also <code>@Default</code>. They are added to an interface, or class. It enables default values of method parameters  in Java and is making the invocations readable.</p>\n<pre><code>@GenerateMethodInvocationBuilder\npublic interface BitBucketServerService {\n @GET(&quot;/rest/api/1.0/projects/{projectkey}/repos/{repositoryslug}/pull-requests?direction={direction}&amp;at={at}&amp;state={state}&amp;order={order}&amp;withattributes={withattributes}&amp;withproperties={withproperties}&quot;)\n Call&lt;BitbucketServerResponse&lt;BitBucketServerPullRequest&gt;&gt; pullRequests(//\n   @Query(&quot;projectkey&quot;) String projectKey,//\n   @Query(&quot;repositoryslug&quot;) String repositoryslug,//\n   @Default(&quot;INCOMING&quot;) @Query(&quot;direction&quot;) String direction,//\n   @Query(&quot;at&quot;) Integer at,//\n   @Default(&quot;OPEN&quot;) @Query(&quot;state&quot;) String state,//\n   @Default(&quot;NEWEST&quot;) @Query(&quot;order&quot;) String order,//\n   @Default(&quot;true&quot;) @Query(&quot;withattributes&quot;) boolean withattributes,//\n   @Default(&quot;true&quot;) @Query(&quot;withproperties&quot;) boolean withproperties);\n}\n</code></pre>\n<p><a href=\"https://github.com/tomasbjerre/java-method-invocation-builder\">Java Method Invocation Builder</a> will generate builders for invoking every method in the <strong>interface</strong>, <strong>or class</strong>. So that the invoking code can look like this instead.</p>\n<pre><code>  BitBucketServerServicePullRequestsBuilder.pullRequests()//\n    .withProjectKey(&quot;projectKey&quot;)//\n    .withRepositoryslug(&quot;repositoryslug&quot;)//\n    .withAt(123)//\n    .invoke(bitBucketServerService);\n</code></pre>\n<p>The code is generated as Java files at compile time. I created <a href=\"https://github.com/tomasbjerre/java-method-invocation-builder/tree/master/example-maven\">Maven</a> and <a href=\"https://github.com/tomasbjerre/java-method-invocation-builder/tree/master/example-gradle\">Gradle</a> examples to show how to use it.</p>\n<p><img src=\"/content/images/2016/06/example_usage.png\" alt=\"Example\"></p>\n','25','I recently started to implement a client for a REST API\n[https://github.com/tomasbjerre/bitbucket-server-java-client]. I was using \nRetroFit [http://square.github.io/retrofit/]  and here is one the resources used\nby the client.\n\npublic interface BitBucketServerService {\n @GET(\"/rest/api/1.0/projects/{projectkey}/repos/{repositoryslug}/pull-requests?direction={direction}&at={at}&state={state}&order={order}&withattributes={withattributes}&withproperties={withproperties}\")\n Call<BitbucketServerResponse<BitBucketServerPullRequest>> pullRequests(//\n   @Query(\"projectkey\") String projectKey,//\n   @Query(\"repositoryslug\") String repositoryslug,//\n   @Query(\"direction\") String direction,//\n   @Query(\"at\") Integer at,//\n   @Query(\"state\") String state,//\n   @Query(\"order\") String order,//\n   @Query(\"withattributes\") boolean withattributes,//\n   @Query(\"withproperties\") boolean withproperties);\n}\n\n\nThe problem here is that calls to this service will be dirty. Alot of parameters\nin each invocation. Alot of strings that can accidentally be added in the wrong\norder. So just for comparison, here is an invocation of that service.\n\nbitBucketServerService //\n .pullRequests(\"projectKey\", \"repositoryslug\", \"direction\", 1, \"state\", \"order\", true, true);\n\n\nI created Java Method Invocation Builder\n[https://github.com/tomasbjerre/java-method-invocation-builder]. It adds the \n@GenerateMethodInvocationBuilder  and also @Default. They are added to an\ninterface, or class. It enables default values of method parameters in Java and\nis making the invocations readable.\n\n@GenerateMethodInvocationBuilder\npublic interface BitBucketServerService {\n @GET(\"/rest/api/1.0/projects/{projectkey}/repos/{repositoryslug}/pull-requests?direction={direction}&at={at}&state={state}&order={order}&withattributes={withattributes}&withproperties={withproperties}\")\n Call<BitbucketServerResponse<BitBucketServerPullRequest>> pullRequests(//\n   @Query(\"projectkey\") String projectKey,//\n   @Query(\"repositoryslug\") String repositoryslug,//\n   @Default(\"INCOMING\") @Query(\"direction\") String direction,//\n   @Query(\"at\") Integer at,//\n   @Default(\"OPEN\") @Query(\"state\") String state,//\n   @Default(\"NEWEST\") @Query(\"order\") String order,//\n   @Default(\"true\") @Query(\"withattributes\") boolean withattributes,//\n   @Default(\"true\") @Query(\"withproperties\") boolean withproperties);\n}\n\n\nJava Method Invocation Builder\n[https://github.com/tomasbjerre/java-method-invocation-builder]  will generate\nbuilders for invoking every method in the interface, or class. So that the\ninvoking code can look like this instead.\n\n  BitBucketServerServicePullRequestsBuilder.pullRequests()//\n    .withProjectKey(\"projectKey\")//\n    .withRepositoryslug(\"repositoryslug\")//\n    .withAt(123)//\n    .invoke(bitBucketServerService);\n\n\nThe code is generated as Java files at compile time. I created Maven\n[https://github.com/tomasbjerre/java-method-invocation-builder/tree/master/example-maven] \n and Gradle\n[https://github.com/tomasbjerre/java-method-invocation-builder/tree/master/example-gradle] \n examples to show how to use it.','/content/images/2018/02/java-1.png',0,0,'published',NULL,'public',NULL,NULL,'1','2016-06-11 16:30:29','1','2018-02-24 09:08:53','1','2016-06-11 16:53:14','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a4','3fd3272e-d690-478c-b03e-742624228a64','Continuous Integration with Bitbucket Server and Jenkins','continuous-integration-with-bitbucket-server-and-jenkins','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I have started several projects where I develop plugins for Bitbucket Server and Jenkins. They are independent of each other but I thought it would be nice to have a blog post about how they can work together and on how I prefer to do continuous integration. This is it! =)\\n\\nWhat is the configuration that I apply?\\n\\n* [Gitflow](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow) workflow.\\n* Every commit, message and code, has to be reviewed before it can be merged.\\n* Every commit has to have an issue in its message.\\n* The size of the files that are put under version control cannot exceed 2000kb.\\n* The committer must use valid name and email in all commits.\\n* Changes can only be made with *feature branches* and merged with *pull requests*.\\n* *Pull requests* cannot be merged unless it its guaranteed that the target branch will still build after merge.\\n* *Pull requests* cannot be merged unless, at least, one other developer has reviewed it.\\n* *Static code analysis* will be made on every *pull request*.\\n\\nWhy do I apply this configuration?\\n\\n* **Git flow enables you to**\\n  * Use plugins for managing releases, like [JGit-Flow](https://bitbucket.org/atlassian/jgit-flow/wiki/Home).\\n  * Reduce time spent on documentation. You just need to refer to some [youtube-videos](https://www.youtube.com/results?search_query=git+flow) or [websites](http://lmgtfy.com/?q=git+flow).\\n  * Discuss any scenario on any public forum like [Stackoverflow](http://stackoverflow.com/search?q=git+flow).\\n  * Let a configuration manager have total control of *master* and *release*-branches. While developers have total control of *dev* and *feature*-branches.\\n  * Some people skip *dev* and use *master* as developer branch. I keep *dev* because when developing tools around GIT, its convenient to have a reference to latest release, *master*, and latest snapshot, *dev*. And since branches in git are basically just a file with a hash I think its a small price to pay for keeping it nice and tidy.\\n* **You will know exactly what is included in your releases**. The commits are reviewed, so you can trust that the message is true. Which means you can create your release notes by looking at the new commits in *dev* that are not yet merged to *master* (production). A special QA-jira is created to use for small fixes, like correcting toggling test case or formatting code.\\n* **You will make the code more maintainable and share knowledge among developers**. When tracking down a bug, the commit messages you find will be clear and understandable. Also the code will be cleaner from the reviews. Duplicated utility classes, diverging patterns... and so on will be denied.\\n* **You will never have a failing build on a shared branch**. Merge is only allowed, blocked by the Bitbucket Server, if it is guaranteed that target will still build after merge. Actually, if you have toggling test cases they can fail. But that should be found in code reviews.\\n* **Testers can pick a feature and try it out in a test environment before its merged to *dev***. By letting *Jenkins* perform the merge (without pushing), build the artifacts and deploy to an environment.\\n* **Your static code analysis will be honored**. You will see dramatically reduced amount of warnings from static code analysis. The developers will not be able to ignore such warnings (often the case with *Sonar*) as they will automatically be converted into comments on the code in the *pull request*.\\n* **Your repository will not quickly increase in size**. Remember that when you clone a Git repo you will download every version of everything. Its mostly a one time thing for a developer but something that a build server does several times a day. Once your repo has grown big its a bit of a pain getting rid of those large files.\\n\\nAnd finally, how do I apply the configuration? With Jenkins, Bitbucket Server (some of this is also possible to do if you are using GitHub) and plugins! I created a small [Docker Compose](https://github.com/tomasbjerre/jenkinsbitbucket) to help fiddle with this.\\n\\n# Bitbucket Server\\nThis is the Git repo that I use. It adds some things that Git does not have:\\n\\n* Authentication.\\n* Branch, repo and project permissions.\\n* Pull request support.\\n* Code review support.\\n* Alot of plugins.\\n\\nThe plugins I will use adds support for:\\n\\n * Commit checks with [Simple Bitbucket Server Commit Checker](https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc/server/overview).\\n * Notifies Jenkins to perform verification of pull requests with [Pull Request Notifier for Bitbucket Server](https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash/server/overview).\\n * Synchronizing settings among repositories with [Settings Synchronizer for Bitbucket Server](https://marketplace.atlassian.com/plugins/se.bjurr.ssfb.settings-synchronizer-for-bitbucket/server/overview) \\n\\nYou can think about Git as an open database. Anyone can change anything. Anyone can add commits to any branch, with any author/committer name/email. It is very dangerous to collaborate around a repo that is not protected with a service like Bitbucket Server.\\n\\n## Simple Bitbucket Server Commit Checker\\nThis is one of my plugins. Its available for easy install from [Atlassian Marketplace](https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc/server/overview) and is developed [on Github](https://github.com/tomasbjerre/simple-bitbucket-commit-checker).\\n\\nThe main reason why I developed this plugin was to allow custom rejection messages. The rejection messages also allows you to use variables so that you can reject a commit with a message saying exactly what the committer needs to do in order to fixup the commit. It also has a *dry run* mode where it only complains about the things that are not correct, but does not actually block the commit. It may be crucial to have such a feature if you introduce commit checks for 100+ committers.\\n\\n![Block committer emails not in Bitbucket Server](/content/images/2016/09/sscc-email.png)\\n\\nThe rejection messages are very clear.\\n\\n![Email rejected](/content/images/2016/09/sscc-reject.png)\\n\\nIt uses Jira:s [JQL](https://confluence.atlassian.com/jirasoftwarecloud/advanced-searching-764478330.html#Advancedsearching-ConstructingJQLqueries) to enable advanced and flexible validation of commit comments. It does not have to be an advanced query, simply adding `issue = ${REGEXP}` will validate that the Jiras exists.\\n\\n![JQL](/content/images/2016/09/sscc-issues.png)\\n\\nI use it to block commits:\\n\\n* Contains files that are too big, larger then 2000kb.\\n* Committer email or name does not match authenticated user in Bitbucket Server.\\n* Message does not contain an issue. Jira or custom incident pattern. I create a special issue in jira, a QA-jira, that can be use for small fixes. A small fix might be correcting toggling test case or formatting code.\\n\\n## Pull Request Notifier for Bitbucket Server\\nThis is one of my plugins. Its available for easy install from [Atlassian Marketplace](https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash/server/overview) and is developed [on Github](https://github.com/tomasbjerre/pull-request-notifier-for-bitbucket).\\n\\nIt invokes a URL when a pull request event happens in Bitbucket Server. The URL is completely customizable and supports variables so that you can notify any system with the information it needs on the format it needs it. \\n\\n![Pull Request Notifier for Bitbucket Server URL config](/content/images/2016/09/pull-request-notifier-url-config.png)\\n\\nThe main reason why I developed this plugin was to allow perfect verification of pull requests in Jenkins. But it is written in a way so that it is in no way limited to Jenkins. It basically just interacts with HTTP and can be used for many other purposes. I know people use it for posting updates regarding pull requests in Slack channels.\\n\\nIt enables you to guarantee that pull requests cannot be merged unless target branch will still build after the merge. If one of several pull requests to *dev* is merged, the plugin can re-trigger all other pull requests to verify them with the new commit that is now *dev*.\\n\\nIt adds support for *custom buttons* on pull request view. These buttons are mapped to a *notifications* (a URL being invoked). This means you can have a button labeled \\\"*Deploy to test env 1*\\\" and when it is pressed you trigger a job in Jenkins. The job is served with the feature branch of the pull request, builds the artifact and deploys it to that environment.\\n\\n![Pull Request Notifier for Bitbucket Server Button Config](/content/images/2016/09/prnfb-buttons.png)\\n\\nAnd in the pull request view you will see that button to the right, when clicking the dots.\\n\\n![Pull Request Notifier for Bitbucket Server Button In PR view](/content/images/2016/09/prnfb-button-pr-view.png)\\n\\nThe button will only be made visible if there is a configured notification that will actually trigger on the button being pressed. So make sure you add that also.\\n\\n![Pull Request Notifier for Bitucket Server Triggers](/content/images/2016/09/prnfb-triggers.png)\\n\\n## Settings Synchronizer for Bitbucket Server\\nThis is one of my plugins. Its available for easy install from [Atlassian Marketplace](https://marketplace.atlassian.com/plugins/se.bjurr.ssfb.settings-synchronizer-for-bitbucket/server/overview) and is developed [on Github](https://github.com/tomasbjerre/settings-synchronizer-for-bitbucket-plugin).\\n\\nIt is a bit of a pain to configure plugins for a 100, or even just a couple, of repositories. This plugin lets you synchronize plugin settings among your repositories.\\n\\n![Settings Synchronizer for Bitbucket Server](/content/images/2016/10/repoadmin.png)\\n\\n# Jenkins\\nThe build server that I use. Some say its just a glorified cron job. And yes thats pretty much it. The good thing about it is all the plugins. I dont think I would ever use it if it was not for the plugins.\\n\\nWhen I verify the pull requests I do it with a simple shell script build step.\\n\\n * From [Pull Request Notifier for Bitbucket Server](https://github.com/tomasbjerre/pull-request-notifier-for-bitbucket) I pass the `${EVERYTHING_URL}`.\\n * I check the job as parameterized and add the variables, used below, ass parameters.\\n * I check delay and set it to 0 seconds.\\n * I check allow parallel jobs.\\n\\nThe script is something like this.\\n\\n```\\ngit clone $PULL_REQUEST_TO_HTTP_CLONE_URL\\ncd *  \\ngit reset --hard $PULL_REQUEST_TO_HASH\\ngit status  \\ngit remote add from $PULL_REQUEST_FROM_HTTP_CLONE_URL\\ngit fetch --all\\ngit merge $PULL_REQUEST_FROM_HASH\\ngit --no-pager log --max-count=10 --graph --abbrev-commit\\n\\n#compile command here ...\\n```\\n\\n## Violation Comments to Bitbucket Server Plugin\\nThis is one of my Jenkins plugins. It is available in [Jenkins update sites](https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Bitbucket+Server+Plugin) and is developed [on Github](https://github.com/jenkinsci/violation-comments-to-stash-plugin). There is also a [Jenkins plugin for Github](https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Github+Plugin) if that is what you are using.\\n\\nMy opinion is that projects often put to much trust in that developers themselves will adhere to rules specified by static code analysis tools. They often trust that developers will have their IDE setup correctly to report problems. They also trust that developers will browse to [SonarQube](http://www.sonarqube.org/), every now and then, and find problems to fix. It is my strongest opinion that this never works. Some developers are really enthusiastic about it in the beginning but soon forgets to check for problems. Some never even cares in the first place. Commenting the pull requests with the problems found, makes them hard to ignore.\\n\\nWhen a pull request is verified, part of the verification is static code analysis. This plugin finds report files generated from that analysis and uses them to comment the pull request in Bitbucket Server.\\n\\n![Violation Comments to Bitbucket Server](/content/images/2016/09/screenshot-stash.png)\\n\\nThe post build action may look something like this.\\n\\n![Violation Comments to Bitbucket Server Post Build Action](/content/images/2016/09/violation-comments-to-bitbucket.png)\\n\\n## Git Changelog Plugin\\nThis is one of my Jenkins plugins. It is available in [Jenkins update sites](https://wiki.jenkins-ci.org/display/JENKINS/Git+Changelog+Plugin) and is developed [on Github](https://github.com/jenkinsci/git-changelog-plugin).\\n\\nThere may be several commits in the feature branch that is the source of the pull request. It may be an integration branch where developers has been working for weeks or even months. Then its very nice get a more organized view of what the feature branch actually contains.\\n\\n![Git Changelog Jenkins Plugin Configuration](/content/images/2016/09/git-changelog-postaction1.png)\\n\\nThis plugin is configured with a template and then creates a changelog where the template is rendered with the content of the feature branch.\\n\\n![Git Changelog Jenkins Plugin Template Configuration](/content/images/2016/09/git-changelog-postaction2.png)\\n\\nIt may look something like this if you chose to publish it on Jenkins job summary page. I used a Github repo here in the example because I dont have access to a Jira installation, but it works just the same with Jira! =)\\n\\n![Git Changelog Jenkins Summary](/content/images/2016/09/gitchangelog-prnfb-github.png)\\n\\nYou can also use this plugin to create release notes, just select *dev* as source branch and *master* as target branch. If you have a Jenkins job for *dev* then that is where to put that. It includes **integration with MediaWiki** so that releasenotes can be posted there. You can also chose to **create a file** where you are totally free to create a formatted HTML with CSS, or just plain text, the way you like it.\\n\\n## Stash Notifier Plugin\\nIt is available in [Jenkins update sites](https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin). It adds a post build step in Jenkns.\\n\\n![Post Build Step in Jenkins](/content/images/2016/09/stash-notifier-plugin.png)\\n\\nIt also reveals build status on Pull request view in Bitbucket Server.\\n\\n![Build status in Bitbucket Server](/content/images/2016/09/stash-notifier-in-bitbucket.png)\\n\\n# Conclusions\\nHope you found something useful here. Pull requests and suggested features are always welcome. Please dont email me directly but instead try to file issues on GitHub!\"}]],\"markups\":[],\"sections\":[[10,0]]}','<p>I have started several projects where I develop plugins for Bitbucket Server and Jenkins. They are independent of each other but I thought it would be nice to have a blog post about how they can work together and on how I prefer to do continuous integration. This is it! =)</p>\n<p>What is the configuration that I apply?</p>\n<ul>\n<li><a href=\"https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow\">Gitflow</a> workflow.</li>\n<li>Every commit, message and code, has to be reviewed before it can be merged.</li>\n<li>Every commit has to have an issue in its message.</li>\n<li>The size of the files that are put under version control cannot exceed 2000kb.</li>\n<li>The committer must use valid name and email in all commits.</li>\n<li>Changes can only be made with <em>feature branches</em> and merged with <em>pull requests</em>.</li>\n<li><em>Pull requests</em> cannot be merged unless it its guaranteed that the target branch will still build after merge.</li>\n<li><em>Pull requests</em> cannot be merged unless, at least, one other developer has reviewed it.</li>\n<li><em>Static code analysis</em> will be made on every <em>pull request</em>.</li>\n</ul>\n<p>Why do I apply this configuration?</p>\n<ul>\n<li><strong>Git flow enables you to</strong>\n<ul>\n<li>Use plugins for managing releases, like <a href=\"https://bitbucket.org/atlassian/jgit-flow/wiki/Home\">JGit-Flow</a>.</li>\n<li>Reduce time spent on documentation. You just need to refer to some <a href=\"https://www.youtube.com/results?search_query=git+flow\">youtube-videos</a> or <a href=\"http://lmgtfy.com/?q=git+flow\">websites</a>.</li>\n<li>Discuss any scenario on any public forum like <a href=\"http://stackoverflow.com/search?q=git+flow\">Stackoverflow</a>.</li>\n<li>Let a configuration manager have total control of <em>master</em> and <em>release</em>-branches. While developers have total control of <em>dev</em> and <em>feature</em>-branches.</li>\n<li>Some people skip <em>dev</em> and use <em>master</em> as developer branch. I keep <em>dev</em> because when developing tools around GIT, its convenient to have a reference to latest release, <em>master</em>, and latest snapshot, <em>dev</em>. And since branches in git are basically just a file with a hash I think its a small price to pay for keeping it nice and tidy.</li>\n</ul>\n</li>\n<li><strong>You will know exactly what is included in your releases</strong>. The commits are reviewed, so you can trust that the message is true. Which means you can create your release notes by looking at the new commits in <em>dev</em> that are not yet merged to <em>master</em> (production). A special QA-jira is created to use for small fixes, like correcting toggling test case or formatting code.</li>\n<li><strong>You will make the code more maintainable and share knowledge among developers</strong>. When tracking down a bug, the commit messages you find will be clear and understandable. Also the code will be cleaner from the reviews. Duplicated utility classes, diverging patterns... and so on will be denied.</li>\n<li><strong>You will never have a failing build on a shared branch</strong>. Merge is only allowed, blocked by the Bitbucket Server, if it is guaranteed that target will still build after merge. Actually, if you have toggling test cases they can fail. But that should be found in code reviews.</li>\n<li><strong>Testers can pick a feature and try it out in a test environment before its merged to <em>dev</em></strong>. By letting <em>Jenkins</em> perform the merge (without pushing), build the artifacts and deploy to an environment.</li>\n<li><strong>Your static code analysis will be honored</strong>. You will see dramatically reduced amount of warnings from static code analysis. The developers will not be able to ignore such warnings (often the case with <em>Sonar</em>) as they will automatically be converted into comments on the code in the <em>pull request</em>.</li>\n<li><strong>Your repository will not quickly increase in size</strong>. Remember that when you clone a Git repo you will download every version of everything. Its mostly a one time thing for a developer but something that a build server does several times a day. Once your repo has grown big its a bit of a pain getting rid of those large files.</li>\n</ul>\n<p>And finally, how do I apply the configuration? With Jenkins, Bitbucket Server (some of this is also possible to do if you are using GitHub) and plugins! I created a small <a href=\"https://github.com/tomasbjerre/jenkinsbitbucket\">Docker Compose</a> to help fiddle with this.</p>\n<h1 id=\"bitbucketserver\">Bitbucket Server</h1>\n<p>This is the Git repo that I use. It adds some things that Git does not have:</p>\n<ul>\n<li>Authentication.</li>\n<li>Branch, repo and project permissions.</li>\n<li>Pull request support.</li>\n<li>Code review support.</li>\n<li>Alot of plugins.</li>\n</ul>\n<p>The plugins I will use adds support for:</p>\n<ul>\n<li>Commit checks with <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc/server/overview\">Simple Bitbucket Server Commit Checker</a>.</li>\n<li>Notifies Jenkins to perform verification of pull requests with <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash/server/overview\">Pull Request Notifier for Bitbucket Server</a>.</li>\n<li>Synchronizing settings among repositories with <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.ssfb.settings-synchronizer-for-bitbucket/server/overview\">Settings Synchronizer for Bitbucket Server</a></li>\n</ul>\n<p>You can think about Git as an open database. Anyone can change anything. Anyone can add commits to any branch, with any author/committer name/email. It is very dangerous to collaborate around a repo that is not protected with a service like Bitbucket Server.</p>\n<h2 id=\"simplebitbucketservercommitchecker\">Simple Bitbucket Server Commit Checker</h2>\n<p>This is one of my plugins. Its available for easy install from <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc/server/overview\">Atlassian Marketplace</a> and is developed <a href=\"https://github.com/tomasbjerre/simple-bitbucket-commit-checker\">on Github</a>.</p>\n<p>The main reason why I developed this plugin was to allow custom rejection messages. The rejection messages also allows you to use variables so that you can reject a commit with a message saying exactly what the committer needs to do in order to fixup the commit. It also has a <em>dry run</em> mode where it only complains about the things that are not correct, but does not actually block the commit. It may be crucial to have such a feature if you introduce commit checks for 100+ committers.</p>\n<p><img src=\"/content/images/2016/09/sscc-email.png\" alt=\"Block committer emails not in Bitbucket Server\"></p>\n<p>The rejection messages are very clear.</p>\n<p><img src=\"/content/images/2016/09/sscc-reject.png\" alt=\"Email rejected\"></p>\n<p>It uses Jira:s <a href=\"https://confluence.atlassian.com/jirasoftwarecloud/advanced-searching-764478330.html#Advancedsearching-ConstructingJQLqueries\">JQL</a> to enable advanced and flexible validation of commit comments. It does not have to be an advanced query, simply adding <code>issue = ${REGEXP}</code> will validate that the Jiras exists.</p>\n<p><img src=\"/content/images/2016/09/sscc-issues.png\" alt=\"JQL\"></p>\n<p>I use it to block commits:</p>\n<ul>\n<li>Contains files that are too big, larger then 2000kb.</li>\n<li>Committer email or name does not match authenticated user in Bitbucket Server.</li>\n<li>Message does not contain an issue. Jira or custom incident pattern. I create a special issue in jira, a QA-jira, that can be use for small fixes. A small fix might be correcting toggling test case or formatting code.</li>\n</ul>\n<h2 id=\"pullrequestnotifierforbitbucketserver\">Pull Request Notifier for Bitbucket Server</h2>\n<p>This is one of my plugins. Its available for easy install from <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash/server/overview\">Atlassian Marketplace</a> and is developed <a href=\"https://github.com/tomasbjerre/pull-request-notifier-for-bitbucket\">on Github</a>.</p>\n<p>It invokes a URL when a pull request event happens in Bitbucket Server. The URL is completely customizable and supports variables so that you can notify any system with the information it needs on the format it needs it.</p>\n<p><img src=\"/content/images/2016/09/pull-request-notifier-url-config.png\" alt=\"Pull Request Notifier for Bitbucket Server URL config\"></p>\n<p>The main reason why I developed this plugin was to allow perfect verification of pull requests in Jenkins. But it is written in a way so that it is in no way limited to Jenkins. It basically just interacts with HTTP and can be used for many other purposes. I know people use it for posting updates regarding pull requests in Slack channels.</p>\n<p>It enables you to guarantee that pull requests cannot be merged unless target branch will still build after the merge. If one of several pull requests to <em>dev</em> is merged, the plugin can re-trigger all other pull requests to verify them with the new commit that is now <em>dev</em>.</p>\n<p>It adds support for <em>custom buttons</em> on pull request view. These buttons are mapped to a <em>notifications</em> (a URL being invoked). This means you can have a button labeled &quot;<em>Deploy to test env 1</em>&quot; and when it is pressed you trigger a job in Jenkins. The job is served with the feature branch of the pull request, builds the artifact and deploys it to that environment.</p>\n<p><img src=\"/content/images/2016/09/prnfb-buttons.png\" alt=\"Pull Request Notifier for Bitbucket Server Button Config\"></p>\n<p>And in the pull request view you will see that button to the right, when clicking the dots.</p>\n<p><img src=\"/content/images/2016/09/prnfb-button-pr-view.png\" alt=\"Pull Request Notifier for Bitbucket Server Button In PR view\"></p>\n<p>The button will only be made visible if there is a configured notification that will actually trigger on the button being pressed. So make sure you add that also.</p>\n<p><img src=\"/content/images/2016/09/prnfb-triggers.png\" alt=\"Pull Request Notifier for Bitucket Server Triggers\"></p>\n<h2 id=\"settingssynchronizerforbitbucketserver\">Settings Synchronizer for Bitbucket Server</h2>\n<p>This is one of my plugins. Its available for easy install from <a href=\"https://marketplace.atlassian.com/plugins/se.bjurr.ssfb.settings-synchronizer-for-bitbucket/server/overview\">Atlassian Marketplace</a> and is developed <a href=\"https://github.com/tomasbjerre/settings-synchronizer-for-bitbucket-plugin\">on Github</a>.</p>\n<p>It is a bit of a pain to configure plugins for a 100, or even just a couple, of repositories. This plugin lets you synchronize plugin settings among your repositories.</p>\n<p><img src=\"/content/images/2016/10/repoadmin.png\" alt=\"Settings Synchronizer for Bitbucket Server\"></p>\n<h1 id=\"jenkins\">Jenkins</h1>\n<p>The build server that I use. Some say its just a glorified cron job. And yes thats pretty much it. The good thing about it is all the plugins. I dont think I would ever use it if it was not for the plugins.</p>\n<p>When I verify the pull requests I do it with a simple shell script build step.</p>\n<ul>\n<li>From <a href=\"https://github.com/tomasbjerre/pull-request-notifier-for-bitbucket\">Pull Request Notifier for Bitbucket Server</a> I pass the <code>${EVERYTHING_URL}</code>.</li>\n<li>I check the job as parameterized and add the variables, used below, ass parameters.</li>\n<li>I check delay and set it to 0 seconds.</li>\n<li>I check allow parallel jobs.</li>\n</ul>\n<p>The script is something like this.</p>\n<pre><code>git clone $PULL_REQUEST_TO_HTTP_CLONE_URL\ncd *  \ngit reset --hard $PULL_REQUEST_TO_HASH\ngit status  \ngit remote add from $PULL_REQUEST_FROM_HTTP_CLONE_URL\ngit fetch --all\ngit merge $PULL_REQUEST_FROM_HASH\ngit --no-pager log --max-count=10 --graph --abbrev-commit\n\n#compile command here ...\n</code></pre>\n<h2 id=\"violationcommentstobitbucketserverplugin\">Violation Comments to Bitbucket Server Plugin</h2>\n<p>This is one of my Jenkins plugins. It is available in <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Bitbucket+Server+Plugin\">Jenkins update sites</a> and is developed <a href=\"https://github.com/jenkinsci/violation-comments-to-stash-plugin\">on Github</a>. There is also a <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Github+Plugin\">Jenkins plugin for Github</a> if that is what you are using.</p>\n<p>My opinion is that projects often put to much trust in that developers themselves will adhere to rules specified by static code analysis tools. They often trust that developers will have their IDE setup correctly to report problems. They also trust that developers will browse to <a href=\"http://www.sonarqube.org/\">SonarQube</a>, every now and then, and find problems to fix. It is my strongest opinion that this never works. Some developers are really enthusiastic about it in the beginning but soon forgets to check for problems. Some never even cares in the first place. Commenting the pull requests with the problems found, makes them hard to ignore.</p>\n<p>When a pull request is verified, part of the verification is static code analysis. This plugin finds report files generated from that analysis and uses them to comment the pull request in Bitbucket Server.</p>\n<p><img src=\"/content/images/2016/09/screenshot-stash.png\" alt=\"Violation Comments to Bitbucket Server\"></p>\n<p>The post build action may look something like this.</p>\n<p><img src=\"/content/images/2016/09/violation-comments-to-bitbucket.png\" alt=\"Violation Comments to Bitbucket Server Post Build Action\"></p>\n<h2 id=\"gitchangelogplugin\">Git Changelog Plugin</h2>\n<p>This is one of my Jenkins plugins. It is available in <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Git+Changelog+Plugin\">Jenkins update sites</a> and is developed <a href=\"https://github.com/jenkinsci/git-changelog-plugin\">on Github</a>.</p>\n<p>There may be several commits in the feature branch that is the source of the pull request. It may be an integration branch where developers has been working for weeks or even months. Then its very nice get a more organized view of what the feature branch actually contains.</p>\n<p><img src=\"/content/images/2016/09/git-changelog-postaction1.png\" alt=\"Git Changelog Jenkins Plugin Configuration\"></p>\n<p>This plugin is configured with a template and then creates a changelog where the template is rendered with the content of the feature branch.</p>\n<p><img src=\"/content/images/2016/09/git-changelog-postaction2.png\" alt=\"Git Changelog Jenkins Plugin Template Configuration\"></p>\n<p>It may look something like this if you chose to publish it on Jenkins job summary page. I used a Github repo here in the example because I dont have access to a Jira installation, but it works just the same with Jira! =)</p>\n<p><img src=\"/content/images/2016/09/gitchangelog-prnfb-github.png\" alt=\"Git Changelog Jenkins Summary\"></p>\n<p>You can also use this plugin to create release notes, just select <em>dev</em> as source branch and <em>master</em> as target branch. If you have a Jenkins job for <em>dev</em> then that is where to put that. It includes <strong>integration with MediaWiki</strong> so that releasenotes can be posted there. You can also chose to <strong>create a file</strong> where you are totally free to create a formatted HTML with CSS, or just plain text, the way you like it.</p>\n<h2 id=\"stashnotifierplugin\">Stash Notifier Plugin</h2>\n<p>It is available in <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin\">Jenkins update sites</a>. It adds a post build step in Jenkns.</p>\n<p><img src=\"/content/images/2016/09/stash-notifier-plugin.png\" alt=\"Post Build Step in Jenkins\"></p>\n<p>It also reveals build status on Pull request view in Bitbucket Server.</p>\n<p><img src=\"/content/images/2016/09/stash-notifier-in-bitbucket.png\" alt=\"Build status in Bitbucket Server\"></p>\n<h1 id=\"conclusions\">Conclusions</h1>\n<p>Hope you found something useful here. Pull requests and suggested features are always welcome. Please dont email me directly but instead try to file issues on GitHub!</p>\n','26','I have started several projects where I develop plugins for Bitbucket Server and\nJenkins. They are independent of each other but I thought it would be nice to\nhave a blog post about how they can work together and on how I prefer to do\ncontinuous integration. This is it! =)\n\nWhat is the configuration that I apply?\n\n * Gitflow\n   [https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow] \n    workflow.\n * Every commit, message and code, has to be reviewed before it can be merged.\n * Every commit has to have an issue in its message.\n * The size of the files that are put under version control cannot exceed\n   2000kb.\n * The committer must use valid name and email in all commits.\n * Changes can only be made with feature branches  and merged with pull requests\n   .\n * Pull requests  cannot be merged unless it its guaranteed that the target\n   branch will still build after merge.\n * Pull requests  cannot be merged unless, at least, one other developer has\n   reviewed it.\n * Static code analysis  will be made on every pull request.\n\nWhy do I apply this configuration?\n\n * Git flow enables you to * Use plugins for managing releases, like JGit-Flow\n      [https://bitbucket.org/atlassian/jgit-flow/wiki/Home].\n    * Reduce time spent on documentation. You just need\n      to refer to some youtube-videos\n      [https://www.youtube.com/results?search_query=git+flow]  or websites\n      [http://lmgtfy.com/?q=git+flow].\n    * Discuss any scenario on any public forum like Stackoverflow\n      [http://stackoverflow.com/search?q=git+flow].\n    * Let a configuration manager have total control of master  and release\n      -branches. While developers have total control of dev  and feature\n      -branches.\n    * Some people skip dev  and use master  as developer branch. I keep dev \n      because when developing tools around GIT, its convenient to have a\n      reference to latest release, master, and latest snapshot, dev. And since\n      branches in git are basically just a file with a hash I think its a small\n      price to pay for keeping it nice and tidy.\n   \n   \n * You will know exactly what is included in your releases. The commits are\n   reviewed, so you can trust that the message is true. Which means you can\n   create your release notes by looking at the new commits in dev  that are not\n   yet merged to master  (production). A special QA-jira is created to use for\n   small fixes, like correcting toggling test case or formatting code.\n * You will make the code more maintainable and share knowledge among developers\n   . When tracking down a bug, the commit messages you find will be clear and\n   understandable. Also the code will be cleaner from the reviews. Duplicated\n   utility classes, diverging patterns... and so on will be denied.\n * You will never have a failing build on a shared branch. Merge is only\n   allowed, blocked by the Bitbucket Server, if it is guaranteed that target\n   will still build after merge. Actually, if you have toggling test cases they\n   can fail. But that should be found in code reviews.\n * Testers can pick a feature and try it out in a test environment before its\n   merged to dev. By letting Jenkins  perform the merge (without pushing), build\n   the artifacts and deploy to an environment.\n * Your static code analysis will be honored. You will see dramatically reduced\n   amount of warnings from static code analysis. The developers will not be able\n   to ignore such warnings (often the case with Sonar) as they will\n   automatically be converted into comments on the code in the pull request.\n * Your repository will not quickly increase in size. Remember that when you\n   clone a Git repo you will download every version of everything. Its mostly a\n   one time thing for a developer but something that a build server does several\n   times a day. Once your repo has grown big its a bit of a pain getting rid of\n   those large files.\n\nAnd finally, how do I apply the configuration? With Jenkins, Bitbucket Server\n(some of this is also possible to do if you are using GitHub) and plugins! I\ncreated a small Docker Compose [https://github.com/tomasbjerre/jenkinsbitbucket] \n to help fiddle with this.\n\nBitbucket Server\nThis is the Git repo that I use. It adds some things that Git does not have:\n\n * Authentication.\n * Branch, repo and project permissions.\n * Pull request support.\n * Code review support.\n * Alot of plugins.\n\nThe plugins I will use adds support for:\n\n * Commit checks with Simple Bitbucket Server Commit Checker\n   [https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc/server/overview]\n   .\n * Notifies Jenkins to perform verification of pull requests with Pull Request\n   Notifier for Bitbucket Server\n   [https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash/server/overview]\n   .\n * Synchronizing settings among repositories with Settings Synchronizer for\n   Bitbucket Server\n   [https://marketplace.atlassian.com/plugins/se.bjurr.ssfb.settings-synchronizer-for-bitbucket/server/overview]\n\nYou can think about Git as an open database. Anyone can change anything. Anyone\ncan add commits to any branch, with any author/committer name/email. It is very\ndangerous to collaborate around a repo that is not protected with a service like\nBitbucket Server.\n\nSimple Bitbucket Server Commit Checker\nThis is one of my plugins. Its available for easy install from Atlassian\nMarketplace\n[https://marketplace.atlassian.com/plugins/se.bjurr.sscc.sscc/server/overview] \nand is developed on Github\n[https://github.com/tomasbjerre/simple-bitbucket-commit-checker].\n\nThe main reason why I developed this plugin was to allow custom rejection\nmessages. The rejection messages also allows you to use variables so that you\ncan reject a commit with a message saying exactly what the committer needs to do\nin order to fixup the commit. It also has a dry run  mode where it only\ncomplains about the things that are not correct, but does not actually block the\ncommit. It may be crucial to have such a feature if you introduce commit checks\nfor 100+ committers.\n\n\n\nThe rejection messages are very clear.\n\n\n\nIt uses Jira:s JQL  to enable advanced and flexible validation of commit\ncomments. It does not have to be an advanced query, simply adding issue =\n${REGEXP}  will validate that the Jiras exists.\n\n\n\nI use it to block commits:\n\n * Contains files that are too big, larger then 2000kb.\n * Committer email or name does not match authenticated user in Bitbucket\n   Server.\n * Message does not contain an issue. Jira or custom incident pattern. I create\n   a special issue in jira, a QA-jira, that can be use for small fixes. A small\n   fix might be correcting toggling test case or formatting code.\n\nPull Request Notifier for Bitbucket Server\nThis is one of my plugins. Its available for easy install from Atlassian\nMarketplace\n[https://marketplace.atlassian.com/plugins/se.bjurr.prnfs.pull-request-notifier-for-stash/server/overview] \n and is developed on Github\n[https://github.com/tomasbjerre/pull-request-notifier-for-bitbucket].\n\nIt invokes a URL when a pull request event happens in Bitbucket Server. The URL\nis completely customizable and supports variables so that you can notify any\nsystem with the information it needs on the format it needs it.\n\n\n\nThe main reason why I developed this plugin was to allow perfect verification of\npull requests in Jenkins. But it is written in a way so that it is in no way\nlimited to Jenkins. It basically just interacts with HTTP and can be used for\nmany other purposes. I know people use it for posting updates regarding pull\nrequests in Slack channels.\n\nIt enables you to guarantee that pull requests cannot be merged unless target\nbranch will still build after the merge. If one of several pull requests to dev \nis merged, the plugin can re-trigger all other pull requests to verify them with\nthe new commit that is now dev.\n\nIt adds support for custom buttons  on pull request view. These buttons are\nmapped to a notifications  (a URL being invoked). This means you can have a\nbutton labeled \"Deploy to test env 1\" and when it is pressed you trigger a job\nin Jenkins. The job is served with the feature branch of the pull request,\nbuilds the artifact and deploys it to that environment.\n\n\n\nAnd in the pull request view you will see that button to the right, when\nclicking the dots.\n\n\n\nThe button will only be made visible if there is a configured notification that\nwill actually trigger on the button being pressed. So make sure you add that\nalso.\n\n\n\nSettings Synchronizer for Bitbucket Server\nThis is one of my plugins. Its available for easy install from Atlassian\nMarketplace\n[https://marketplace.atlassian.com/plugins/se.bjurr.ssfb.settings-synchronizer-for-bitbucket/server/overview] \n and is developed on Github\n[https://github.com/tomasbjerre/settings-synchronizer-for-bitbucket-plugin].\n\nIt is a bit of a pain to configure plugins for a 100, or even just a couple, of\nrepositories. This plugin lets you synchronize plugin settings among your\nrepositories.\n\n\n\nJenkins\nThe build server that I use. Some say its just a glorified cron job. And yes\nthats pretty much it. The good thing about it is all the plugins. I dont think I\nwould ever use it if it was not for the plugins.\n\nWhen I verify the pull requests I do it with a simple shell script build step.\n\n * From Pull Request Notifier for Bitbucket Server\n   [https://github.com/tomasbjerre/pull-request-notifier-for-bitbucket]  I pass\n   the ${EVERYTHING_URL}.\n * I check the job as parameterized and add the variables, used below, ass\n   parameters.\n * I check delay and set it to 0 seconds.\n * I check allow parallel jobs.\n\nThe script is something like this.\n\ngit clone $PULL_REQUEST_TO_HTTP_CLONE_URL\ncd *  \ngit reset --hard $PULL_REQUEST_TO_HASH\ngit status  \ngit remote add from $PULL_REQUEST_FROM_HTTP_CLONE_URL\ngit fetch --all\ngit merge $PULL_REQUEST_FROM_HASH\ngit --no-pager log --max-count=10 --graph --abbrev-commit\n\n#compile command here ...\n\n\nViolation Comments to Bitbucket Server Plugin\nThis is one of my Jenkins plugins. It is available in Jenkins update sites\n[https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Bitbucket+Server+Plugin] \n and is developed on Github\n[https://github.com/jenkinsci/violation-comments-to-stash-plugin]. There is also\na Jenkins plugin for Github\n[https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+Github+Plugin] \n if that is what you are using.\n\nMy opinion is that projects often put to much trust in that developers\nthemselves will adhere to rules specified by static code analysis tools. They\noften trust that developers will have their IDE setup correctly to report\nproblems. They also trust that developers will browse to SonarQube\n[http://www.sonarqube.org/], every now and then, and find problems to fix. It is\nmy strongest opinion that this never works. Some developers are really\nenthusiastic about it in the beginning but soon forgets to check for problems.\nSome never even cares in the first place. Commenting the pull requests with the\nproblems found, makes them hard to ignore.\n\nWhen a pull request is verified, part of the verification is static code\nanalysis. This plugin finds report files generated from that analysis and uses\nthem to comment the pull request in Bitbucket Server.\n\n\n\nThe post build action may look something like this.\n\n\n\nGit Changelog Plugin\nThis is one of my Jenkins plugins. It is available in Jenkins update sites\n[https://wiki.jenkins-ci.org/display/JENKINS/Git+Changelog+Plugin]  and is\ndeveloped on Github [https://github.com/jenkinsci/git-changelog-plugin].\n\nThere may be several commits in the feature branch that is the source of the\npull request. It may be an integration branch where developers has been working\nfor weeks or even months. Then its very nice get a more organized view of what\nthe feature branch actually contains.\n\n\n\nThis plugin is configured with a template and then creates a changelog where the\ntemplate is rendered with the content of the feature branch.\n\n\n\nIt may look something like this if you chose to publish it on Jenkins job\nsummary page. I used a Github repo here in the example because I dont have\naccess to a Jira installation, but it works just the same with Jira! =)\n\n\n\nYou can also use this plugin to create release notes, just select dev  as source\nbranch and master  as target branch. If you have a Jenkins job for dev  then\nthat is where to put that. It includes integration with MediaWiki  so that\nreleasenotes can be posted there. You can also chose to create a file  where you\nare totally free to create a formatted HTML with CSS, or just plain text, the\nway you like it.\n\nStash Notifier Plugin\nIt is available in Jenkins update sites\n[https://wiki.jenkins-ci.org/display/JENKINS/StashNotifier+Plugin]. It adds a\npost build step in Jenkns.\n\n\n\nIt also reveals build status on Pull request view in Bitbucket Server.\n\n\n\nConclusions\nHope you found something useful here. Pull requests and suggested features are\nalways welcome. Please dont email me directly but instead try to file issues on\nGitHub!','/content/images/2018/02/0722.sdt-atlassian.png',0,0,'published',NULL,'public',NULL,NULL,'1','2016-09-07 17:17:25','1','2018-08-31 05:51:39','1','2016-09-10 08:07:34','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a5','3099c298-3f00-497a-86a9-67ad0789d172','GitFlow and when you should use it','gitflow-and-when-you-should-use-it','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"There are a lot of people explaining [GitFlow](http://nvie.com/posts/a-successful-git-branching-model/) in different blog posts and videos. I could not find one that, using the correct arguments, explained what it is good for. These are my personal opinions and you don\'t have to agree =)\\n\\n*GitFlow* is really the simplest thing. Let\'s divide the people working with a project into 2 different roles, **developers** and **configuration manager(s)**, and look at what they do.\\n\\n# GitFlow as a developer\\nAs a **developer** you have 2 kinds of branches.\\n\\n * **develop** Integration branch. Refers to the *next release*.\\n * **feature/X** Feature branch where feature *X* is developed.\\n\\nYou may have **feature** branches that branch out of other **feature** branches. In order to, easily, collaborate with other developers on a larger feature. You may call that large feature branch, **integration** branch.\\n\\n* You should always merge code to **develop** as fast as you can. To spread commits quick and avoid merge conflicts.\\n* You should regularly *rebase*, or *merge*, **develop** into you **feature** branch. To base you code on the latest features and refactorings and avoid merge conflicts.\\n\\nTo work on the next release. You branch out of **develop**, work with the **feature**, merge back to **develop**.\\n\\n```\\n*    (develop)  Merge branch \'feature/work-with-correcting-a\' into develop \\n|\\\\  \\n| *  (feature/work-with-correcting-a) Correcting a\\n|/  \\n*     Merge branch \'feature/work-with-a\' into develop \\n|\\\\  \\n| *  (feature/work-with-a)  a \\n|/  \\n* \\n```\\n\\nTo work on the *next*-*next* release, or even further into the future, simply don\'t merge it back to **develop** before *next*-*next* is *next*. And keep in mind, it may be a good idea to merge it into **develop** earlier, to avoid merge conflicts, if you can do feature toggling.\\n\\nAs a **developer**, this is all there is to it! This is how **developers** most often prefer to work. Just look at any open source repository. Most often you fork the main repository and the **master** in your fork is your **feature** branch. But still, **this is as simple as it can get and this is how developers do it!**\\n\\n# GitFlow as a configuration manager\\nAs a **configuration manager** you have 2 kinds of branches.\\n\\n * **master** This is the what\'s in **production**.\\n * **release-X** This is what\'s included in **release** *X*.\\n\\nAs soon as it\'s *code freeze* it\'s time to create the first *release candidate*. That is done by branching out of **develop** into **release-X**. The version of **develop** is now set to 1.1, in Java that would mean changing the version to *1.1-SNAPSHOT* while in the release branch its still *1.0-SNAPSHOT*. The semantics of the versions is a side track, but I can recommend [this blog post](http://kylelieber.com/2012/06/maven-versioning-strategy/).\\n```\\n| *  (release-1, tag: 1.0-RC-1) Releasing RC 1\\n| *  Setting version to 1.0-RC1\\n* | (develop) Setting version to 1.1-SNAPSHOT\\n|/\\n```\\n\\nThe whole point of the **release-X** branch is to provide a *code freeze* for the release. The **developers** can continue their work exactly as described above. Most Git services allows you to define permissions per branch and only configuration managers should have permission to **release-X**.\\n\\nThe release candidate, *RC1*, is deployed and tested and if it\'s all good then it\'s released. But there is probably something that needs to be fixed. It can be done in different ways.\\n\\n* With *cherry-picking* from **develop**. \\nThis is the prettiest but it may also not be possible. Merge conflicts may hinder this. There is also a risk that the **develop**-branch is too far ahead making it hard to know if the fix will work once it\'s in the **release**-branch. But I feel that it gives the **configuration manager** the most control of the release process.\\n\\n* With merging a **feature**-branch that branches out of the **release**-branch.\\nThis often my preferred choice in large teams. It gives the **configuration manager** control of what\'s included in the release. It avoids risk of merge conflicts. The tests done on that **feature**-branch can be trusted. It can be done with pull requests.\\n\\n* With direct commits to the **release**-branch.\\nThis may fit a technical team where the **configuration manager** is also a **developer**. It provides a release process for the current release and enables the **developer** to work on the next release in **develop**. It lacks support for code review but you may not need that in a small project.\\n\\nHere is what it may look like if *cherry-picking* is used.\\n\\n```\\n| *  Setting version to 1.0-SNAPSHOT\\n| *  (release-1, tag: 1.0-RC-2) Releasing RC 2\\n| *  Setting version to 1.0-RC2\\n| *  Correcting a\\n| *  Setting version to 1.0-SNAPSHOT\\n| *  (tag: 1.0-RC-1) Releasing RC 1\\n| *  Setting version to 1.0-RC1\\n* | (develop) Setting version to 1.1-SNAPSHOT\\n|/\\n```\\n\\nNow if *RC2* is good and should be released then that exact artifact is deployed in production.\\n\\n* Released commit, **release-tag**, in **release-1** is merged into **master**.\\n    * To keep track of what is in production.\\n* **master** is merged into **develop**, this is important:\\n    * To make sure any changes made in production are also in the next release.\\n    * To allow a *fast-forward* of **master** next time you merge **release-tag** to **master**.\\n\\n#Conclusions\\n\\nI\'ve shown the two roles in *GitFlow* and I hope I made it obvious that its actually a very simple workflow! It will not fit all projects.\\n\\nYou will probably **not gain from using *GitFLow*** in a library. That is just used by other software. Where a release is just a process of packaging an artifact an making it available for other software to use. You can probably just release a new version if something needs to be fixed later, and the users of the software can just keep using an older version until things are worked out.\\n\\nYou will probably **gain from using *GitFlow*** if you have a *code freeze*. It enables a *code freeze* without preventing developers from working on the next release.\\n\\nIf you just fire releases from *master*, without any branching, you can easily automate that process. If you need to make a release-branch and eventually merge it back into *master*, and *develop*, you will need a more complex flow. Which is a price you may not want to pay unless you are gaining something else from the flow!\"}]],\"sections\":[[10,0]]}','<p>There are a lot of people explaining <a href=\"http://nvie.com/posts/a-successful-git-branching-model/\">GitFlow</a> in different blog posts and videos. I could not find one that, using the correct arguments, explained what it is good for. These are my personal opinions and you don\'t have to agree =)</p>\n<p><em>GitFlow</em> is really the simplest thing. Let\'s divide the people working with a project into 2 different roles, <strong>developers</strong> and <strong>configuration manager(s)</strong>, and look at what they do.</p>\n<h1 id=\"gitflowasadeveloper\">GitFlow as a developer</h1>\n<p>As a <strong>developer</strong> you have 2 kinds of branches.</p>\n<ul>\n<li><strong>develop</strong> Integration branch. Refers to the <em>next release</em>.</li>\n<li><strong>feature/X</strong> Feature branch where feature <em>X</em> is developed.</li>\n</ul>\n<p>You may have <strong>feature</strong> branches that branch out of other <strong>feature</strong> branches. In order to, easily, collaborate with other developers on a larger feature. You may call that large feature branch, <strong>integration</strong> branch.</p>\n<ul>\n<li>You should always merge code to <strong>develop</strong> as fast as you can. To spread commits quick and avoid merge conflicts.</li>\n<li>You should regularly <em>rebase</em>, or <em>merge</em>, <strong>develop</strong> into you <strong>feature</strong> branch. To base you code on the latest features and refactorings and avoid merge conflicts.</li>\n</ul>\n<p>To work on the next release. You branch out of <strong>develop</strong>, work with the <strong>feature</strong>, merge back to <strong>develop</strong>.</p>\n<pre><code>*    (develop)  Merge branch \'feature/work-with-correcting-a\' into develop \n|\\  \n| *  (feature/work-with-correcting-a) Correcting a\n|/  \n*     Merge branch \'feature/work-with-a\' into develop \n|\\  \n| *  (feature/work-with-a)  a \n|/  \n* \n</code></pre>\n<p>To work on the <em>next</em>-<em>next</em> release, or even further into the future, simply don\'t merge it back to <strong>develop</strong> before <em>next</em>-<em>next</em> is <em>next</em>. And keep in mind, it may be a good idea to merge it into <strong>develop</strong> earlier, to avoid merge conflicts, if you can do feature toggling.</p>\n<p>As a <strong>developer</strong>, this is all there is to it! This is how <strong>developers</strong> most often prefer to work. Just look at any open source repository. Most often you fork the main repository and the <strong>master</strong> in your fork is your <strong>feature</strong> branch. But still, <strong>this is as simple as it can get and this is how developers do it!</strong></p>\n<h1 id=\"gitflowasaconfigurationmanager\">GitFlow as a configuration manager</h1>\n<p>As a <strong>configuration manager</strong> you have 2 kinds of branches.</p>\n<ul>\n<li><strong>master</strong> This is the what\'s in <strong>production</strong>.</li>\n<li><strong>release-X</strong> This is what\'s included in <strong>release</strong> <em>X</em>.</li>\n</ul>\n<p>As soon as it\'s <em>code freeze</em> it\'s time to create the first <em>release candidate</em>. That is done by branching out of <strong>develop</strong> into <strong>release-X</strong>. The version of <strong>develop</strong> is now set to 1.1, in Java that would mean changing the version to <em>1.1-SNAPSHOT</em> while in the release branch its still <em>1.0-SNAPSHOT</em>. The semantics of the versions is a side track, but I can recommend <a href=\"http://kylelieber.com/2012/06/maven-versioning-strategy/\">this blog post</a>.</p>\n<pre><code>| *  (release-1, tag: 1.0-RC-1) Releasing RC 1\n| *  Setting version to 1.0-RC1\n* | (develop) Setting version to 1.1-SNAPSHOT\n|/\n</code></pre>\n<p>The whole point of the <strong>release-X</strong> branch is to provide a <em>code freeze</em> for the release. The <strong>developers</strong> can continue their work exactly as described above. Most Git services allows you to define permissions per branch and only configuration managers should have permission to <strong>release-X</strong>.</p>\n<p>The release candidate, <em>RC1</em>, is deployed and tested and if it\'s all good then it\'s released. But there is probably something that needs to be fixed. It can be done in different ways.</p>\n<ul>\n<li>\n<p>With <em>cherry-picking</em> from <strong>develop</strong>.<br>\nThis is the prettiest but it may also not be possible. Merge conflicts may hinder this. There is also a risk that the <strong>develop</strong>-branch is too far ahead making it hard to know if the fix will work once it\'s in the <strong>release</strong>-branch. But I feel that it gives the <strong>configuration manager</strong> the most control of the release process.</p>\n</li>\n<li>\n<p>With merging a <strong>feature</strong>-branch that branches out of the <strong>release</strong>-branch.<br>\nThis often my preferred choice in large teams. It gives the <strong>configuration manager</strong> control of what\'s included in the release. It avoids risk of merge conflicts. The tests done on that <strong>feature</strong>-branch can be trusted. It can be done with pull requests.</p>\n</li>\n<li>\n<p>With direct commits to the <strong>release</strong>-branch.<br>\nThis may fit a technical team where the <strong>configuration manager</strong> is also a <strong>developer</strong>. It provides a release process for the current release and enables the <strong>developer</strong> to work on the next release in <strong>develop</strong>. It lacks support for code review but you may not need that in a small project.</p>\n</li>\n</ul>\n<p>Here is what it may look like if <em>cherry-picking</em> is used.</p>\n<pre><code>| *  Setting version to 1.0-SNAPSHOT\n| *  (release-1, tag: 1.0-RC-2) Releasing RC 2\n| *  Setting version to 1.0-RC2\n| *  Correcting a\n| *  Setting version to 1.0-SNAPSHOT\n| *  (tag: 1.0-RC-1) Releasing RC 1\n| *  Setting version to 1.0-RC1\n* | (develop) Setting version to 1.1-SNAPSHOT\n|/\n</code></pre>\n<p>Now if <em>RC2</em> is good and should be released then that exact artifact is deployed in production.</p>\n<ul>\n<li>Released commit, <strong>release-tag</strong>, in <strong>release-1</strong> is merged into <strong>master</strong>.\n<ul>\n<li>To keep track of what is in production.</li>\n</ul>\n</li>\n<li><strong>master</strong> is merged into <strong>develop</strong>, this is important:\n<ul>\n<li>To make sure any changes made in production are also in the next release.</li>\n<li>To allow a <em>fast-forward</em> of <strong>master</strong> next time you merge <strong>release-tag</strong> to <strong>master</strong>.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"conclusions\">Conclusions</h1>\n<p>I\'ve shown the two roles in <em>GitFlow</em> and I hope I made it obvious that its actually a very simple workflow! It will not fit all projects.</p>\n<p>You will probably <strong>not gain from using <em>GitFLow</em></strong> in a library. That is just used by other software. Where a release is just a process of packaging an artifact an making it available for other software to use. You can probably just release a new version if something needs to be fixed later, and the users of the software can just keep using an older version until things are worked out.</p>\n<p>You will probably <strong>gain from using <em>GitFlow</em></strong> if you have a <em>code freeze</em>. It enables a <em>code freeze</em> without preventing developers from working on the next release.</p>\n<p>If you just fire releases from <em>master</em>, without any branching, you can easily automate that process. If you need to make a release-branch and eventually merge it back into <em>master</em>, and <em>develop</em>, you will need a more complex flow. Which is a price you may not want to pay unless you are gaining something else from the flow!</p>\n','27','There are a lot of people explaining GitFlow\n[http://nvie.com/posts/a-successful-git-branching-model/]  in different blog\nposts and videos. I could not find one that, using the correct arguments,\nexplained what it is good for. These are my personal opinions and you don\'t have\nto agree =)\n\nGitFlow  is really the simplest thing. Let\'s divide the people working with a\nproject into 2 different roles, developers  and configuration manager(s), and\nlook at what they do.\n\nGitFlow as a developer\nAs a developer  you have 2 kinds of branches.\n\n * develop  Integration branch. Refers to the next release.\n * feature/X  Feature branch where feature X  is developed.\n\nYou may have feature  branches that branch out of other feature  branches. In\norder to, easily, collaborate with other developers on a larger feature. You may\ncall that large feature branch, integration  branch.\n\n * You should always merge code to develop  as fast as you can. To spread\n   commits quick and avoid merge conflicts.\n * You should regularly rebase, or merge, develop  into you feature  branch. To\n   base you code on the latest features and refactorings and avoid merge\n   conflicts.\n\nTo work on the next release. You branch out of develop, work with the feature,\nmerge back to develop.\n\n*    (develop)  Merge branch \'feature/work-with-correcting-a\' into develop \n|\\  \n| *  (feature/work-with-correcting-a) Correcting a\n|/  \n*     Merge branch \'feature/work-with-a\' into develop \n|\\  \n| *  (feature/work-with-a)  a \n|/  \n* \n\n\nTo work on the next-next  release, or even further into the future, simply don\'t\nmerge it back to develop  before next-next  is next. And keep in mind, it may be\na good idea to merge it into develop  earlier, to avoid merge conflicts, if you\ncan do feature toggling.\n\nAs a developer, this is all there is to it! This is how developers  most often\nprefer to work. Just look at any open source repository. Most often you fork the\nmain repository and the master  in your fork is your feature  branch. But still,\n this is as simple as it can get and this is how developers do it!\n\nGitFlow as a configuration manager\nAs a configuration manager  you have 2 kinds of branches.\n\n * master  This is the what\'s in production.\n * release-X  This is what\'s included in release  X.\n\nAs soon as it\'s code freeze  it\'s time to create the first release candidate.\nThat is done by branching out of develop  into release-X. The version of develop \n is now set to 1.1, in Java that would mean changing the version to 1.1-SNAPSHOT \n while in the release branch its still 1.0-SNAPSHOT. The semantics of the\nversions is a side track, but I can recommend this blog post\n[http://kylelieber.com/2012/06/maven-versioning-strategy/].\n\n| *  (release-1, tag: 1.0-RC-1) Releasing RC 1\n| *  Setting version to 1.0-RC1\n* | (develop) Setting version to 1.1-SNAPSHOT\n|/\n\n\nThe whole point of the release-X  branch is to provide a code freeze  for the\nrelease. The developers  can continue their work exactly as described above.\nMost Git services allows you to define permissions per branch and only\nconfiguration managers should have permission to release-X.\n\nThe release candidate, RC1, is deployed and tested and if it\'s all good then\nit\'s released. But there is probably something that needs to be fixed. It can be\ndone in different ways.\n\n * With cherry-picking  from develop.\n   This is the prettiest but it may also not be possible. Merge conflicts may\n   hinder this. There is also a risk that the develop-branch is too far ahead\n   making it hard to know if the fix will work once it\'s in the release-branch.\n   But I feel that it gives the configuration manager  the most control of the\n   release process.\n   \n   \n * With merging a feature-branch that branches out of the release-branch.\n   This often my preferred choice in large teams. It gives the configuration\n   manager  control of what\'s included in the release. It avoids risk of merge\n   conflicts. The tests done on that feature-branch can be trusted. It can be\n   done with pull requests.\n   \n   \n * With direct commits to the release-branch.\n   This may fit a technical team where the configuration manager  is also a \n   developer. It provides a release process for the current release and enables\n   the developer  to work on the next release in develop. It lacks support for\n   code review but you may not need that in a small project.\n   \n   \n\nHere is what it may look like if cherry-picking  is used.\n\n| *  Setting version to 1.0-SNAPSHOT\n| *  (release-1, tag: 1.0-RC-2) Releasing RC 2\n| *  Setting version to 1.0-RC2\n| *  Correcting a\n| *  Setting version to 1.0-SNAPSHOT\n| *  (tag: 1.0-RC-1) Releasing RC 1\n| *  Setting version to 1.0-RC1\n* | (develop) Setting version to 1.1-SNAPSHOT\n|/\n\n\nNow if RC2  is good and should be released then that exact artifact is deployed\nin production.\n\n * Released commit, release-tag, in release-1  is merged into master. * To keep\n      track of what is in production.\n   \n   \n * master  is merged into develop, this is important: * To make sure any changes\n      made in production are also in the next release.\n    * To allow a fast-forward  of master  next time you merge release-tag  to \n      master.\n   \n   \n\nConclusions\nI\'ve shown the two roles in GitFlow  and I hope I made it obvious that its\nactually a very simple workflow! It will not fit all projects.\n\nYou will probably not gain from using GitFLow  in a library. That is just used\nby other software. Where a release is just a process of packaging an artifact an\nmaking it available for other software to use. You can probably just release a\nnew version if something needs to be fixed later, and the users of the software\ncan just keep using an older version until things are worked out.\n\nYou will probably gain from using GitFlow  if you have a code freeze. It enables\na code freeze  without preventing developers from working on the next release.\n\nIf you just fire releases from master, without any branching, you can easily\nautomate that process. If you need to make a release-branch and eventually merge\nit back into master, and develop, you will need a more complex flow. Which is a\nprice you may not want to pay unless you are gaining something else from the\nflow!','/content/images/2018/02/poshgitflowversion.png',0,0,'published',NULL,'public',NULL,NULL,'1','2017-02-25 03:28:42','1','2018-07-16 16:38:50','1','2017-02-25 04:56:31','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a6','0c0c6499-af7f-4067-b900-17f8d7864667','Continuous Integration with GitLab and Jenkins','continuous-integration-with-gitlab-and-jenkins','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"My client recently started using GitLab. I did not find the plugins needed to properly verify merge requests. Here is what I wanted to do:\\n\\n * Trigger on merge requests events.\\n * Merge, compile, test and run static code analysis on it.\\n * Report static code analysis results to GitLab.\\n\\nWhen fiddling with this I came up with a Job DSL solution and a pipeline solution. Here is how they work.\\n\\n# Job DSL approach\\nI developed a couple of Jenkins Plugins to do this.\\n\\n* [Generic Webhook Trigger Jenkins Plugin](https://github.com/jenkinsci/generic-webhook-trigger-plugin)\\n* [Violation Comments To GitLab Jenkins Plugin](https://github.com/jenkinsci/violation-comments-to-gitlab-plugin)\\n\\nI also use the [HTTP Request Plugin](https://wiki.jenkins-ci.org/display/JENKINS/HTTP+Request+Plugin) and [Conditional BuildStep Plugin](https://wiki.jenkins-ci.org/display/JENKINS/Conditional+BuildStep+Plugin).\\n\\nHere is what you need to do to use this workflow.\\n\\n\\nIn Jenkins:\\n\\n 1. Install the plugins.\\n 2. Use the job DSL to create the job. The DSL is [in the wiki](https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin) and [in the repo](https://github.com/jenkinsci/violation-comments-to-gitlab-plugin).\\n\\nIn GitLab:\\n\\n 1. Create a API token. You will find it at `/profile/personal_access_tokens`\\n 2. Add a webhook. You will find it at `/root/violations-test/settings/integrations`. Point it at `http://user:password@jenkins:8080/generic-webhook-trigger/invoke`.\\n 3. Make sure you are authenticated, in this request, with a user that can view any jobs that should be triggered.\\n\\n\\nThe [GitLab merge request](https://gitlab.com/tomas.bjerre85/violations-test/merge_requests/1) will then be commented like this:\\n![alt](/content/images/2017/03/mergerequest-onecomment-1.png)\\n\\n#Pipeline approach\\n\\nI just use curl, [GitLab plugin](https://github.com/jenkinsci/gitlab-plugin) and [Violation Comments to GitLab Plugin](https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin) to implement it.\\n\\n![Violation Comments to GitLab pipeline](/content/images/2017/04/violation-pipeline.PNG)\\nHere is what you need to do to use this workflow.\\n\\nIn Jenkins:\\n\\n 1. Install the plugins.\\n 2. Create a pipeline job. The pipeline script is [in the wiki](https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin) and [in the repo](https://github.com/jenkinsci/violation-comments-to-gitlab-plugin).\\n\\nIn GitLab:\\n\\n 1. Create a API token. You will find it at `/profile/personal_access_tokens`\\n 2. Add a webhook. You will find it at `/root/violations-test/settings/integrations`.\\n\"}]],\"sections\":[[10,0]]}','<p>My client recently started using GitLab. I did not find the plugins needed to properly verify merge requests. Here is what I wanted to do:</p>\n<ul>\n<li>Trigger on merge requests events.</li>\n<li>Merge, compile, test and run static code analysis on it.</li>\n<li>Report static code analysis results to GitLab.</li>\n</ul>\n<p>When fiddling with this I came up with a Job DSL solution and a pipeline solution. Here is how they work.</p>\n<h1 id=\"jobdslapproach\">Job DSL approach</h1>\n<p>I developed a couple of Jenkins Plugins to do this.</p>\n<ul>\n<li><a href=\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\">Generic Webhook Trigger Jenkins Plugin</a></li>\n<li><a href=\"https://github.com/jenkinsci/violation-comments-to-gitlab-plugin\">Violation Comments To GitLab Jenkins Plugin</a></li>\n</ul>\n<p>I also use the <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/HTTP+Request+Plugin\">HTTP Request Plugin</a> and <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Conditional+BuildStep+Plugin\">Conditional BuildStep Plugin</a>.</p>\n<p>Here is what you need to do to use this workflow.</p>\n<p>In Jenkins:</p>\n<ol>\n<li>Install the plugins.</li>\n<li>Use the job DSL to create the job. The DSL is <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin\">in the wiki</a> and <a href=\"https://github.com/jenkinsci/violation-comments-to-gitlab-plugin\">in the repo</a>.</li>\n</ol>\n<p>In GitLab:</p>\n<ol>\n<li>Create a API token. You will find it at <code>/profile/personal_access_tokens</code></li>\n<li>Add a webhook. You will find it at <code>/root/violations-test/settings/integrations</code>. Point it at <code>http://user:password@jenkins:8080/generic-webhook-trigger/invoke</code>.</li>\n<li>Make sure you are authenticated, in this request, with a user that can view any jobs that should be triggered.</li>\n</ol>\n<p>The <a href=\"https://gitlab.com/tomas.bjerre85/violations-test/merge_requests/1\">GitLab merge request</a> will then be commented like this:<br>\n<img src=\"/content/images/2017/03/mergerequest-onecomment-1.png\" alt=\"alt\"></p>\n<h1 id=\"pipelineapproach\">Pipeline approach</h1>\n<p>I just use curl, <a href=\"https://github.com/jenkinsci/gitlab-plugin\">GitLab plugin</a> and <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin\">Violation Comments to GitLab Plugin</a> to implement it.</p>\n<p><img src=\"/content/images/2017/04/violation-pipeline.PNG\" alt=\"Violation Comments to GitLab pipeline\"><br>\nHere is what you need to do to use this workflow.</p>\n<p>In Jenkins:</p>\n<ol>\n<li>Install the plugins.</li>\n<li>Create a pipeline job. The pipeline script is <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin\">in the wiki</a> and <a href=\"https://github.com/jenkinsci/violation-comments-to-gitlab-plugin\">in the repo</a>.</li>\n</ol>\n<p>In GitLab:</p>\n<ol>\n<li>Create a API token. You will find it at <code>/profile/personal_access_tokens</code></li>\n<li>Add a webhook. You will find it at <code>/root/violations-test/settings/integrations</code>.</li>\n</ol>\n','29','My client recently started using GitLab. I did not find the plugins needed to\nproperly verify merge requests. Here is what I wanted to do:\n\n * Trigger on merge requests events.\n * Merge, compile, test and run static code analysis on it.\n * Report static code analysis results to GitLab.\n\nWhen fiddling with this I came up with a Job DSL solution and a pipeline\nsolution. Here is how they work.\n\nJob DSL approach\nI developed a couple of Jenkins Plugins to do this.\n\n * Generic Webhook Trigger Jenkins Plugin\n   [https://github.com/jenkinsci/generic-webhook-trigger-plugin]\n * Violation Comments To GitLab Jenkins Plugin\n   [https://github.com/jenkinsci/violation-comments-to-gitlab-plugin]\n\nI also use the HTTP Request Plugin\n[https://wiki.jenkins-ci.org/display/JENKINS/HTTP+Request+Plugin]  and \nConditional BuildStep Plugin\n[https://wiki.jenkins-ci.org/display/JENKINS/Conditional+BuildStep+Plugin].\n\nHere is what you need to do to use this workflow.\n\nIn Jenkins:\n\n 1. Install the plugins.\n 2. Use the job DSL to create the job. The DSL is in the wiki\n    [https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin] \n     and in the repo\n    [https://github.com/jenkinsci/violation-comments-to-gitlab-plugin].\n\nIn GitLab:\n\n 1. Create a API token. You will find it at /profile/personal_access_tokens\n 2. Add a webhook. You will find it at \n    /root/violations-test/settings/integrations. Point it at \n    http://user:password@jenkins:8080/generic-webhook-trigger/invoke.\n 3. Make sure you are authenticated, in this request, with a user that can view\n    any jobs that should be triggered.\n\nThe GitLab merge request\n[https://gitlab.com/tomas.bjerre85/violations-test/merge_requests/1]  will then\nbe commented like this:\n\n\nPipeline approach\nI just use curl, GitLab plugin [https://github.com/jenkinsci/gitlab-plugin]  and\n Violation Comments to GitLab Plugin\n[https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin] \n to implement it.\n\n\nHere is what you need to do to use this workflow.\n\nIn Jenkins:\n\n 1. Install the plugins.\n 2. Create a pipeline job. The pipeline script is in the wiki\n    [https://wiki.jenkins-ci.org/display/JENKINS/Violation+Comments+to+GitLab+Plugin] \n     and in the repo\n    [https://github.com/jenkinsci/violation-comments-to-gitlab-plugin].\n\nIn GitLab:\n\n 1. Create a API token. You will find it at /profile/personal_access_tokens\n 2. Add a webhook. You will find it at \n    /root/violations-test/settings/integrations.','/content/images/2018/02/gitlab.jpg',0,0,'published',NULL,'public',NULL,NULL,'1','2017-03-12 21:51:44','1','2018-02-24 08:57:27','1','2017-03-12 22:01:49','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a7','221c2c8c-f050-4657-bbaa-da6bd8d6890c','Java Code Formatting With Google Java Format','java-code-formatting-with-google-java-format','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Most projects, that I work with nowadays, have a defined code standard that includes how the code should be formatted. That is great and improves code quality a lot... if followed!\\n\\nA single person, in the project, can lower the quality significantly by not formatting the code correctly. If such code is not blocked, from making it into shared branches, you will have the choice of keeping it or fixing it. This is a problem that I find incredibly annoying!\\n\\n* *Keeping it* means you have to live with faulty formatted code. If some developers use something like *save actions* in **Eclipse** then they will have a hard time keeping the lines unchanged when working on the file.\\n* *Re-formatting it* in a new commit makes it harder to maintain the project because things like `git blame` will show the re-format commit, not the original feature commit.\\n\\nThe most common solution to this, among the clients I\'ve been working with, is to use a code style defined in **Eclipse**. Along with *save actions*. The code style is imported to **Eclipse** from an XML-file and some clients also use [Workspace Mechanics](http://marketplace.eclipse.org/content/workspace-mechanic) to setup *save actions* properly.\\n\\nDefining the code style in **Eclipse** is a very bad idea:\\n\\n* All people are not productive in **Eclipse**, some might for example be using **InteliJ** or **NetBeans**. They will have to have **Eclipse** installed just to use it for formatting.\\n* The semantics of the settings in the code style may change between versions of **Eclipse** (I have seen it!). Then you may start getting unnecessary diffs in commits after an upgrade.\\n* There is no way, as far as I know, to verify **Eclipse**-formatting as a step in a continuous integration flow. While it is easy for a reviewer to see that brackets are incorrectly positioned, other things may not be as obvious.\\n\\n# Google Java Format\\n\\nIf it is up to me, I choose [Google Java Format](https://github.com/google/google-java-format). Because:\\n\\n* There are plugins for Gradle and Maven so that it can easily be integrated in the build process. Applied when building and verified in continuous integration.\\n* It takes all decisions regarding formatting of Java code. It can even optimize imports, sorting and removing unused imports.\\n* All decisions in this code style is carefully taken considering how diffs will appear in files where it is applied.\\n* The code style, and the tool support for that, is completely separate from any IDE used. You can let the developers use whatever IDE they want. The important thing is what they produce, the code, which should have no references to any IDE.\\n\\nI use these in different projects:\\n\\n* [FMT Maven Plugin](https://github.com/coveo/fmt-maven-plugin). You just need to add it to the `pom.xml` and it will format the code at compile time. It includes a [validate](https://github.com/coveo/fmt-maven-plugin/blob/master/src/main/java/com/coveo/FMT.java#L62) attribute that can be used in continuous integration, perhaps with a build property, to validate that the code is formatted correctly.\\n* [Google Java Format Gradle Plugin](https://github.com/sherter/google-java-format-gradle-plugin). You just need to add it to the `build.gradle` and it can format the code at compile time with something like `compileJava.dependsOn \'googleJavaFormat\'`. It adds a `verifyGoogleJavaFormat` task to be used in continuous integration to verify formatting.\\n\\nA common problem when auto formatting coding is with newlines. I use *phantom comments* to deal with that. Just add `//` at the end of the line, like [this](https://github.com/tomasbjerre/violations-lib/blob/master/src/main/java/se/bjurr/violations/lib/parsers/CheckStyleParser.java), to force the formatter to keep it that way.\"}]],\"markups\":[],\"sections\":[[10,0]]}','<p>Most projects, that I work with nowadays, have a defined code standard that includes how the code should be formatted. That is great and improves code quality a lot... if followed!</p>\n<p>A single person, in the project, can lower the quality significantly by not formatting the code correctly. If such code is not blocked, from making it into shared branches, you will have the choice of keeping it or fixing it. This is a problem that I find incredibly annoying!</p>\n<ul>\n<li><em>Keeping it</em> means you have to live with faulty formatted code. If some developers use something like <em>save actions</em> in <strong>Eclipse</strong> then they will have a hard time keeping the lines unchanged when working on the file.</li>\n<li><em>Re-formatting it</em> in a new commit makes it harder to maintain the project because things like <code>git blame</code> will show the re-format commit, not the original feature commit.</li>\n</ul>\n<p>The most common solution to this, among the clients I\'ve been working with, is to use a code style defined in <strong>Eclipse</strong>. Along with <em>save actions</em>. The code style is imported to <strong>Eclipse</strong> from an XML-file and some clients also use <a href=\"http://marketplace.eclipse.org/content/workspace-mechanic\">Workspace Mechanics</a> to setup <em>save actions</em> properly.</p>\n<p>Defining the code style in <strong>Eclipse</strong> is a very bad idea:</p>\n<ul>\n<li>All people are not productive in <strong>Eclipse</strong>, some might for example be using <strong>InteliJ</strong> or <strong>NetBeans</strong>. They will have to have <strong>Eclipse</strong> installed just to use it for formatting.</li>\n<li>The semantics of the settings in the code style may change between versions of <strong>Eclipse</strong> (I have seen it!). Then you may start getting unnecessary diffs in commits after an upgrade.</li>\n<li>There is no way, as far as I know, to verify <strong>Eclipse</strong>-formatting as a step in a continuous integration flow. While it is easy for a reviewer to see that brackets are incorrectly positioned, other things may not be as obvious.</li>\n</ul>\n<h1 id=\"googlejavaformat\">Google Java Format</h1>\n<p>If it is up to me, I choose <a href=\"https://github.com/google/google-java-format\">Google Java Format</a>. Because:</p>\n<ul>\n<li>There are plugins for Gradle and Maven so that it can easily be integrated in the build process. Applied when building and verified in continuous integration.</li>\n<li>It takes all decisions regarding formatting of Java code. It can even optimize imports, sorting and removing unused imports.</li>\n<li>All decisions in this code style is carefully taken considering how diffs will appear in files where it is applied.</li>\n<li>The code style, and the tool support for that, is completely separate from any IDE used. You can let the developers use whatever IDE they want. The important thing is what they produce, the code, which should have no references to any IDE.</li>\n</ul>\n<p>I use these in different projects:</p>\n<ul>\n<li><a href=\"https://github.com/coveo/fmt-maven-plugin\">FMT Maven Plugin</a>. You just need to add it to the <code>pom.xml</code> and it will format the code at compile time. It includes a <a href=\"https://github.com/coveo/fmt-maven-plugin/blob/master/src/main/java/com/coveo/FMT.java#L62\">validate</a> attribute that can be used in continuous integration, perhaps with a build property, to validate that the code is formatted correctly.</li>\n<li><a href=\"https://github.com/sherter/google-java-format-gradle-plugin\">Google Java Format Gradle Plugin</a>. You just need to add it to the <code>build.gradle</code> and it can format the code at compile time with something like <code>compileJava.dependsOn \'googleJavaFormat\'</code>. It adds a <code>verifyGoogleJavaFormat</code> task to be used in continuous integration to verify formatting.</li>\n</ul>\n<p>A common problem when auto formatting coding is with newlines. I use <em>phantom comments</em> to deal with that. Just add <code>//</code> at the end of the line, like <a href=\"https://github.com/tomasbjerre/violations-lib/blob/master/src/main/java/se/bjurr/violations/lib/parsers/CheckStyleParser.java\">this</a>, to force the formatter to keep it that way.</p>\n','30','Most projects, that I work with nowadays, have a defined code standard that\nincludes how the code should be formatted. That is great and improves code\nquality a lot... if followed!\n\nA single person, in the project, can lower the quality significantly by not\nformatting the code correctly. If such code is not blocked, from making it into\nshared branches, you will have the choice of keeping it or fixing it. This is a\nproblem that I find incredibly annoying!\n\n * Keeping it  means you have to live with faulty formatted code. If some\n   developers use something like save actions  in Eclipse  then they will have a\n   hard time keeping the lines unchanged when working on the file.\n * Re-formatting it  in a new commit makes it harder to maintain the project\n   because things like git blame  will show the re-format commit, not the\n   original feature commit.\n\nThe most common solution to this, among the clients I\'ve been working with, is\nto use a code style defined in Eclipse. Along with save actions. The code style\nis imported to Eclipse  from an XML-file and some clients also use Workspace\nMechanics [http://marketplace.eclipse.org/content/workspace-mechanic]  to setup \nsave actions  properly.\n\nDefining the code style in Eclipse  is a very bad idea:\n\n * All people are not productive in Eclipse, some might for example be using \n   InteliJ  or NetBeans. They will have to have Eclipse  installed just to use\n   it for formatting.\n * The semantics of the settings in the code style may change between versions\n   of Eclipse  (I have seen it!). Then you may start getting unnecessary diffs\n   in commits after an upgrade.\n * There is no way, as far as I know, to verify Eclipse-formatting as a step in\n   a continuous integration flow. While it is easy for a reviewer to see that\n   brackets are incorrectly positioned, other things may not be as obvious.\n\nGoogle Java Format\nIf it is up to me, I choose Google Java Format\n[https://github.com/google/google-java-format]. Because:\n\n * There are plugins for Gradle and Maven so that it can easily be integrated in\n   the build process. Applied when building and verified in continuous\n   integration.\n * It takes all decisions regarding formatting of Java code. It can even\n   optimize imports, sorting and removing unused imports.\n * All decisions in this code style is carefully taken considering how diffs\n   will appear in files where it is applied.\n * The code style, and the tool support for that, is completely separate from\n   any IDE used. You can let the developers use whatever IDE they want. The\n   important thing is what they produce, the code, which should have no\n   references to any IDE.\n\nI use these in different projects:\n\n * FMT Maven Plugin [https://github.com/coveo/fmt-maven-plugin]. You just need\n   to add it to the pom.xml  and it will format the code at compile time. It\n   includes a validate  attribute that can be used in continuous integration,\n   perhaps with a build property, to validate that the code is formatted\n   correctly.\n * Google Java Format Gradle Plugin\n   [https://github.com/sherter/google-java-format-gradle-plugin]. You just need\n   to add it to the build.gradle  and it can format the code at compile time\n   with something like compileJava.dependsOn \'googleJavaFormat\'. It adds a \n   verifyGoogleJavaFormat  task to be used in continuous integration to verify\n   formatting.\n\nA common problem when auto formatting coding is with newlines. I use phantom\ncomments  to deal with that. Just add //  at the end of the line, like this\n[https://github.com/tomasbjerre/violations-lib/blob/master/src/main/java/se/bjurr/violations/lib/parsers/CheckStyleParser.java]\n, to force the formatter to keep it that way.','/content/images/2018/02/java.png',0,0,'published',NULL,'public',NULL,NULL,'1','2017-06-11 19:21:23','1','2018-08-30 18:18:39','1','2017-06-11 20:47:00','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('597b4433e521cb4fbd9186a8','9d62c3dd-e910-47a8-bb7a-f060f74e8993','Starta Eget Konsultbolag','starta-eget-konsultbolag','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Efter över 7 år som anställd konsult (mjukvaruutvecklare) valde jag nyligen att starta eget aktiebolag. Jag gör precis samma sak nu som jag gjorde som anställd men numera i mitt eget bolag istället. Det här inlägget handlar om hur jag gjorde detta. Googlar man kring att starta eget så hittar man mycket. Men jag saknade någon som beskriver det utifrån mina förutsättningar, det är vad jag försöker göra här, eftersom jag tror att många har samma förutsättningar som mig. **Oh, and sorry if you don\'t speak Swedish =) The topic is about starting your own company in Sweden, you are probably not interested.**\\n\\nSka också säga att allt jag tagit reda på här är genom googling och jag tar inget ansvar för att det jag säger här är korrekt och riktigt! Så var kritisk när du läser vad jag skriver nedan!\\n\\nJag blir ofta kontaktad angående det här blogginlägget, men jag vill gärna hålla diskussionerna öppet här istället: https://www.facebook.com/groups/791240028009906/\\n\\n# Varför Starta Eget?\\n\\nDu kan **skatteplanera**. Om du redan innan året är slut vet [gräns för statlig skatt](http://www.regeringen.se/pressmeddelanden/2017/03/upprakningen-av-skiktgranserna-for-statlig-inkomstskatt-2018-begransas-och-den-sarskilda-inkomstskatten-for-utomlands-bosatta-hojs/) och eventuella avdrag du kommer att göra. Då kan du räkna ut exakt vilken lön du kan betala ut från bolaget för att du som privatperson ska hamna precis på gränsen för statlig skatt.\\n\\nUtöver lönen kan du också göra en **utdelning**. Då skattar du 20% på summan upp till `2.75` gånger föregående års *inkomstbasbeloppet* för 2018 blir det `2.75 * 59300 = 163075`. Det är det som kallas förenklingsregeln. Man kan också räkna fram en gräns, som är hälften av utbetald lön, och använda den om den är mer fördelaktig. [Den här blog-posten](https://www.foretagande.se/sa-beraknas-skatt-pa-utdelning-3-12-reglerna/) beskriver detta bra. Första året kan du eventuellt inte använda förenklingsregeln, de diskuterar det i [den här tråden](https://forum.vismaspcs.se/visma_spcs/topics/utdelning-forsta-aret).\\n\\nJag har [gjort ett excel-ark](https://docs.google.com/spreadsheets/d/1jfQSSOEINNq3OVc5pYG5NeFkvgcDqOEu1HjzpU2RQm0/edit?usp=sharing) för att **optimera skatten**. Skatteverket har även [en sida](https://www.skatteverket.se/privat/sjalvservice/allaetjanster/tjanster/raknautdinskatt.4.18e1b10334ebe8bc80001950.html) där man kan räkna ut sin skatt. Du kan även läsa mer om [skatter och avgifter på Verksamt](https://www.verksamt.se/starta/skatter-och-avgifter/aktiebolag). Se även [den här sidan](http://www.driva-eget.se/kalkyler/lonekalkyl) för att få en uppfattning om hur mycket lön man kan ta ut.\\n\\nDet finns anledning att vara rädd för det faktum att man inte får några pengar alls om man inte har ett uppdrag. Men eget bolag kan du använda en **[periodiseringsfond](https://www.skatteverket.se/foretagochorganisationer/foretagare/enskildnaringsverksamhet/periodiseringsfond.4.361dc8c15312eff6fd2b8f2.html)** där du lägger undan pengar mellan räkenskapsår. Du kan alltså optimera din lön för att slippa statlig skatt, lägga undan pengar i en periodiseringsfond och sedan betala ut dem ett år då du har mindre inkomster. På så vis slipper du både bolagsskatten och statlig inkomstskatt.\\n\\nDu är **säkrare som egen**. Bolaget kommer förmodligen att dra in mer pengar än vad som är lämpligt att direkt batala ut som lön. Får du *500*kr/timme (ett väldigt lågt pris!) så drar du in cirka *80 000*kr under en månad. Då kan du betala *60 000*kr i lön. Men du vill förmodligen bara betala ut *36 575*kr eftersom det är gränsen för statlig skatt. Att betala den lönen kostar cirka *48 000*kr för bolaget och du har alltså *32 000*kr kvar i bolaget. Gör du det varje månad har du *384 000*kr kvar i bolaget när räkenskapsåret är slut. Du vill förmodligen använda en del för att göra en utdelning. Men för att jämföra med att vara anställd så skulle du kunna betala ut samma lön *6* månader in på nästa år, trots att du står helt utan inkomster! För att räkna ut hur mycket lön du kan betala ut kan du alltså räkna såhär `timpenning*(52-semesterveckor)*40/12/1.31` så kanske `500*(52-6)*40/12/1.31 = 58 524`.\\n\\nDu väljer själv om du vill betala in till en **tjänstepension** och i så fall vilken. Jag har tvingats betala till SEB under en längre tid. Det enda jag bryr mig om är deras fondutbud. I SEB hittade jag bara en fond, [en räntefond](http://web.msse.se/SEB/SGF/quicktake.aspx?six=SEB-BDSEKA), som kändes \\\"ok\\\". Det är även en fördel att du kan maximera lönen, göra utdelning och om det efter detta fortfarande finns pengar över kan du använda dem till tjänstepension. Du slipper alltså låsa in pengar i en sådan pension om det inte är så att det är en skattemässig fördel. Med tanke på att du, via utdelning, kan få ut kapital med en skatt på under 40% så kanske du inte vill använda en tjänstepension. Pengarna du får ut via utdelningen är ju helt olåsta och du kan investera hur du vill.\\n\\nDu **slipper ha lön efter ålder**. Ju yngre du är, ju mer tjänar du på att ha eget konsultbolag. Den dumma tanken att man ska börja på en låg lönenivå och sedan öka ett par procent per år mappar inte alls mot [vad kunderna betalar](https://computersweden.idg.se/2.2683/1.717731/java-utvecklare-heta) för dig per timme. Kunderna har oftast ett pris på juniorer och ett på seniorer. Att du är 30, 31 eller 40 år gammal spelar ingen roll för priset. Är du yngre anställd konsult betalar du för dina äldre kollegors lön och förmåner. Det kan mycket väl vara så att du drar in mer pengar än någon som är 20 år äldre. Med eget bolag mer än dubblade jag min lön.\\n\\nDu kan **köpa kontorsmaterial väldigt billigt**. Dator, headset, hörlurar, skärmar... till mindre än halva priset jämfört med om du köper det privat. Då tänker jag inte bara på momsen. Tänk även på hur mycket pengar bolaget behöver betala ut för att, efter skatt och arbetsgivaravgift, kunna sätta in summan på det privata kontot. Såklart måste man tänka på att det man köper ägs av företaget och inte dig privat.\\n\\nSå fort du har möjlighet att betala ut **[skattefritt traktamente](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm)** så ska du göra det. Du slipper tjafs med en arbetsgivare som nekar dig något som är så självklart.\\n\\nDu kan **åka på vilken konferens du vill** och låta bolaget betala. Det finns ju intressanta utvecklarkonferenser nästan överallt. Du behöver bara [hitta en](http://lmgtfy.com/?q=sidney+developer+conference) och åka. Inget tjafs med en arbetsgivare som tycker det är för dyrt. Inga dryga övningar, innan eller på plats, utan bara du som bestämmer vilka föreläsningar du tycker är intressanta och som du går på.\\n\\nJag tycker att de flesta möten är meningslösa och väldigt tråkiga. Det blir klart **färre möten** med eget bolag. Du kan spendera din tid med kunden istället för tråkiga månadsmöten =)\\n\\nDu kan få **mindre administrativt jobb**. Som anställd hade jag kvitton att rapportera varje vecka. Jag köpte tågbiljetter via SJ:s hemsida och fick alltså kvittot som PDF. Jag var tvungen att rapporera dem varje vecka med kostnad och moms. Skriva ut ett papper i samband med tidrapportering. Skriva ut mina kvitton. Häffta fast mina kvitton på pappret från tidrapporteringssystemet. Leta upp kuvert samt frimärke. Leta upp brevlåda och posta kuvert med tidrapporten till konsultbolagets kontor. Som egen lägger jag aldrig mer än 2 timmar totalt under en hel månad på att sköta den fullständiga bokföring inklusive alla kvitton. Allt gör jag digitalt.\\n\\nTill sist, och kanske det absolut bästa argumentet. Jag har **alltid sett mig själv som mitt eget varumärke**. Då är ju inte steget långt till att faktiskt vara det också.\\n\\n# Hur Starta Eget?\\nJag listar här kortfattat vad man behöver göra. Längre ner finns vissa steg mer utförligt beskrivna. Du kan räkna med att det tar 1-2 månader från att du skickar in första ansökan till att allt runt bolaget är klart och du kan börja jobba i det. Skadar inte att titta på [andra listor också](https://www.verksamt.se/alla-e-tjanster/din-checklista/aktiebolag) för att säkra att du inte glömt något.\\n\\nJag valde **aktiebolag** framför enskild firma. Då är bolaget en juridisk person som också har ansvaret. En tydlig gräns mellan bolag och privat ekonomi alltså. Jag kan rekommendera [Verksamts jämförelse av företagsformer](https://www.verksamt.se/fundera/valj-foretagsform).\\n\\n**Skapa ansökan** på [Verksamt](https://www.verksamt.se/). Du kommer behöva en verksamhetsbeskrivning. Min ser ut såhär:\\n> Företaget ska bedriva konsultverksamhet företrädesvis inom IT, samt utveckla mjukvaruprodukter, äga och förvalta värdepapper och utöva därmed förenlig verksamhet.\\n\\nNär det gäller **företagsnamn** så rekommenderar jag att bara ta ditt eget namn. AB eller aktiebolag måste vara med i namnet. Så jag valde *Tomas Bjerre AB*. Risken om du väljer något annat är att Bolagsverket nekar dig för att namnet liknar något som redan finns. Onödigt strul och du kan alltid ändra senare.\\n\\nDu kommer behöva ange en **SNI-kod**. Jag valde *62010*.\\n\\nNär du skickat iväg ansökan till Bolagsverket kommer de invänta ett **bankintyg**. Läs mer om att välja bank nedan. Du behöver kontakta en bank för att skapa ett företagskonto. Banken kommer be dig sätta in 50 000 kr på ett speciellt konto. Banken behöver kunna tala om för Bolagsverket att här finns det 50 000 kr som satts in med avsikt att användas för aktiekapitalet. När du gjort detta ger de dig en e-postadress som du sedan ger till Bolagsverket (via verksamt.se). Sedan frågan Bolagsverket banken om pengarna är där och när banken svarar ja registreras bolaget. Så fort bolaget är skapat så har du sedan ett företagskonto där det från start finns 50 000 kr.\\n\\nJag valde **räkenskapsåret** som *0101 - 1231*. Jag kan dock rekommendera brutet räkenskapsår. Alltså att man väljer ett år som inte är ett kalenderår. Fördelen blir då att du kan välja vilket år du betalar ut lönen på, om du når statlig skatt 2017 kan du vänta till 2018 med att fortsätta betala ut lön. Det är inte helt enkelt att [ändra i efterhand](https://www.skatteverket.se/download/18.3aa8c78a1466c584587d059/1408515815569/42405.pdf) och att ändra det bara för att undvika skatt är inte en godtagbar anledning.\\n\\nNär du startar aktiebolag ska bolaget ha **F-skatt** och du som privatperson har A-skatt. Jag valde **kvartalsmoms**. Som **redovisningsmetod** valde jag [kontantmetoden](https://www.e-conomic.se/bokforingsprogram/ordlista/kontantmetoden) (kallas även bokslutsmetoden). Jag uppfattade den som enklast, se hur man [bokför köpt med kreditkort](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) t.ex.\\n\\nDet är väldigt svårt att uppskatta den **förväntade vinsten** första året. Andra året kommer Skatteverket föreslå en. Ett tips är att ange den lågt. I mitt fall skulle jag först betala in 20 000 kr innan jag hunnit få en enda inkomst i bolaget. Skulle du råka ut för det är det bara att göra en ny preliminär inkomstdeklaration via Skatteverket och ange en lägre förväntad vinst. Då fick jag ner denna siffran till 7 000 kr istället.\\n\\nDet finns många smidiga **bokföringsprogram**. [Redovisningen](https://sv.wikipedia.org/wiki/Redovisning) var nog det jag var mest rädd för i början. Jag märkte tidigt att när jag googlade om hur man bokför olika saker så kom jag ofta in på [Visma:s support forum](https://forum.vismaspcs.se/visma_spcs). Jag har tidigare använt deras tidrapportering, PX, vilket suger något helt otroligt. Ser ut som en sommarjobbare hackade ihop det för 15 år sedan och att man inte rört det sedan dess. Men även om jag var väldigt skeptisk till detta bolag så valde jag ändå <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> för bokföring just för att deras support verkar vara helt exemplarisk. Att direkt bokföra ett kvitto i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> är betydligt enklare än det jag tidigare gjort via tidrapportering som anställd. Du kanske t.ex. vill bokföra [bokföringsprogrammet](https://forum.vismaspcs.se/visma_spcs/topics/8fxz374h0czze?topic-reply-list%5Bsettings%5D%5Bfilter_by%5D=all), [köp av dator och försäkring](https://forum.vismaspcs.se/visma_spcs/topics/7h3lv4csqal8w) eller [köp av tågbiljett](https://forum.vismaspcs.se/visma_spcs/topics/con2lpq5fex2f). De [gör t.o.m. filmer](https://www.youtube.com/watch?v=jGgnxd6uBh4) där de visar vissa flöden. Hoppas jag är trovärdig trots att jag gått med i deras affiliate-program och får betalt för att hänvisa andra dit =)\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">\\n\\n![Visma eEkonomi](/content/images/2017/07/affiliate-2014-vismaeekonomi1_728x90_eeko.gif)\\n</a>\\n\\nDu lär behöva en **mobil** i bolaget. Som jag förstår det är det ok att köpa telefonen på bolaget. Även telefoni och SMS är ok. Datatrafiken däremot kan behöva förmånsbeskattas. Det i kombination med att man slipper många dryga samtal om man istället har ett privat abonnemang, gjorde att jag köpte den privat.\\n\\nDu lär behöva en **ansvarsförsäkring**. Jag valde en [från If](https://www.if.se/web/se/foretag/varaforsakringar/offert/pages/default.aspx?varde=startforsakring) och fick betala 5 294 kr. Har du inte det och du råkar ut för något, säg att någon kräver dig på flera miljoner, så konkursar du bara bolaget och slipper undan. Men med en ansvarsförsäkring så kan du, kanske, göra rätt för dig och betala. På så vis blir du en mer attraktiv konsult att anlita och det står även ofta i kontrakten att du ska ha en försäkring.\\n\\nJag valde att kontakta en **redovisningskonsult** som hjälper mig med ekonomiska frågor samt tar hand om **bokslut** / **årsredovisning** / **deklaration**. Jag tar hand om löpande bokföring samt kvartalsmoms, skatter och lön. Det kostar cirka 6 000 kr per år. En redovisningskonsult kan hjälpa dig med allt möjligt i bolaget. Vissa ger dem fullmakt så de kan sköta all ekonomi. Jag fick tag på honom genom en bekant som tipsade men annars är det enkelt att googla. Har du valt Visma så [kan de hjälpa dig](https://vismaspcs.se/produkter/redovisning/visma-eekonomi-byra). I <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du även skapa en inloggning till din redovisningskonsult.\\n\\n**Revisor** är valfritt om man [omsätter under 3 miljoner](http://www.bolagsverket.se/ff/foretagsformer/aktiebolag/starta/revisor). Det använder jag inte.\\n\\nDet kan också vara bra att ha klart för sig **hur man hittar kunder**. Det finns många rena konsultmäklare som tar en procentsats, ofta mellan 10-20% på din timpenning, om du tar ett uppdrag via dem. De har ofta ramavtal med stora konsultköpare. Är du redan anställd som konsult kanske du i början fortsätter med samma kund fast som underkonsult istället. Se, t.ex.:\\n\\n * [Assistera](http://www.assistera.se/sv)\\n * [Brainville](https://www.brainville.com/)\\n * [EWork](https://www.eworkgroup.com/se/)\\n * [Experis](https://cv.experis.se/)\\n * [Kvadrat](http://www.kvadrat.se/bli-kvadratare/)\\n * [Toptal](https://www.toptal.com/)\\n\\nOch till sist, du kan eventuellt göra **[investeraravdraget](http://www.kompred.se/nyttiga-tips/missa-inte-investeraravdraget/)** i din privata deklaration efter att du startat aktiebolag.\\n\\n## Bank\\n\\nJag frågade först några banker via telefon och mail (ICA, Avanza, Swedbank). Det kändes som att de inte riktigt förstod vad jag menade när jag sa att jag ville ha ett **bankintyg**.\\n\\n### Danskebank\\n\\n**Jag valde Danskebank**. Du fyller bara i [deras formulär](https://danskebank.se/sv-se/Foretag/Smaforetag/Pages/Product-packages.aspx) så ringer de upp. Danskebank sköter allt per telefon och det fungerar mycket bra. De bad inte om någon affärsplan eller budget utan litade på mig direkt. De visste också direkt vad det handlade om och de kändes pålitliga. Eftersom jag inte var kund där sedan tidigare så behövde de skicka lite papper till mig med posten först. Men inom en vecka hade jag intyget! Snabbt, enkelt, smidigt och inget meningslöst krångel.\\n\\nJag valde *Danske Business Plus* för 99 kr/mån. **<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> stödjer även Danskebank** på så vis att man kan klistra in kontoutdraget från banken in i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>, för att automatiskt skapa bankhändelser. Sedan matchar man dem mot leverantörsfakturor eller verifikationer.\\n\\n# Hur Driva Eget?\\n\\nJag har lagt in återkommande händelser i Google Calendar för de olika datumen. Sedan angivit att de ska upprepas månadsvis eller, i vissa fall, årsvis. Jag lägger ungefär 2 timmar varje månad på att sköta företaget. Vad jag gör är:\\n\\n* En gång per månaden\\n  * Importerar transaktioner från skattekontot från Skatteverket.\\n  * Importerar transaktioner från företagskontot från banken.\\n  * Skickar fakturor för månaden.\\n  * Betalar ut lön, till anställde.\\n  * Betalar skatt och avgifter till Skatteverket\\n      * Arbetsgivaravgift.\\n      * Inkomstskatt.\\n      * Debiterad preliminärskatt.\\n* En gång per kvartal.\\n  * Betalar in kvartalsmoms.\\n* En gång per år.\\n  * Bokslut, årsredovisning, deklaration.\\n  * I januari skickar in kontrolluppgit\\n      * Av utbetald lön för föregående år till Skatteverket.\\n      * Av föregående års utdelning (KU31).\\n  * Lämnar K10 i privata deklarationen, om utdelning mottagits under året som deklareras.\\n\\nJag valde **kvartalsmoms**, alltså att jag efter varje kvartal redovisar och betalar in eventuell moms. Mer information om det finns [här]( https://www.verksamt.se/driva/skatter-och-avgifter/moms-redovisa-och-betala). Så fort kvartalet är slut kan man lämna in deklarationen. Man har ungefär 1.5 månad på sig att lämna in den. Även för detta har Visma gjort en film som visar [hur du gör i eEkonomi](https://www.youtube.com/watch?v=gPv_SahMnEw). Som jag förstår det använder man kontot *1630* (eller *2012* om det är enskild firma) just för att [alla händelser mot Skatteverket ska gå via detta konto](https://forum.vismaspcs.se/visma_spcs/topics/maste-man-anvanda-sig-av-skattekontot-1630). Hur det fungerar med att betala, och få tillbaka moms, [beskriver Skatteverkets här](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html). Så kortfattat om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> och kvartalsmoms så ska du vid varje kvartalsslut:\\n\\n * Skapa en momsredovisning för kvartalet via bokföring / momsredovisning.\\n * Ladda ner *eskd*-filen.\\n * Skapa en [momsdeklaration](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms.4.7459477810df5bccdd480006935.html) enligt [Visma:s video](https://www.youtube.com/watch?v=gPv_SahMnEw).\\n * Ladda upp *eskd*-filen hos [Skatteverket](https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html).\\n * Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n * När du senare betalat in, eller fått betalt, skapa och bokför bankhändelsen på företagskontot. Det visas i slutet på [den här videon](https://www.youtube.com/watch?v=gPv_SahMnEw). Tänk på att använda *1630* för AB och *2012* för enskild firma.\\n\\nSkatteverket har skickat brev till dig med datum då du ska betala **debiterad preliminärskatt**. Det här är något du betalar varje månad och baserar sig på din förväntade vinst som du angav i din preliminära inkomstdeklaration. Du har fått ett besked från Skatteverket där det står datum och hur mycket du ska betala. Om du har <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du följa deras [film om skattekontot](https://www.youtube.com/watch?v=-7cDnn-NaME).\\n* Betala från företagskontot till företagets skattekonto.\\n* Importerar bankhändelser från företagskontot.\\n* Ladda ner [transaktioner från Skatteverket till ditt skattekonto](https://www.youtube.com/watch?v=-7cDnn-NaME) och matchar dem sedan mot momshändelsen i dina bokföringshändelser.\\n* Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n* Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n* När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n* Matcha händelsen mot bokföringsförslaget *Debiterad preliminärskatt*.\\n\\nNär du ska betala ut **Lön** behöver du betala *skatt*, *arbetsgivaravgift* samt själva *lönen*. Både *skatt* och *arbetsgivaravgift* betalas till företagets *skattekonto* så det är 2 betalningar som behöver göras från företagskontot. Om du använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kan du göra såhär. Finns även en [film här](https://www.youtube.com/watch?v=ig2C9gQg2Eg) och en [bra tråd](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-lon-i-eekonomi) om detta i supportforumet.\\n* Under *lön* i huvudnenyn väljer du *anställda*. Lägg upp den anställde. Du kommer behöva *Skattetabell* att använda. Den anställde kan [logga in](https://www.skatteverket.se/) hos Skatteverket och hittar då skattetabell under \\\"Skatter och deklarationer\\\".\\n* Under *lön* i huvudnenyn väljer du *lönebesked* och skapar ett nytt.\\n* Ange lönen och välj bokför.\\n* Logga in på banken och betala ut lönen, efter skatt, till den anställdes konto.\\n* Importera bankhändelsen till *företagskontot* via *kassa och bankhändelser*.\\n* <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> kommer föreslå att du matchar bankhändelsen mot löneutbetalningen, gör det. Nu är du klar med lönen, och nästa moment är att betala skatt och arbetsgivaravgift.\\n\\n**Arbetsgivardeklaration** lämnas enkelt via [Skatteverkets webbsida](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html).\\n\\n* Om du inte har betalat ut någon lön sätter du bara **0** i de obligatoriska fälten och skickar in.\\n* Om du har betalat ut lön, och använder <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>.\\n  * Gå in under *lön* i huvudmenyn och välj *arbetsgivardeklaration*.\\n  * Skapa ny.\\n  * Välj månaden då lönen betaldes ut.\\n  * Välj *bokför*. \\n  * Nu kan du [exportera en fil](https://www.youtube.com/watch?v=ig2C9gQg2Eg) om du klickar på *åtgärder* för deklarationen.\\n  * [Lämna in arbetsgivardeklarationen](https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html) hos Skatteverket genom att importera filen.\\n  * På kvittensen visas ett bankgiro och OCR att använda för att betala in skatten till företagets skattekonto.\\n  * Betala in summan från företagets bankkonto.\\n  * Importera bankhändelsen från banken till företagskontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a>\\n  * Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.\\n  * När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.\\n  * Matcha händelsen mot bokföringsförslaget arbetsgivardeklaration. Skatteverkat drar arbetsgivardeklartionen i 2 transaktioner, du behöver [manuellt slå ihop dem](https://forum.vismaspcs.se/visma_spcs/topics/hur-matchar-jag-tva-belopp-pa-skattekontot-mot-en-arbetsgivardeklaration) för att kunna använda det här bokföringsföslaget. Men annars är det *1630* som krediteras och *2710* (personalskatt) samt *2731* (arbetsgivaravgift) som debiteras. Jag tycker även [den här sidan](https://www.blinfo.se/foretagskunskap/bokfora-lon__15472) är bra här.\\n\\n<a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> har en färdig mall för **fakturor**. Du anger ditt företags bankgiro, och/eller kontonummer, lägger upp kunder och artiklar. Sedan kan du enkelt skapa fakturor som du sedan kan skriva ut, spara som PDF eller maila.\\n\\nOm du **betalar ut traktamente** behöver du kunna bevisa att [resorna ägt rum och varit tjänsteresor](https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm). Därför kan det vara klokt att föra anteckningar för resor. I mitt fall åker jag alltid tåg och kommer hänvisa till tågbiljetterna om jag får revision av Skatteverket. Jag skapar exceldokument, med uträkningen, som jag sedan exporterar till PDF och sparar för att motivera traktamentet.\\n\\n![Traktamente](/content/images/2017/11/traktamente-utr.png)\\n\\nMan hör ibland uttrycket **skriva av** och jag tror många missförstår det, kanske även jag =) Men jag tolkar det som att allt man köper för under ett halvt prisbasbelopp (ungefär 22 000 kr) skrivs av direkt, kostar det mer skriver man av det under flera år. Att *skriva av* innebär att man inte tar upp det som en tillgång, en inventarie, i bokföringen. Det är dock inte samma sak som att man får ge bort prylen till sig själv eller någon annan. Men jag är osäker, [och många med mig](https://www.flashback.org/p49423662).\\n\\nAtt bokföra köp gjorda med **kreditkort** är lite speciellt men det har Visma en väldigt [bra artikel](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi) om.\\n\\nJag sparar alltid alla **kvitton** direkt på Google Drive. Är det inte digitalt fotar jag av med mobilen, laddar upp bilden och sparar även original-kvittot i en pärm. I bokföringsprogrammet är jag också noga med att **ladda upp bilder på varje verifikation**. [Bokföringslagen](https://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/bokforingslag-19991078_sfs-1999-1078) säger att originalen ska sparas, så digitala ska sparas digitalt och de på papper sparas på papper. Jag tycker även att [den här artikeln](https://vismaspcs.se/ditt-foretagande/byra/de-har-kopiorna-behover-du-inte-skriva-ut) beskriver det bra. Även andra dokument fotar jag av och sparar på Google Drive, som beslut från Skatteverket t.ex. Väldigt smidigt att kunna läsa allt var man än är.  Det är ganska enkelt att få tag på ett rejält brandskyddat kassaskåp ([#1](http://www.clasohlson.com/se/Dokumentsk%C3%A5p-Chubbsafes-Executive-40/Pr342428000) eller [#2](http://www.biltema.se/sv/Hem/Sakerhet-och-larm/Sakerhetsbox/Kassaskap-brandsakert-2000033385/)) så att man kan spara papper korrekt enligt lagstiftningen.\\n\\nPå **bokslutsdagen, sista dagen på räkenskapsåret, måste [obetalda fakturor tas upp i bokföring](https://www.verksamt.se/driva/ekonomi-och-bokforing/lopande-bokforing)** även om du använder kontantmetoden. Det beskriver också Visma [här](https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi). Om du har skapat en faktura som ännu inte blivit betald kommer <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> att ge dig följande meddelande när du skapar sista **kvartalsmomsen**.\\n\\n> Enligt Kontantmetoden ska du bokföra dina obetalda kundfordringar och leverantörsskulder vid räkenskapsårets slut och redovisa momsen. Detta kommer nu att göras automatiskt. Du bör kontrollera att alla inbetalningar och utbetalningar på året är avstämda innan du fortsätter. Vill du fortsätta?\\n\\nDet är viktigt att ta en extra titt på de sista transaktionerna innan räkenskapsåret är slut så att det inte dykt upp något i sista sekunden. I mitt fall hade Danskebank skapat en faktura, och dragit pengarna, för bankavgifter sista dagen i december.\\n\\nNär räkenskapsåret är slut ska **kontrolluppgifter** för anställda skickas till Skatteverket. Det här gäller 2017 men är på gång att ändras så att man skickar samma uppgifter i arbetsgivardeklarationen istället. Men i <a href=\\\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\\\" rel=\\\"nofollow\\\">Visma eEkonomi</a> finns en funktion under *lön* i huvudmenyn som kan användas om du skapat lönespecar.\\n\\n* Klicka på *Ny kontrolluppgift*.\\n* Fyll i dina uppgifter.\\n* Spara.\\n* Nu kan man ladda ner en *xml-fil* som kan laddas upp hos Skatteverket för att fylla i *KU10*-blanketten.\\n\\nVisma har en väldigt bra [gratis broschyr om bokföring](http://download.vismaspcs.se/pdf/bokf%C3%B6ring-f%C3%B6r-nyb%C3%B6rjare.pdf?utm_source=Eloqua&utm_medium=email&utm_content=SE_SP_SI_Onboarding-eEko-Std-4&utm_campaign=SE_SP_SI_Onboarding-eA-Std-1704).\\n\\n# Övrigt\\n\\nOm du **veckopendlar** som jag så kanske du har en **lägenhet på arbetsorten**. Då kan man tänka sig att man [låter bolaget betala den](http://www.skatter.se/?q=node/2619). Jag blev rekommenderad att ta den privat och betala ut mer lön från bolaget istället. Då räknar jag på detta vid utbetalning av lön så att lön efter avdrag hamnar på gränsen för statlig skatt. Avdraget gör jag alltså i min privata deklaration. Väljer man att låta bolaget direkt betala lägenheten blir det lite krångligare med bokföring och deklaration. Det kan också vara så att Skatteverket ser det som en förmån. Så det här kändes enklast.\\n\\nDet finns bolag som har som **affärsidé att bara ha underkonsulter**. De åker på konferensresor och har gemensam kontorslokal precis som vilket annat bolag som helst. Skillnaden är bara att alla är underkonsulter. Detta bolag hjälper dig att starta ditt eget bolag och hitta kunder. De har ramavtal med stora konsultköpare och kan ordna bra timpriser. Ett sånt bolag är [Kvadrat](http://www.kvadrat.se/).\\n\\nJag valde först att ta över kontraktet jag var på och då gå från anställd konsult till underkonsult. Senare bytte jag uppdrag och då anslöt jag mig till [Kvadrat](http://www.kvadrat.se/).\\n\\nHar du frågor om skatter så är det smidigt att man kan [maila Skatteverket](https://www.skatteverket.se/omoss/kontaktaoss/mejla/).\\n\\nJag har skapat en facebook-grupp för att diskutera det här, går gärna med! https://www.facebook.com/groups/791240028009906/\"}]],\"markups\":[],\"sections\":[[10,0]]}','<p>Efter över 7 år som anställd konsult (mjukvaruutvecklare) valde jag nyligen att starta eget aktiebolag. Jag gör precis samma sak nu som jag gjorde som anställd men numera i mitt eget bolag istället. Det här inlägget handlar om hur jag gjorde detta. Googlar man kring att starta eget så hittar man mycket. Men jag saknade någon som beskriver det utifrån mina förutsättningar, det är vad jag försöker göra här, eftersom jag tror att många har samma förutsättningar som mig. <strong>Oh, and sorry if you don\'t speak Swedish =) The topic is about starting your own company in Sweden, you are probably not interested.</strong></p>\n<p>Ska också säga att allt jag tagit reda på här är genom googling och jag tar inget ansvar för att det jag säger här är korrekt och riktigt! Så var kritisk när du läser vad jag skriver nedan!</p>\n<p>Jag blir ofta kontaktad angående det här blogginlägget, men jag vill gärna hålla diskussionerna öppet här istället: <a href=\"https://www.facebook.com/groups/791240028009906/\">https://www.facebook.com/groups/791240028009906/</a></p>\n<h1 id=\"varfrstartaeget\">Varför Starta Eget?</h1>\n<p>Du kan <strong>skatteplanera</strong>. Om du redan innan året är slut vet <a href=\"http://www.regeringen.se/pressmeddelanden/2017/03/upprakningen-av-skiktgranserna-for-statlig-inkomstskatt-2018-begransas-och-den-sarskilda-inkomstskatten-for-utomlands-bosatta-hojs/\">gräns för statlig skatt</a> och eventuella avdrag du kommer att göra. Då kan du räkna ut exakt vilken lön du kan betala ut från bolaget för att du som privatperson ska hamna precis på gränsen för statlig skatt.</p>\n<p>Utöver lönen kan du också göra en <strong>utdelning</strong>. Då skattar du 20% på summan upp till <code>2.75</code> gånger föregående års <em>inkomstbasbeloppet</em> för 2018 blir det <code>2.75 * 59300 = 163075</code>. Det är det som kallas förenklingsregeln. Man kan också räkna fram en gräns, som är hälften av utbetald lön, och använda den om den är mer fördelaktig. <a href=\"https://www.foretagande.se/sa-beraknas-skatt-pa-utdelning-3-12-reglerna/\">Den här blog-posten</a> beskriver detta bra. Första året kan du eventuellt inte använda förenklingsregeln, de diskuterar det i <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/utdelning-forsta-aret\">den här tråden</a>.</p>\n<p>Jag har <a href=\"https://docs.google.com/spreadsheets/d/1jfQSSOEINNq3OVc5pYG5NeFkvgcDqOEu1HjzpU2RQm0/edit?usp=sharing\">gjort ett excel-ark</a> för att <strong>optimera skatten</strong>. Skatteverket har även <a href=\"https://www.skatteverket.se/privat/sjalvservice/allaetjanster/tjanster/raknautdinskatt.4.18e1b10334ebe8bc80001950.html\">en sida</a> där man kan räkna ut sin skatt. Du kan även läsa mer om <a href=\"https://www.verksamt.se/starta/skatter-och-avgifter/aktiebolag\">skatter och avgifter på Verksamt</a>. Se även <a href=\"http://www.driva-eget.se/kalkyler/lonekalkyl\">den här sidan</a> för att få en uppfattning om hur mycket lön man kan ta ut.</p>\n<p>Det finns anledning att vara rädd för det faktum att man inte får några pengar alls om man inte har ett uppdrag. Men eget bolag kan du använda en <strong><a href=\"https://www.skatteverket.se/foretagochorganisationer/foretagare/enskildnaringsverksamhet/periodiseringsfond.4.361dc8c15312eff6fd2b8f2.html\">periodiseringsfond</a></strong> där du lägger undan pengar mellan räkenskapsår. Du kan alltså optimera din lön för att slippa statlig skatt, lägga undan pengar i en periodiseringsfond och sedan betala ut dem ett år då du har mindre inkomster. På så vis slipper du både bolagsskatten och statlig inkomstskatt.</p>\n<p>Du är <strong>säkrare som egen</strong>. Bolaget kommer förmodligen att dra in mer pengar än vad som är lämpligt att direkt batala ut som lön. Får du <em>500</em>kr/timme (ett väldigt lågt pris!) så drar du in cirka <em>80 000</em>kr under en månad. Då kan du betala <em>60 000</em>kr i lön. Men du vill förmodligen bara betala ut <em>36 575</em>kr eftersom det är gränsen för statlig skatt. Att betala den lönen kostar cirka <em>48 000</em>kr för bolaget och du har alltså <em>32 000</em>kr kvar i bolaget. Gör du det varje månad har du <em>384 000</em>kr kvar i bolaget när räkenskapsåret är slut. Du vill förmodligen använda en del för att göra en utdelning. Men för att jämföra med att vara anställd så skulle du kunna betala ut samma lön <em>6</em> månader in på nästa år, trots att du står helt utan inkomster! För att räkna ut hur mycket lön du kan betala ut kan du alltså räkna såhär <code>timpenning*(52-semesterveckor)*40/12/1.31</code> så kanske <code>500*(52-6)*40/12/1.31 = 58 524</code>.</p>\n<p>Du väljer själv om du vill betala in till en <strong>tjänstepension</strong> och i så fall vilken. Jag har tvingats betala till SEB under en längre tid. Det enda jag bryr mig om är deras fondutbud. I SEB hittade jag bara en fond, <a href=\"http://web.msse.se/SEB/SGF/quicktake.aspx?six=SEB-BDSEKA\">en räntefond</a>, som kändes &quot;ok&quot;. Det är även en fördel att du kan maximera lönen, göra utdelning och om det efter detta fortfarande finns pengar över kan du använda dem till tjänstepension. Du slipper alltså låsa in pengar i en sådan pension om det inte är så att det är en skattemässig fördel. Med tanke på att du, via utdelning, kan få ut kapital med en skatt på under 40% så kanske du inte vill använda en tjänstepension. Pengarna du får ut via utdelningen är ju helt olåsta och du kan investera hur du vill.</p>\n<p>Du <strong>slipper ha lön efter ålder</strong>. Ju yngre du är, ju mer tjänar du på att ha eget konsultbolag. Den dumma tanken att man ska börja på en låg lönenivå och sedan öka ett par procent per år mappar inte alls mot <a href=\"https://computersweden.idg.se/2.2683/1.717731/java-utvecklare-heta\">vad kunderna betalar</a> för dig per timme. Kunderna har oftast ett pris på juniorer och ett på seniorer. Att du är 30, 31 eller 40 år gammal spelar ingen roll för priset. Är du yngre anställd konsult betalar du för dina äldre kollegors lön och förmåner. Det kan mycket väl vara så att du drar in mer pengar än någon som är 20 år äldre. Med eget bolag mer än dubblade jag min lön.</p>\n<p>Du kan <strong>köpa kontorsmaterial väldigt billigt</strong>. Dator, headset, hörlurar, skärmar... till mindre än halva priset jämfört med om du köper det privat. Då tänker jag inte bara på momsen. Tänk även på hur mycket pengar bolaget behöver betala ut för att, efter skatt och arbetsgivaravgift, kunna sätta in summan på det privata kontot. Såklart måste man tänka på att det man köper ägs av företaget och inte dig privat.</p>\n<p>Så fort du har möjlighet att betala ut <strong><a href=\"https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm\">skattefritt traktamente</a></strong> så ska du göra det. Du slipper tjafs med en arbetsgivare som nekar dig något som är så självklart.</p>\n<p>Du kan <strong>åka på vilken konferens du vill</strong> och låta bolaget betala. Det finns ju intressanta utvecklarkonferenser nästan överallt. Du behöver bara <a href=\"http://lmgtfy.com/?q=sidney+developer+conference\">hitta en</a> och åka. Inget tjafs med en arbetsgivare som tycker det är för dyrt. Inga dryga övningar, innan eller på plats, utan bara du som bestämmer vilka föreläsningar du tycker är intressanta och som du går på.</p>\n<p>Jag tycker att de flesta möten är meningslösa och väldigt tråkiga. Det blir klart <strong>färre möten</strong> med eget bolag. Du kan spendera din tid med kunden istället för tråkiga månadsmöten =)</p>\n<p>Du kan få <strong>mindre administrativt jobb</strong>. Som anställd hade jag kvitton att rapportera varje vecka. Jag köpte tågbiljetter via SJ:s hemsida och fick alltså kvittot som PDF. Jag var tvungen att rapporera dem varje vecka med kostnad och moms. Skriva ut ett papper i samband med tidrapportering. Skriva ut mina kvitton. Häffta fast mina kvitton på pappret från tidrapporteringssystemet. Leta upp kuvert samt frimärke. Leta upp brevlåda och posta kuvert med tidrapporten till konsultbolagets kontor. Som egen lägger jag aldrig mer än 2 timmar totalt under en hel månad på att sköta den fullständiga bokföring inklusive alla kvitton. Allt gör jag digitalt.</p>\n<p>Till sist, och kanske det absolut bästa argumentet. Jag har <strong>alltid sett mig själv som mitt eget varumärke</strong>. Då är ju inte steget långt till att faktiskt vara det också.</p>\n<h1 id=\"hurstartaeget\">Hur Starta Eget?</h1>\n<p>Jag listar här kortfattat vad man behöver göra. Längre ner finns vissa steg mer utförligt beskrivna. Du kan räkna med att det tar 1-2 månader från att du skickar in första ansökan till att allt runt bolaget är klart och du kan börja jobba i det. Skadar inte att titta på <a href=\"https://www.verksamt.se/alla-e-tjanster/din-checklista/aktiebolag\">andra listor också</a> för att säkra att du inte glömt något.</p>\n<p>Jag valde <strong>aktiebolag</strong> framför enskild firma. Då är bolaget en juridisk person som också har ansvaret. En tydlig gräns mellan bolag och privat ekonomi alltså. Jag kan rekommendera <a href=\"https://www.verksamt.se/fundera/valj-foretagsform\">Verksamts jämförelse av företagsformer</a>.</p>\n<p><strong>Skapa ansökan</strong> på <a href=\"https://www.verksamt.se/\">Verksamt</a>. Du kommer behöva en verksamhetsbeskrivning. Min ser ut såhär:</p>\n<blockquote>\n<p>Företaget ska bedriva konsultverksamhet företrädesvis inom IT, samt utveckla mjukvaruprodukter, äga och förvalta värdepapper och utöva därmed förenlig verksamhet.</p>\n</blockquote>\n<p>När det gäller <strong>företagsnamn</strong> så rekommenderar jag att bara ta ditt eget namn. AB eller aktiebolag måste vara med i namnet. Så jag valde <em>Tomas Bjerre AB</em>. Risken om du väljer något annat är att Bolagsverket nekar dig för att namnet liknar något som redan finns. Onödigt strul och du kan alltid ändra senare.</p>\n<p>Du kommer behöva ange en <strong>SNI-kod</strong>. Jag valde <em>62010</em>.</p>\n<p>När du skickat iväg ansökan till Bolagsverket kommer de invänta ett <strong>bankintyg</strong>. Läs mer om att välja bank nedan. Du behöver kontakta en bank för att skapa ett företagskonto. Banken kommer be dig sätta in 50 000 kr på ett speciellt konto. Banken behöver kunna tala om för Bolagsverket att här finns det 50 000 kr som satts in med avsikt att användas för aktiekapitalet. När du gjort detta ger de dig en e-postadress som du sedan ger till Bolagsverket (via verksamt.se). Sedan frågan Bolagsverket banken om pengarna är där och när banken svarar ja registreras bolaget. Så fort bolaget är skapat så har du sedan ett företagskonto där det från start finns 50 000 kr.</p>\n<p>Jag valde <strong>räkenskapsåret</strong> som <em>0101 - 1231</em>. Jag kan dock rekommendera brutet räkenskapsår. Alltså att man väljer ett år som inte är ett kalenderår. Fördelen blir då att du kan välja vilket år du betalar ut lönen på, om du når statlig skatt 2017 kan du vänta till 2018 med att fortsätta betala ut lön. Det är inte helt enkelt att <a href=\"https://www.skatteverket.se/download/18.3aa8c78a1466c584587d059/1408515815569/42405.pdf\">ändra i efterhand</a> och att ändra det bara för att undvika skatt är inte en godtagbar anledning.</p>\n<p>När du startar aktiebolag ska bolaget ha <strong>F-skatt</strong> och du som privatperson har A-skatt. Jag valde <strong>kvartalsmoms</strong>. Som <strong>redovisningsmetod</strong> valde jag <a href=\"https://www.e-conomic.se/bokforingsprogram/ordlista/kontantmetoden\">kontantmetoden</a> (kallas även bokslutsmetoden). Jag uppfattade den som enklast, se hur man <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi\">bokför köpt med kreditkort</a> t.ex.</p>\n<p>Det är väldigt svårt att uppskatta den <strong>förväntade vinsten</strong> första året. Andra året kommer Skatteverket föreslå en. Ett tips är att ange den lågt. I mitt fall skulle jag först betala in 20 000 kr innan jag hunnit få en enda inkomst i bolaget. Skulle du råka ut för det är det bara att göra en ny preliminär inkomstdeklaration via Skatteverket och ange en lägre förväntad vinst. Då fick jag ner denna siffran till 7 000 kr istället.</p>\n<p>Det finns många smidiga <strong>bokföringsprogram</strong>. <a href=\"https://sv.wikipedia.org/wiki/Redovisning\">Redovisningen</a> var nog det jag var mest rädd för i början. Jag märkte tidigt att när jag googlade om hur man bokför olika saker så kom jag ofta in på <a href=\"https://forum.vismaspcs.se/visma_spcs\">Visma:s support forum</a>. Jag har tidigare använt deras tidrapportering, PX, vilket suger något helt otroligt. Ser ut som en sommarjobbare hackade ihop det för 15 år sedan och att man inte rört det sedan dess. Men även om jag var väldigt skeptisk till detta bolag så valde jag ändå <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> för bokföring just för att deras support verkar vara helt exemplarisk. Att direkt bokföra ett kvitto i <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> är betydligt enklare än det jag tidigare gjort via tidrapportering som anställd. Du kanske t.ex. vill bokföra <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/8fxz374h0czze?topic-reply-list%5Bsettings%5D%5Bfilter_by%5D=all\">bokföringsprogrammet</a>, <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/7h3lv4csqal8w\">köp av dator och försäkring</a> eller <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/con2lpq5fex2f\">köp av tågbiljett</a>. De <a href=\"https://www.youtube.com/watch?v=jGgnxd6uBh4\">gör t.o.m. filmer</a> där de visar vissa flöden. Hoppas jag är trovärdig trots att jag gått med i deras affiliate-program och får betalt för att hänvisa andra dit =)</p>\n<a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">\n<p><img src=\"/content/images/2017/07/affiliate-2014-vismaeekonomi1_728x90_eeko.gif\" alt=\"Visma eEkonomi\"><br>\n</a></p>\n<p>Du lär behöva en <strong>mobil</strong> i bolaget. Som jag förstår det är det ok att köpa telefonen på bolaget. Även telefoni och SMS är ok. Datatrafiken däremot kan behöva förmånsbeskattas. Det i kombination med att man slipper många dryga samtal om man istället har ett privat abonnemang, gjorde att jag köpte den privat.</p>\n<p>Du lär behöva en <strong>ansvarsförsäkring</strong>. Jag valde en <a href=\"https://www.if.se/web/se/foretag/varaforsakringar/offert/pages/default.aspx?varde=startforsakring\">från If</a> och fick betala 5 294 kr. Har du inte det och du råkar ut för något, säg att någon kräver dig på flera miljoner, så konkursar du bara bolaget och slipper undan. Men med en ansvarsförsäkring så kan du, kanske, göra rätt för dig och betala. På så vis blir du en mer attraktiv konsult att anlita och det står även ofta i kontrakten att du ska ha en försäkring.</p>\n<p>Jag valde att kontakta en <strong>redovisningskonsult</strong> som hjälper mig med ekonomiska frågor samt tar hand om <strong>bokslut</strong> / <strong>årsredovisning</strong> / <strong>deklaration</strong>. Jag tar hand om löpande bokföring samt kvartalsmoms, skatter och lön. Det kostar cirka 6 000 kr per år. En redovisningskonsult kan hjälpa dig med allt möjligt i bolaget. Vissa ger dem fullmakt så de kan sköta all ekonomi. Jag fick tag på honom genom en bekant som tipsade men annars är det enkelt att googla. Har du valt Visma så <a href=\"https://vismaspcs.se/produkter/redovisning/visma-eekonomi-byra\">kan de hjälpa dig</a>. I <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> kan du även skapa en inloggning till din redovisningskonsult.</p>\n<p><strong>Revisor</strong> är valfritt om man <a href=\"http://www.bolagsverket.se/ff/foretagsformer/aktiebolag/starta/revisor\">omsätter under 3 miljoner</a>. Det använder jag inte.</p>\n<p>Det kan också vara bra att ha klart för sig <strong>hur man hittar kunder</strong>. Det finns många rena konsultmäklare som tar en procentsats, ofta mellan 10-20% på din timpenning, om du tar ett uppdrag via dem. De har ofta ramavtal med stora konsultköpare. Är du redan anställd som konsult kanske du i början fortsätter med samma kund fast som underkonsult istället. Se, t.ex.:</p>\n<ul>\n<li><a href=\"http://www.assistera.se/sv\">Assistera</a></li>\n<li><a href=\"https://www.brainville.com/\">Brainville</a></li>\n<li><a href=\"https://www.eworkgroup.com/se/\">EWork</a></li>\n<li><a href=\"https://cv.experis.se/\">Experis</a></li>\n<li><a href=\"http://www.kvadrat.se/bli-kvadratare/\">Kvadrat</a></li>\n<li><a href=\"https://www.toptal.com/\">Toptal</a></li>\n</ul>\n<p>Och till sist, du kan eventuellt göra <strong><a href=\"http://www.kompred.se/nyttiga-tips/missa-inte-investeraravdraget/\">investeraravdraget</a></strong> i din privata deklaration efter att du startat aktiebolag.</p>\n<h2 id=\"bank\">Bank</h2>\n<p>Jag frågade först några banker via telefon och mail (ICA, Avanza, Swedbank). Det kändes som att de inte riktigt förstod vad jag menade när jag sa att jag ville ha ett <strong>bankintyg</strong>.</p>\n<h3 id=\"danskebank\">Danskebank</h3>\n<p><strong>Jag valde Danskebank</strong>. Du fyller bara i <a href=\"https://danskebank.se/sv-se/Foretag/Smaforetag/Pages/Product-packages.aspx\">deras formulär</a> så ringer de upp. Danskebank sköter allt per telefon och det fungerar mycket bra. De bad inte om någon affärsplan eller budget utan litade på mig direkt. De visste också direkt vad det handlade om och de kändes pålitliga. Eftersom jag inte var kund där sedan tidigare så behövde de skicka lite papper till mig med posten först. Men inom en vecka hade jag intyget! Snabbt, enkelt, smidigt och inget meningslöst krångel.</p>\n<p>Jag valde <em>Danske Business Plus</em> för 99 kr/mån. <strong><a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> stödjer även Danskebank</strong> på så vis att man kan klistra in kontoutdraget från banken in i <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a>, för att automatiskt skapa bankhändelser. Sedan matchar man dem mot leverantörsfakturor eller verifikationer.</p>\n<h1 id=\"hurdrivaeget\">Hur Driva Eget?</h1>\n<p>Jag har lagt in återkommande händelser i Google Calendar för de olika datumen. Sedan angivit att de ska upprepas månadsvis eller, i vissa fall, årsvis. Jag lägger ungefär 2 timmar varje månad på att sköta företaget. Vad jag gör är:</p>\n<ul>\n<li>En gång per månaden\n<ul>\n<li>Importerar transaktioner från skattekontot från Skatteverket.</li>\n<li>Importerar transaktioner från företagskontot från banken.</li>\n<li>Skickar fakturor för månaden.</li>\n<li>Betalar ut lön, till anställde.</li>\n<li>Betalar skatt och avgifter till Skatteverket\n<ul>\n<li>Arbetsgivaravgift.</li>\n<li>Inkomstskatt.</li>\n<li>Debiterad preliminärskatt.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>En gång per kvartal.\n<ul>\n<li>Betalar in kvartalsmoms.</li>\n</ul>\n</li>\n<li>En gång per år.\n<ul>\n<li>Bokslut, årsredovisning, deklaration.</li>\n<li>I januari skickar in kontrolluppgit\n<ul>\n<li>Av utbetald lön för föregående år till Skatteverket.</li>\n<li>Av föregående års utdelning (KU31).</li>\n</ul>\n</li>\n<li>Lämnar K10 i privata deklarationen, om utdelning mottagits under året som deklareras.</li>\n</ul>\n</li>\n</ul>\n<p>Jag valde <strong>kvartalsmoms</strong>, alltså att jag efter varje kvartal redovisar och betalar in eventuell moms. Mer information om det finns <a href=\"https://www.verksamt.se/driva/skatter-och-avgifter/moms-redovisa-och-betala\">här</a>. Så fort kvartalet är slut kan man lämna in deklarationen. Man har ungefär 1.5 månad på sig att lämna in den. Även för detta har Visma gjort en film som visar <a href=\"https://www.youtube.com/watch?v=gPv_SahMnEw\">hur du gör i eEkonomi</a>. Som jag förstår det använder man kontot <em>1630</em> (eller <em>2012</em> om det är enskild firma) just för att <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/maste-man-anvanda-sig-av-skattekontot-1630\">alla händelser mot Skatteverket ska gå via detta konto</a>. Hur det fungerar med att betala, och få tillbaka moms, <a href=\"https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html\">beskriver Skatteverkets här</a>. Så kortfattat om du har <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> och kvartalsmoms så ska du vid varje kvartalsslut:</p>\n<ul>\n<li>Skapa en momsredovisning för kvartalet via bokföring / momsredovisning.</li>\n<li>Ladda ner <em>eskd</em>-filen.</li>\n<li>Skapa en <a href=\"https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms.4.7459477810df5bccdd480006935.html\">momsdeklaration</a> enligt <a href=\"https://www.youtube.com/watch?v=gPv_SahMnEw\">Visma:s video</a>.</li>\n<li>Ladda upp <em>eskd</em>-filen hos <a href=\"https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html\">Skatteverket</a>.</li>\n<li>Ladda ner <a href=\"https://www.youtube.com/watch?v=-7cDnn-NaME\">transaktioner från Skatteverket till ditt skattekonto</a> och matchar dem sedan mot momshändelsen i dina bokföringshändelser.</li>\n<li>När du senare betalat in, eller fått betalt, skapa och bokför bankhändelsen på företagskontot. Det visas i slutet på <a href=\"https://www.youtube.com/watch?v=gPv_SahMnEw\">den här videon</a>. Tänk på att använda <em>1630</em> för AB och <em>2012</em> för enskild firma.</li>\n</ul>\n<p>Skatteverket har skickat brev till dig med datum då du ska betala <strong>debiterad preliminärskatt</strong>. Det här är något du betalar varje månad och baserar sig på din förväntade vinst som du angav i din preliminära inkomstdeklaration. Du har fått ett besked från Skatteverket där det står datum och hur mycket du ska betala. Om du har <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> kan du följa deras <a href=\"https://www.youtube.com/watch?v=-7cDnn-NaME\">film om skattekontot</a>.</p>\n<ul>\n<li>Betala från företagskontot till företagets skattekonto.</li>\n<li>Importerar bankhändelser från företagskontot.</li>\n<li>Ladda ner <a href=\"https://www.youtube.com/watch?v=-7cDnn-NaME\">transaktioner från Skatteverket till ditt skattekonto</a> och matchar dem sedan mot momshändelsen i dina bokföringshändelser.</li>\n<li>Importera bankhändelsen från banken till företagskontot i <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a></li>\n<li>Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a></li>\n<li>Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.</li>\n<li>När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.</li>\n<li>Matcha händelsen mot bokföringsförslaget <em>Debiterad preliminärskatt</em>.</li>\n</ul>\n<p>När du ska betala ut <strong>Lön</strong> behöver du betala <em>skatt</em>, <em>arbetsgivaravgift</em> samt själva <em>lönen</em>. Både <em>skatt</em> och <em>arbetsgivaravgift</em> betalas till företagets <em>skattekonto</em> så det är 2 betalningar som behöver göras från företagskontot. Om du använder <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> kan du göra såhär. Finns även en <a href=\"https://www.youtube.com/watch?v=ig2C9gQg2Eg\">film här</a> och en <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/bokfora-lon-i-eekonomi\">bra tråd</a> om detta i supportforumet.</p>\n<ul>\n<li>Under <em>lön</em> i huvudnenyn väljer du <em>anställda</em>. Lägg upp den anställde. Du kommer behöva <em>Skattetabell</em> att använda. Den anställde kan <a href=\"https://www.skatteverket.se/\">logga in</a> hos Skatteverket och hittar då skattetabell under &quot;Skatter och deklarationer&quot;.</li>\n<li>Under <em>lön</em> i huvudnenyn väljer du <em>lönebesked</em> och skapar ett nytt.</li>\n<li>Ange lönen och välj bokför.</li>\n<li>Logga in på banken och betala ut lönen, efter skatt, till den anställdes konto.</li>\n<li>Importera bankhändelsen till <em>företagskontot</em> via <em>kassa och bankhändelser</em>.</li>\n<li><a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> kommer föreslå att du matchar bankhändelsen mot löneutbetalningen, gör det. Nu är du klar med lönen, och nästa moment är att betala skatt och arbetsgivaravgift.</li>\n</ul>\n<p><strong>Arbetsgivardeklaration</strong> lämnas enkelt via <a href=\"https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html\">Skatteverkets webbsida</a>.</p>\n<ul>\n<li>Om du inte har betalat ut någon lön sätter du bara <strong>0</strong> i de obligatoriska fälten och skickar in.</li>\n<li>Om du har betalat ut lön, och använder <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a>.\n<ul>\n<li>Gå in under <em>lön</em> i huvudmenyn och välj <em>arbetsgivardeklaration</em>.</li>\n<li>Skapa ny.</li>\n<li>Välj månaden då lönen betaldes ut.</li>\n<li>Välj <em>bokför</em>.</li>\n<li>Nu kan du <a href=\"https://www.youtube.com/watch?v=ig2C9gQg2Eg\">exportera en fil</a> om du klickar på <em>åtgärder</em> för deklarationen.</li>\n<li><a href=\"https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html\">Lämna in arbetsgivardeklarationen</a> hos Skatteverket genom att importera filen.</li>\n<li>På kvittensen visas ett bankgiro och OCR att använda för att betala in skatten till företagets skattekonto.</li>\n<li>Betala in summan från företagets bankkonto.</li>\n<li>Importera bankhändelsen från banken till företagskontot i <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a></li>\n<li>Importera bankhändelsen från Skatteverket (du kan exportera den till fil) till skattekontot i <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a></li>\n<li>Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen kontering mellan egna bankkonton.</li>\n<li>När pengarna dragits av Skatteverket från Skattekontot, importera den händelsen.</li>\n<li>Matcha händelsen mot bokföringsförslaget arbetsgivardeklaration. Skatteverkat drar arbetsgivardeklartionen i 2 transaktioner, du behöver <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/hur-matchar-jag-tva-belopp-pa-skattekontot-mot-en-arbetsgivardeklaration\">manuellt slå ihop dem</a> för att kunna använda det här bokföringsföslaget. Men annars är det <em>1630</em> som krediteras och <em>2710</em> (personalskatt) samt <em>2731</em> (arbetsgivaravgift) som debiteras. Jag tycker även <a href=\"https://www.blinfo.se/foretagskunskap/bokfora-lon__15472\">den här sidan</a> är bra här.</li>\n</ul>\n</li>\n</ul>\n<p><a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> har en färdig mall för <strong>fakturor</strong>. Du anger ditt företags bankgiro, och/eller kontonummer, lägger upp kunder och artiklar. Sedan kan du enkelt skapa fakturor som du sedan kan skriva ut, spara som PDF eller maila.</p>\n<p>Om du <strong>betalar ut traktamente</strong> behöver du kunna bevisa att <a href=\"https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm\">resorna ägt rum och varit tjänsteresor</a>. Därför kan det vara klokt att föra anteckningar för resor. I mitt fall åker jag alltid tåg och kommer hänvisa till tågbiljetterna om jag får revision av Skatteverket. Jag skapar exceldokument, med uträkningen, som jag sedan exporterar till PDF och sparar för att motivera traktamentet.</p>\n<p><img src=\"/content/images/2017/11/traktamente-utr.png\" alt=\"Traktamente\"></p>\n<p>Man hör ibland uttrycket <strong>skriva av</strong> och jag tror många missförstår det, kanske även jag =) Men jag tolkar det som att allt man köper för under ett halvt prisbasbelopp (ungefär 22 000 kr) skrivs av direkt, kostar det mer skriver man av det under flera år. Att <em>skriva av</em> innebär att man inte tar upp det som en tillgång, en inventarie, i bokföringen. Det är dock inte samma sak som att man får ge bort prylen till sig själv eller någon annan. Men jag är osäker, <a href=\"https://www.flashback.org/p49423662\">och många med mig</a>.</p>\n<p>Att bokföra köp gjorda med <strong>kreditkort</strong> är lite speciellt men det har Visma en väldigt <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi\">bra artikel</a> om.</p>\n<p>Jag sparar alltid alla <strong>kvitton</strong> direkt på Google Drive. Är det inte digitalt fotar jag av med mobilen, laddar upp bilden och sparar även original-kvittot i en pärm. I bokföringsprogrammet är jag också noga med att <strong>ladda upp bilder på varje verifikation</strong>. <a href=\"https://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/bokforingslag-19991078_sfs-1999-1078\">Bokföringslagen</a> säger att originalen ska sparas, så digitala ska sparas digitalt och de på papper sparas på papper. Jag tycker även att <a href=\"https://vismaspcs.se/ditt-foretagande/byra/de-har-kopiorna-behover-du-inte-skriva-ut\">den här artikeln</a> beskriver det bra. Även andra dokument fotar jag av och sparar på Google Drive, som beslut från Skatteverket t.ex. Väldigt smidigt att kunna läsa allt var man än är.  Det är ganska enkelt att få tag på ett rejält brandskyddat kassaskåp (<a href=\"http://www.clasohlson.com/se/Dokumentsk%C3%A5p-Chubbsafes-Executive-40/Pr342428000\">#1</a> eller <a href=\"http://www.biltema.se/sv/Hem/Sakerhet-och-larm/Sakerhetsbox/Kassaskap-brandsakert-2000033385/\">#2</a>) så att man kan spara papper korrekt enligt lagstiftningen.</p>\n<p>På <strong>bokslutsdagen, sista dagen på räkenskapsåret, måste <a href=\"https://www.verksamt.se/driva/ekonomi-och-bokforing/lopande-bokforing\">obetalda fakturor tas upp i bokföring</a></strong> även om du använder kontantmetoden. Det beskriver också Visma <a href=\"https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi\">här</a>. Om du har skapat en faktura som ännu inte blivit betald kommer <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> att ge dig följande meddelande när du skapar sista <strong>kvartalsmomsen</strong>.</p>\n<blockquote>\n<p>Enligt Kontantmetoden ska du bokföra dina obetalda kundfordringar och leverantörsskulder vid räkenskapsårets slut och redovisa momsen. Detta kommer nu att göras automatiskt. Du bör kontrollera att alla inbetalningar och utbetalningar på året är avstämda innan du fortsätter. Vill du fortsätta?</p>\n</blockquote>\n<p>Det är viktigt att ta en extra titt på de sista transaktionerna innan räkenskapsåret är slut så att det inte dykt upp något i sista sekunden. I mitt fall hade Danskebank skapat en faktura, och dragit pengarna, för bankavgifter sista dagen i december.</p>\n<p>När räkenskapsåret är slut ska <strong>kontrolluppgifter</strong> för anställda skickas till Skatteverket. Det här gäller 2017 men är på gång att ändras så att man skickar samma uppgifter i arbetsgivardeklarationen istället. Men i <a href=\"https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282\" rel=\"nofollow\">Visma eEkonomi</a> finns en funktion under <em>lön</em> i huvudmenyn som kan användas om du skapat lönespecar.</p>\n<ul>\n<li>Klicka på <em>Ny kontrolluppgift</em>.</li>\n<li>Fyll i dina uppgifter.</li>\n<li>Spara.</li>\n<li>Nu kan man ladda ner en <em>xml-fil</em> som kan laddas upp hos Skatteverket för att fylla i <em>KU10</em>-blanketten.</li>\n</ul>\n<p>Visma har en väldigt bra <a href=\"http://download.vismaspcs.se/pdf/bokf%C3%B6ring-f%C3%B6r-nyb%C3%B6rjare.pdf?utm_source=Eloqua&amp;utm_medium=email&amp;utm_content=SE_SP_SI_Onboarding-eEko-Std-4&amp;utm_campaign=SE_SP_SI_Onboarding-eA-Std-1704\">gratis broschyr om bokföring</a>.</p>\n<h1 id=\"vrigt\">Övrigt</h1>\n<p>Om du <strong>veckopendlar</strong> som jag så kanske du har en <strong>lägenhet på arbetsorten</strong>. Då kan man tänka sig att man <a href=\"http://www.skatter.se/?q=node/2619\">låter bolaget betala den</a>. Jag blev rekommenderad att ta den privat och betala ut mer lön från bolaget istället. Då räknar jag på detta vid utbetalning av lön så att lön efter avdrag hamnar på gränsen för statlig skatt. Avdraget gör jag alltså i min privata deklaration. Väljer man att låta bolaget direkt betala lägenheten blir det lite krångligare med bokföring och deklaration. Det kan också vara så att Skatteverket ser det som en förmån. Så det här kändes enklast.</p>\n<p>Det finns bolag som har som <strong>affärsidé att bara ha underkonsulter</strong>. De åker på konferensresor och har gemensam kontorslokal precis som vilket annat bolag som helst. Skillnaden är bara att alla är underkonsulter. Detta bolag hjälper dig att starta ditt eget bolag och hitta kunder. De har ramavtal med stora konsultköpare och kan ordna bra timpriser. Ett sånt bolag är <a href=\"http://www.kvadrat.se/\">Kvadrat</a>.</p>\n<p>Jag valde först att ta över kontraktet jag var på och då gå från anställd konsult till underkonsult. Senare bytte jag uppdrag och då anslöt jag mig till <a href=\"http://www.kvadrat.se/\">Kvadrat</a>.</p>\n<p>Har du frågor om skatter så är det smidigt att man kan <a href=\"https://www.skatteverket.se/omoss/kontaktaoss/mejla/\">maila Skatteverket</a>.</p>\n<p>Jag har skapat en facebook-grupp för att diskutera det här, går gärna med! <a href=\"https://www.facebook.com/groups/791240028009906/\">https://www.facebook.com/groups/791240028009906/</a></p>\n','31','Efter över 7 år som anställd konsult (mjukvaruutvecklare) valde jag nyligen att\nstarta eget aktiebolag. Jag gör precis samma sak nu som jag gjorde som anställd\nmen numera i mitt eget bolag istället. Det här inlägget handlar om hur jag\ngjorde detta. Googlar man kring att starta eget så hittar man mycket. Men jag\nsaknade någon som beskriver det utifrån mina förutsättningar, det är vad jag\nförsöker göra här, eftersom jag tror att många har samma förutsättningar som\nmig. Oh, and sorry if you don\'t speak Swedish =) The topic is about starting\nyour own company in Sweden, you are probably not interested.\n\nSka också säga att allt jag tagit reda på här är genom googling och jag tar\ninget ansvar för att det jag säger här är korrekt och riktigt! Så var kritisk\nnär du läser vad jag skriver nedan!\n\nJag blir ofta kontaktad angående det här blogginlägget, men jag vill gärna hålla\ndiskussionerna öppet här istället: \nhttps://www.facebook.com/groups/791240028009906/\n\nVarför Starta Eget?\nDu kan skatteplanera. Om du redan innan året är slut vet gräns för statlig skatt\n[http://www.regeringen.se/pressmeddelanden/2017/03/upprakningen-av-skiktgranserna-for-statlig-inkomstskatt-2018-begransas-och-den-sarskilda-inkomstskatten-for-utomlands-bosatta-hojs/] \n och eventuella avdrag du kommer att göra. Då kan du räkna ut exakt vilken lön\ndu kan betala ut från bolaget för att du som privatperson ska hamna precis på\ngränsen för statlig skatt.\n\nUtöver lönen kan du också göra en utdelning. Då skattar du 20% på summan upp\ntill 2.75  gånger föregående års inkomstbasbeloppet  för 2018 blir det 2.75 *\n59300 = 163075. Det är det som kallas förenklingsregeln. Man kan också räkna\nfram en gräns, som är hälften av utbetald lön, och använda den om den är mer\nfördelaktig. Den här blog-posten\n[https://www.foretagande.se/sa-beraknas-skatt-pa-utdelning-3-12-reglerna/] \nbeskriver detta bra. Första året kan du eventuellt inte använda\nförenklingsregeln, de diskuterar det i den här tråden\n[https://forum.vismaspcs.se/visma_spcs/topics/utdelning-forsta-aret].\n\nJag har gjort ett excel-ark\n[https://docs.google.com/spreadsheets/d/1jfQSSOEINNq3OVc5pYG5NeFkvgcDqOEu1HjzpU2RQm0/edit?usp=sharing] \n för att optimera skatten. Skatteverket har även en sida\n[https://www.skatteverket.se/privat/sjalvservice/allaetjanster/tjanster/raknautdinskatt.4.18e1b10334ebe8bc80001950.html] \n där man kan räkna ut sin skatt. Du kan även läsa mer om skatter och avgifter\npå\nVerksamt [https://www.verksamt.se/starta/skatter-och-avgifter/aktiebolag]. Se\näven den här sidan [http://www.driva-eget.se/kalkyler/lonekalkyl]  för att få en\nuppfattning om hur mycket lön man kan ta ut.\n\nDet finns anledning att vara rädd för det faktum att man inte får några pengar\nalls om man inte har ett uppdrag. Men eget bolag kan du använda en \nperiodiseringsfond\n[https://www.skatteverket.se/foretagochorganisationer/foretagare/enskildnaringsverksamhet/periodiseringsfond.4.361dc8c15312eff6fd2b8f2.html] \n där du lägger undan pengar mellan räkenskapsår. Du kan alltså optimera din lön\nför att slippa statlig skatt, lägga undan pengar i en periodiseringsfond och\nsedan betala ut dem ett år då du har mindre inkomster. På så vis slipper du både\nbolagsskatten och statlig inkomstskatt.\n\nDu är säkrare som egen. Bolaget kommer förmodligen att dra in mer pengar än vad\nsom är lämpligt att direkt batala ut som lön. Får du 500kr/timme (ett väldigt\nlågt pris!) så drar du in cirka 80 000kr under en månad. Då kan du betala 60 000\nkr i lön. Men du vill förmodligen bara betala ut 36 575kr eftersom det är\ngränsen för statlig skatt. Att betala den lönen kostar cirka 48 000kr för\nbolaget och du har alltså 32 000kr kvar i bolaget. Gör du det varje månad har du\n 384 000kr kvar i bolaget när räkenskapsåret är slut. Du vill förmodligen\nanvända en del för att göra en utdelning. Men för att jämföra med att vara\nanställd så skulle du kunna betala ut samma lön 6  månader in på nästa år, trots\natt du står helt utan inkomster! För att räkna ut hur mycket lön du kan betala\nut kan du alltså räkna såhär timpenning*(52-semesterveckor)*40/12/1.31  så\nkanske 500*(52-6)*40/12/1.31 = 58 524.\n\nDu väljer själv om du vill betala in till en tjänstepension  och i så fall\nvilken. Jag har tvingats betala till SEB under en längre tid. Det enda jag bryr\nmig om är deras fondutbud. I SEB hittade jag bara en fond, en räntefond\n[http://web.msse.se/SEB/SGF/quicktake.aspx?six=SEB-BDSEKA], som kändes \"ok\". Det\när även en fördel att du kan maximera lönen, göra utdelning och om det efter\ndetta fortfarande finns pengar över kan du använda dem till tjänstepension. Du\nslipper alltså låsa in pengar i en sådan pension om det inte är så att det är en\nskattemässig fördel. Med tanke på att du, via utdelning, kan få ut kapital med\nen skatt på under 40% så kanske du inte vill använda en tjänstepension. Pengarna\ndu får ut via utdelningen är ju helt olåsta och du kan investera hur du vill.\n\nDu slipper ha lön efter ålder. Ju yngre du är, ju mer tjänar du på att ha eget\nkonsultbolag. Den dumma tanken att man ska börja på en låg lönenivå och sedan\nöka ett par procent per år mappar inte alls mot vad kunderna betalar\n[https://computersweden.idg.se/2.2683/1.717731/java-utvecklare-heta]  för dig\nper timme. Kunderna har oftast ett pris på juniorer och ett på seniorer. Att du\när 30, 31 eller 40 år gammal spelar ingen roll för priset. Är du yngre anställd\nkonsult betalar du för dina äldre kollegors lön och förmåner. Det kan mycket väl\nvara så att du drar in mer pengar än någon som är 20 år äldre. Med eget bolag\nmer än dubblade jag min lön.\n\nDu kan köpa kontorsmaterial väldigt billigt. Dator, headset, hörlurar,\nskärmar... till mindre än halva priset jämfört med om du köper det privat. Då\ntänker jag inte bara på momsen. Tänk även på hur mycket pengar bolaget behöver\nbetala ut för att, efter skatt och arbetsgivaravgift, kunna sätta in summan på\ndet privata kontot. Såklart måste man tänka på att det man köper ägs av\nföretaget och inte dig privat.\n\nSå fort du har möjlighet att betala ut skattefritt traktamente\n[https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm] \n så ska du göra det. Du slipper tjafs med en arbetsgivare som nekar dig något\nsom är så självklart.\n\nDu kan åka på vilken konferens du vill  och låta bolaget betala. Det finns ju\nintressanta utvecklarkonferenser nästan överallt. Du behöver bara hitta en\n[http://lmgtfy.com/?q=sidney+developer+conference]  och åka. Inget tjafs med en\narbetsgivare som tycker det är för dyrt. Inga dryga övningar, innan eller på\nplats, utan bara du som bestämmer vilka föreläsningar du tycker är intressanta\noch som du går på.\n\nJag tycker att de flesta möten är meningslösa och väldigt tråkiga. Det blir\nklart färre möten  med eget bolag. Du kan spendera din tid med kunden istället\nför tråkiga månadsmöten =)\n\nDu kan få mindre administrativt jobb. Som anställd hade jag kvitton att\nrapportera varje vecka. Jag köpte tågbiljetter via SJ:s hemsida och fick alltså\nkvittot som PDF. Jag var tvungen att rapporera dem varje vecka med kostnad och\nmoms. Skriva ut ett papper i samband med tidrapportering. Skriva ut mina\nkvitton. Häffta fast mina kvitton på pappret från tidrapporteringssystemet. Leta\nupp kuvert samt frimärke. Leta upp brevlåda och posta kuvert med tidrapporten\ntill konsultbolagets kontor. Som egen lägger jag aldrig mer än 2 timmar totalt\nunder en hel månad på att sköta den fullständiga bokföring inklusive alla\nkvitton. Allt gör jag digitalt.\n\nTill sist, och kanske det absolut bästa argumentet. Jag har alltid sett mig\nsjälv som mitt eget varumärke. Då är ju inte steget långt till att faktiskt vara\ndet också.\n\nHur Starta Eget?\nJag listar här kortfattat vad man behöver göra. Längre ner finns vissa steg mer\nutförligt beskrivna. Du kan räkna med att det tar 1-2 månader från att du\nskickar in första ansökan till att allt runt bolaget är klart och du kan börja\njobba i det. Skadar inte att titta på andra listor också\n[https://www.verksamt.se/alla-e-tjanster/din-checklista/aktiebolag]  för att\nsäkra att du inte glömt något.\n\nJag valde aktiebolag  framför enskild firma. Då är bolaget en juridisk person\nsom också har ansvaret. En tydlig gräns mellan bolag och privat ekonomi alltså.\nJag kan rekommendera Verksamts jämförelse av företagsformer\n[https://www.verksamt.se/fundera/valj-foretagsform].\n\nSkapa ansökan  på Verksamt [https://www.verksamt.se/]. Du kommer behöva en\nverksamhetsbeskrivning. Min ser ut såhär:\n\nFöretaget ska bedriva konsultverksamhet företrädesvis inom IT, samt utveckla\nmjukvaruprodukter, äga och förvalta värdepapper och utöva därmed förenlig\nverksamhet.\n\nNär det gäller företagsnamn  så rekommenderar jag att bara ta ditt eget namn. AB\neller aktiebolag måste vara med i namnet. Så jag valde Tomas Bjerre AB. Risken\nom du väljer något annat är att Bolagsverket nekar dig för att namnet liknar\nnågot som redan finns. Onödigt strul och du kan alltid ändra senare.\n\nDu kommer behöva ange en SNI-kod. Jag valde 62010.\n\nNär du skickat iväg ansökan till Bolagsverket kommer de invänta ett bankintyg.\nLäs mer om att välja bank nedan. Du behöver kontakta en bank för att skapa ett\nföretagskonto. Banken kommer be dig sätta in 50 000 kr på ett speciellt konto.\nBanken behöver kunna tala om för Bolagsverket att här finns det 50 000 kr som\nsatts in med avsikt att användas för aktiekapitalet. När du gjort detta ger de\ndig en e-postadress som du sedan ger till Bolagsverket (via verksamt.se). Sedan\nfrågan Bolagsverket banken om pengarna är där och när banken svarar ja\nregistreras bolaget. Så fort bolaget är skapat så har du sedan ett företagskonto\ndär det från start finns 50 000 kr.\n\nJag valde räkenskapsåret  som 0101 - 1231. Jag kan dock rekommendera brutet\nräkenskapsår. Alltså att man väljer ett år som inte är ett kalenderår. Fördelen\nblir då att du kan välja vilket år du betalar ut lönen på, om du når statlig\nskatt 2017 kan du vänta till 2018 med att fortsätta betala ut lön. Det är inte\nhelt enkelt att ändra i efterhand\n[https://www.skatteverket.se/download/18.3aa8c78a1466c584587d059/1408515815569/42405.pdf] \n och att ändra det bara för att undvika skatt är inte en godtagbar anledning.\n\nNär du startar aktiebolag ska bolaget ha F-skatt  och du som privatperson har\nA-skatt. Jag valde kvartalsmoms. Som redovisningsmetod  valde jag kontantmetoden\n[https://www.e-conomic.se/bokforingsprogram/ordlista/kontantmetoden]  (kallas\näven bokslutsmetoden). Jag uppfattade den som enklast, se hur man bokför köpt\nmed kreditkort\n[https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi] \n t.ex.\n\nDet är väldigt svårt att uppskatta den förväntade vinsten  första året. Andra\nåret kommer Skatteverket föreslå en. Ett tips är att ange den lågt. I mitt fall\nskulle jag först betala in 20 000 kr innan jag hunnit få en enda inkomst i\nbolaget. Skulle du råka ut för det är det bara att göra en ny preliminär\ninkomstdeklaration via Skatteverket och ange en lägre förväntad vinst. Då fick\njag ner denna siffran till 7 000 kr istället.\n\nDet finns många smidiga bokföringsprogram. Redovisningen\n[https://sv.wikipedia.org/wiki/Redovisning]  var nog det jag var mest rädd för i\nbörjan. Jag märkte tidigt att när jag googlade om hur man bokför olika saker så\nkom jag ofta in på Visma:s support forum [https://forum.vismaspcs.se/visma_spcs]\n. Jag har tidigare använt deras tidrapportering, PX, vilket suger något helt\notroligt. Ser ut som en sommarjobbare hackade ihop det för 15 år sedan och att\nman inte rört det sedan dess. Men även om jag var väldigt skeptisk till detta\nbolag så valde jag ändå Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \nför bokföring just för att deras support verkar vara helt exemplarisk. Att\ndirekt bokföra ett kvitto i Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \när betydligt enklare än det jag tidigare gjort via tidrapportering som anställd.\nDu kanske t.ex. vill bokföra bokföringsprogrammet\n[https://forum.vismaspcs.se/visma_spcs/topics/8fxz374h0czze?topic-reply-list%5Bsettings%5D%5Bfilter_by%5D=all]\n, köp av dator och försäkring\n[https://forum.vismaspcs.se/visma_spcs/topics/7h3lv4csqal8w]  eller köp av\ntågbiljett [https://forum.vismaspcs.se/visma_spcs/topics/con2lpq5fex2f]. De gör\nt.o.m. filmer [https://www.youtube.com/watch?v=jGgnxd6uBh4]  där de visar vissa\nflöden. Hoppas jag är trovärdig trots att jag gått med i deras affiliate-program\noch får betalt för att hänvisa andra dit =)\n\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282]\n\nDu lär behöva en mobil  i bolaget. Som jag förstår det är det ok att köpa\ntelefonen på bolaget. Även telefoni och SMS är ok. Datatrafiken däremot kan\nbehöva förmånsbeskattas. Det i kombination med att man slipper många dryga\nsamtal om man istället har ett privat abonnemang, gjorde att jag köpte den\nprivat.\n\nDu lär behöva en ansvarsförsäkring. Jag valde en från If\n[https://www.if.se/web/se/foretag/varaforsakringar/offert/pages/default.aspx?varde=startforsakring] \n och fick betala 5 294 kr. Har du inte det och du råkar ut för något, säg att\nnågon kräver dig på flera miljoner, så konkursar du bara bolaget och slipper\nundan. Men med en ansvarsförsäkring så kan du, kanske, göra rätt för dig och\nbetala. På så vis blir du en mer attraktiv konsult att anlita och det står även\nofta i kontrakten att du ska ha en försäkring.\n\nJag valde att kontakta en redovisningskonsult  som hjälper mig med ekonomiska\nfrågor samt tar hand om bokslut  / årsredovisning  / deklaration. Jag tar hand\nom löpande bokföring samt kvartalsmoms, skatter och lön. Det kostar cirka 6 000\nkr per år. En redovisningskonsult kan hjälpa dig med allt möjligt i bolaget.\nVissa ger dem fullmakt så de kan sköta all ekonomi. Jag fick tag på honom genom\nen bekant som tipsade men annars är det enkelt att googla. Har du valt Visma så \nkan de hjälpa dig\n[https://vismaspcs.se/produkter/redovisning/visma-eekonomi-byra]. I Visma\neEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \nkan du även skapa en inloggning till din redovisningskonsult.\n\nRevisor  är valfritt om man omsätter under 3 miljoner\n[http://www.bolagsverket.se/ff/foretagsformer/aktiebolag/starta/revisor]. Det\nanvänder jag inte.\n\nDet kan också vara bra att ha klart för sig hur man hittar kunder. Det finns\nmånga rena konsultmäklare som tar en procentsats, ofta mellan 10-20% på din\ntimpenning, om du tar ett uppdrag via dem. De har ofta ramavtal med stora\nkonsultköpare. Är du redan anställd som konsult kanske du i början fortsätter\nmed samma kund fast som underkonsult istället. Se, t.ex.:\n\n * Assistera [http://www.assistera.se/sv]\n * Brainville [https://www.brainville.com/]\n * EWork [https://www.eworkgroup.com/se/]\n * Experis [https://cv.experis.se/]\n * Kvadrat [http://www.kvadrat.se/bli-kvadratare/]\n * Toptal [https://www.toptal.com/]\n\nOch till sist, du kan eventuellt göra investeraravdraget\n[http://www.kompred.se/nyttiga-tips/missa-inte-investeraravdraget/]  i din\nprivata deklaration efter att du startat aktiebolag.\n\nBank\nJag frågade först några banker via telefon och mail (ICA, Avanza, Swedbank). Det\nkändes som att de inte riktigt förstod vad jag menade när jag sa att jag ville\nha ett bankintyg.\n\nDanskebank\nJag valde Danskebank. Du fyller bara i deras formulär\n[https://danskebank.se/sv-se/Foretag/Smaforetag/Pages/Product-packages.aspx]  så\nringer de upp. Danskebank sköter allt per telefon och det fungerar mycket bra.\nDe bad inte om någon affärsplan eller budget utan litade på mig direkt. De\nvisste också direkt vad det handlade om och de kändes pålitliga. Eftersom jag\ninte var kund där sedan tidigare så behövde de skicka lite papper till mig med\nposten först. Men inom en vecka hade jag intyget! Snabbt, enkelt, smidigt och\ninget meningslöst krångel.\n\nJag valde Danske Business Plus  för 99 kr/mån. Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \nstödjer även Danskebank  på så vis att man kan klistra in kontoutdraget från\nbanken in i Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282],\nför att automatiskt skapa bankhändelser. Sedan matchar man dem mot\nleverantörsfakturor eller verifikationer.\n\nHur Driva Eget?\nJag har lagt in återkommande händelser i Google Calendar för de olika datumen.\nSedan angivit att de ska upprepas månadsvis eller, i vissa fall, årsvis. Jag\nlägger ungefär 2 timmar varje månad på att sköta företaget. Vad jag gör är:\n\n * En gång per månaden * Importerar transaktioner från skattekontot från\n      Skatteverket.\n    * Importerar transaktioner från företagskontot från\n      banken.\n    * Skickar fakturor för månaden.\n    * Betalar ut lön, till anställde.\n    * Betalar skatt och avgifter till Skatteverket * Arbetsgivaravgift.\n       * Inkomstskatt.\n       * Debiterad preliminärskatt.\n      \n      \n   \n   \n * En gång per kvartal. * Betalar in kvartalsmoms.\n   \n   \n * En gång per år. * Bokslut, årsredovisning, deklaration.\n    * I januari skickar in kontrolluppgit * Av utbetald lön för föregående år\n         till Skatteverket.\n       * Av föregående års utdelning (KU31).\n      \n      \n    * Lämnar K10 i privata deklarationen, om utdelning mottagits\n      under året som deklareras.\n   \n   \n\nJag valde kvartalsmoms, alltså att jag efter varje kvartal redovisar och betalar\nin eventuell moms. Mer information om det finns här\n[https://www.verksamt.se/driva/skatter-och-avgifter/moms-redovisa-och-betala].\nSå fort kvartalet är slut kan man lämna in deklarationen. Man har ungefär 1.5\nmånad på sig att lämna in den. Även för detta har Visma gjort en film som visar \nhur du gör i eEkonomi [https://www.youtube.com/watch?v=gPv_SahMnEw]. Som jag\nförstår det använder man kontot 1630  (eller 2012  om det är enskild firma) just\nför att alla händelser mot Skatteverket ska gå via detta konto\n[https://forum.vismaspcs.se/visma_spcs/topics/maste-man-anvanda-sig-av-skattekontot-1630]\n. Hur det fungerar med att betala, och få tillbaka moms, beskriver\nSkatteverkets\nhär\n[https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html]\n. Så kortfattat om du har Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \noch kvartalsmoms så ska du vid varje kvartalsslut:\n\n * Skapa en momsredovisning för kvartalet via bokföring / momsredovisning.\n * Ladda ner eskd-filen.\n * Skapa en momsdeklaration\n   [https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms.4.7459477810df5bccdd480006935.html] \n    enligt Visma:s video [https://www.youtube.com/watch?v=gPv_SahMnEw].\n * Ladda upp eskd-filen hos Skatteverket\n   [https://www.skatteverket.se/foretagochorganisationer/moms/deklareraochbetalamoms/betalaochfatillbakamoms.4.58d555751259e4d66168000354.html]\n   .\n * Ladda ner transaktioner från Skatteverket till ditt skattekonto\n   [https://www.youtube.com/watch?v=-7cDnn-NaME]  och matchar dem sedan mot\n   momshändelsen i dina bokföringshändelser.\n * När du senare betalat in, eller fått betalt, skapa och bokför bankhändelsen\n   på företagskontot. Det visas i slutet på den här videon\n   [https://www.youtube.com/watch?v=gPv_SahMnEw]. Tänk på att använda 1630  för\n   AB och 2012  för enskild firma.\n\nSkatteverket har skickat brev till dig med datum då du ska betala debiterad\npreliminärskatt. Det här är något du betalar varje månad och baserar sig på din\nförväntade vinst som du angav i din preliminära inkomstdeklaration. Du har fått\nett besked från Skatteverket där det står datum och hur mycket du ska betala. Om\ndu har Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \nkan du följa deras film om skattekontot\n[https://www.youtube.com/watch?v=-7cDnn-NaME].\n\n * Betala från företagskontot till företagets skattekonto.\n * Importerar bankhändelser från företagskontot.\n * Ladda ner transaktioner från Skatteverket till ditt skattekonto\n   [https://www.youtube.com/watch?v=-7cDnn-NaME]  och matchar dem sedan mot\n   momshändelsen i dina bokföringshändelser.\n * Importera bankhändelsen från banken till företagskontot i Visma eEkonomi\n   [https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282]\n * Importera bankhändelsen från Skatteverket (du kan exportera den till fil)\n   till skattekontot i Visma eEkonomi\n   [https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282]\n * Matcha bankhändelsen på företagskontot mot händelsen på skattekontot. Egen\n   kontering mellan egna bankkonton.\n * När pengarna dragits av Skatteverket från Skattekontot, importera den\n   händelsen.\n * Matcha händelsen mot bokföringsförslaget Debiterad preliminärskatt.\n\nNär du ska betala ut Lön  behöver du betala skatt, arbetsgivaravgift  samt\nsjälva lönen. Både skatt  och arbetsgivaravgift  betalas till företagets \nskattekonto  så det är 2 betalningar som behöver göras från företagskontot. Om\ndu använder Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \nkan du göra såhär. Finns även en film här\n[https://www.youtube.com/watch?v=ig2C9gQg2Eg]  och en bra tråd\n[https://forum.vismaspcs.se/visma_spcs/topics/bokfora-lon-i-eekonomi]  om detta\ni supportforumet.\n\n * Under lön  i huvudnenyn väljer du anställda. Lägg upp den anställde. Du\n   kommer behöva Skattetabell  att använda. Den anställde kan logga in\n   [https://www.skatteverket.se/]  hos Skatteverket och hittar då skattetabell\n   under \"Skatter och deklarationer\".\n * Under lön  i huvudnenyn väljer du lönebesked  och skapar ett nytt.\n * Ange lönen och välj bokför.\n * Logga in på banken och betala ut lönen, efter skatt, till den anställdes\n   konto.\n * Importera bankhändelsen till företagskontot  via kassa och bankhändelser.\n * Visma eEkonomi\n   [https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \n    kommer föreslå att du matchar bankhändelsen mot löneutbetalningen, gör det.\n   Nu är du klar med lönen, och nästa moment är att betala skatt och\n   arbetsgivaravgift.\n\nArbetsgivardeklaration  lämnas enkelt via Skatteverkets webbsida\n[https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html]\n.\n\n * Om du inte har betalat ut någon lön sätter du bara 0  i de obligatoriska\n   fälten och skickar in.\n * Om du har betalat ut lön, och använder Visma eEkonomi\n   [https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282]\n   . * Gå in under lön  i huvudmenyn och välj arbetsgivardeklaration.\n    * Skapa ny.\n    * Välj månaden då lönen betaldes ut.\n    * Välj bokför.\n    * Nu kan du exportera en fil [https://www.youtube.com/watch?v=ig2C9gQg2Eg] \n      om du klickar på åtgärder  för deklarationen.\n    * Lämna in arbetsgivardeklarationen\n      [https://www.skatteverket.se/foretagochorganisationer/arbetsgivare/lamnaarbetsgivardeklaration.4.6a6688231259309ff1f800029116.html] \n       hos Skatteverket genom att importera filen.\n    * På kvittensen visas ett bankgiro och OCR att använda för att betala in\n      skatten till företagets skattekonto.\n    * Betala in summan från företagets bankkonto.\n    * Importera bankhändelsen från banken till företagskontot i Visma eEkonomi\n      [https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282]\n    * Importera bankhändelsen från Skatteverket (du kan exportera den till fil)\n      till skattekontot i Visma eEkonomi\n      [https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282]\n    * Matcha bankhändelsen på företagskontot mot händelsen på skattekontot.\n      Egen kontering mellan egna bankkonton.\n    * När pengarna dragits av Skatteverket från Skattekontot, importera den\n      händelsen.\n    * Matcha händelsen mot bokföringsförslaget arbetsgivardeklaration.\n      Skatteverkat drar arbetsgivardeklartionen i 2 transaktioner, du behöver \n      manuellt slå ihop dem\n      [https://forum.vismaspcs.se/visma_spcs/topics/hur-matchar-jag-tva-belopp-pa-skattekontot-mot-en-arbetsgivardeklaration] \n       för att kunna använda det här bokföringsföslaget. Men annars är det 1630 \n      som krediteras och 2710  (personalskatt) samt 2731  (arbetsgivaravgift)\n      som debiteras. Jag tycker även den här sidan\n      [https://www.blinfo.se/foretagskunskap/bokfora-lon__15472]  är bra här.\n   \n   \n\nVisma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \nhar en färdig mall för fakturor. Du anger ditt företags bankgiro, och/eller\nkontonummer, lägger upp kunder och artiklar. Sedan kan du enkelt skapa fakturor\nsom du sedan kan skriva ut, spara som PDF eller maila.\n\nOm du betalar ut traktamente  behöver du kunna bevisa att resorna ägt rum och\nvarit tjänsteresor\n[https://www.vismaspcs.se/visma-support/visma-eget-aktiebolag/content/skattefria_traktamenten.htm]\n. Därför kan det vara klokt att föra anteckningar för resor. I mitt fall åker\njag alltid tåg och kommer hänvisa till tågbiljetterna om jag får revision av\nSkatteverket. Jag skapar exceldokument, med uträkningen, som jag sedan\nexporterar till PDF och sparar för att motivera traktamentet.\n\n\n\nMan hör ibland uttrycket skriva av  och jag tror många missförstår det, kanske\näven jag =) Men jag tolkar det som att allt man köper för under ett halvt\nprisbasbelopp (ungefär 22 000 kr) skrivs av direkt, kostar det mer skriver man\nav det under flera år. Att skriva av  innebär att man inte tar upp det som en\ntillgång, en inventarie, i bokföringen. Det är dock inte samma sak som att man\nfår ge bort prylen till sig själv eller någon annan. Men jag är osäker, och\nmånga med mig [https://www.flashback.org/p49423662].\n\nAtt bokföra köp gjorda med kreditkort  är lite speciellt men det har Visma en\nväldigt bra artikel\n[https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi] \n om.\n\nJag sparar alltid alla kvitton  direkt på Google Drive. Är det inte digitalt\nfotar jag av med mobilen, laddar upp bilden och sparar även original-kvittot i\nen pärm. I bokföringsprogrammet är jag också noga med att ladda upp bilder på\nvarje verifikation. Bokföringslagen\n[https://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/bokforingslag-19991078_sfs-1999-1078] \n säger att originalen ska sparas, så digitala ska sparas digitalt och de på\npapper sparas på papper. Jag tycker även att den här artikeln\n[https://vismaspcs.se/ditt-foretagande/byra/de-har-kopiorna-behover-du-inte-skriva-ut] \n beskriver det bra. Även andra dokument fotar jag av och sparar på Google Drive,\nsom beslut från Skatteverket t.ex. Väldigt smidigt att kunna läsa allt var man\nän är. Det är ganska enkelt att få tag på ett rejält brandskyddat kassaskåp (#1\n[http://www.clasohlson.com/se/Dokumentsk%C3%A5p-Chubbsafes-Executive-40/Pr342428000] \n eller #2\n[http://www.biltema.se/sv/Hem/Sakerhet-och-larm/Sakerhetsbox/Kassaskap-brandsakert-2000033385/]\n) så att man kan spara papper korrekt enligt lagstiftningen.\n\nPå bokslutsdagen, sista dagen på räkenskapsåret, måste obetalda fakturor tas\nupp\ni bokföring\n[https://www.verksamt.se/driva/ekonomi-och-bokforing/lopande-bokforing]  även om\ndu använder kontantmetoden. Det beskriver också Visma här\n[https://forum.vismaspcs.se/visma_spcs/topics/bokfora-inkop-med-kreditkort-i-visma-eekonomi]\n. Om du har skapat en faktura som ännu inte blivit betald kommer Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \natt ge dig följande meddelande när du skapar sista kvartalsmomsen.\n\nEnligt Kontantmetoden ska du bokföra dina obetalda kundfordringar och\nleverantörsskulder vid räkenskapsårets slut och redovisa momsen. Detta kommer nu\natt göras automatiskt. Du bör kontrollera att alla inbetalningar och\nutbetalningar på året är avstämda innan du fortsätter. Vill du fortsätta?\n\nDet är viktigt att ta en extra titt på de sista transaktionerna innan\nräkenskapsåret är slut så att det inte dykt upp något i sista sekunden. I mitt\nfall hade Danskebank skapat en faktura, och dragit pengarna, för bankavgifter\nsista dagen i december.\n\nNär räkenskapsåret är slut ska kontrolluppgifter  för anställda skickas till\nSkatteverket. Det här gäller 2017 men är på gång att ändras så att man skickar\nsamma uppgifter i arbetsgivardeklarationen istället. Men i Visma eEkonomi\n[https://vismaspcs.se/produkter/bokforingsprogram/visma-eekonomi?pixid=15282] \nfinns en funktion under lön  i huvudmenyn som kan användas om du skapat\nlönespecar.\n\n * Klicka på Ny kontrolluppgift.\n * Fyll i dina uppgifter.\n * Spara.\n * Nu kan man ladda ner en xml-fil  som kan laddas upp hos Skatteverket för att\n   fylla i KU10-blanketten.\n\nVisma har en väldigt bra gratis broschyr om bokföring\n[http://download.vismaspcs.se/pdf/bokf%C3%B6ring-f%C3%B6r-nyb%C3%B6rjare.pdf?utm_source=Eloqua&utm_medium=email&utm_content=SE_SP_SI_Onboarding-eEko-Std-4&utm_campaign=SE_SP_SI_Onboarding-eA-Std-1704]\n.\n\nÖvrigt\nOm du veckopendlar  som jag så kanske du har en lägenhet på arbetsorten. Då kan\nman tänka sig att man låter bolaget betala den\n[http://www.skatter.se/?q=node/2619]. Jag blev rekommenderad att ta den privat\noch betala ut mer lön från bolaget istället. Då räknar jag på detta vid\nutbetalning av lön så att lön efter avdrag hamnar på gränsen för statlig skatt.\nAvdraget gör jag alltså i min privata deklaration. Väljer man att låta bolaget\ndirekt betala lägenheten blir det lite krångligare med bokföring och\ndeklaration. Det kan också vara så att Skatteverket ser det som en förmån. Så\ndet här kändes enklast.\n\nDet finns bolag som har som affärsidé att bara ha underkonsulter. De åker på\nkonferensresor och har gemensam kontorslokal precis som vilket annat bolag som\nhelst. Skillnaden är bara att alla är underkonsulter. Detta bolag hjälper dig\natt starta ditt eget bolag och hitta kunder. De har ramavtal med stora\nkonsultköpare och kan ordna bra timpriser. Ett sånt bolag är Kvadrat\n[http://www.kvadrat.se/].\n\nJag valde först att ta över kontraktet jag var på och då gå från anställd\nkonsult till underkonsult. Senare bytte jag uppdrag och då anslöt jag mig till \nKvadrat [http://www.kvadrat.se/].\n\nHar du frågor om skatter så är det smidigt att man kan maila Skatteverket\n[https://www.skatteverket.se/omoss/kontaktaoss/mejla/].\n\nJag har skapat en facebook-grupp för att diskutera det här, går gärna med! \nhttps://www.facebook.com/groups/791240028009906/','/content/images/2017/12/vid_poolen.png',0,0,'published',NULL,'public',NULL,NULL,'1','2017-06-27 20:01:37','1','2020-01-02 09:46:15','1','2017-06-29 10:42:56','1',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL),('59e067ad8edbbd05ee0cef90','8d1514e6-ed37-441c-ae20-e0c12288a7df','FitNesse with Maven and Jenkins','fitnesse-with-maven-and-jenkins','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I needed to automate [FitNesse](http://www.fitnesse.org/) tests in my clients build process. I also needed the test cases in Git so that *testers* can develop tests in the same feature branches as the *developers* are using.\\n\\nI was looking for a simple way of doing that in a Java project where Maven and Jenkins is being used. I did not find any acceptable solutiion, here is my solution.\\n\\n# What I Found\\nI found [Running from Junit](http://www.fitnesse.org/FitNesse.UserGuide.WritingAcceptanceTests.RunningFromJunit) in the official documentation. But I was unable to use it.\\n* That API looks strange and it is unclear how to use it.\\n* I could not find any API for specifying where the jar files, with the *service under test*, are on the filesystem. When digging into the code on GitHub I concluded that there are no such feature.\\n\\nI found the [Jenkins Plugin](https://wiki.jenkins.io/display/JENKINS/FitNesse+Plugin) but I wanted testers to be able to work with the test cases locally. That plugin would only, potentially, solve it in Jenkins. There is also [Xebia](http://blog.xebia.com/how-to-integrate-fitnesse-tests-into-jenkins/) but same problem there.\\n\\n# What I Did\\nI created a `pom` that uses `antrun` to basically just run the `java` program with command line arguments. The version of FitNesse is managed in the `pom`. \\n\\nThe wiki pages have:\\n```\\n!define TEST_SYSTEM {slim}\\n!path target/dependencies/*.jar\\n```\\n\\nHere is the `pom.xml`.\\n```\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\"\\n xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\\\">\\n <modelVersion>4.0.0</modelVersion>\\n\\n <artifactId>bjurr-fitnesse</artifactId>\\n\\n <dependencies>\\n  <dependency>\\n   <groupId>org.fitnesse</groupId>\\n   <artifactId>fitnesse</artifactId>\\n   <version>20161106</version>\\n   <scope>test</scope>\\n  </dependency>\\n </dependencies>\\n\\n <properties>\\n  <fitnesse.command></fitnesse.command>\\n </properties>\\n\\n <build>\\n  <plugins>\\n   <plugin>\\n    <artifactId>maven-antrun-plugin</artifactId>\\n    <version>1.3</version>\\n    <executions>\\n     <execution>\\n      <id>start-fitnesse</id>\\n      <phase>test</phase>\\n      <configuration>\\n       <tasks>\\n        <copy todir=\\\"target/dependencies\\\" flatten=\\\"true\\\">\\n         <fileset dir=\\\"../\\\">\\n          <include name=\\\"**/*.jar\\\" />\\n         </fileset>\\n        </copy>\\n        <echo message=\\\"\\\" />\\n        <echo message=\\\"\\\" />\\n        <echo message=\\\"Fitnesse wiki available here:\\\" />\\n        <echo message=\\\"\\\" />\\n        <echo message=\\\"  http://localhost:8888/MyTests.MySuite\\\" />\\n        <echo message=\\\"\\\" />\\n        <echo message=\\\"\\\" />\\n        <java classname=\\\"fitnesseMain.FitNesseMain\\\" classpathref=\\\"maven.test.classpath\\\"\\n         fork=\\\"true\\\">\\n         <arg line=\\\"-p 8888\\\" />\\n         <arg line=\\\"-d .\\\" />\\n         <arg line=\\\"-e 9999\\\" />\\n         <arg line=\\\"-b target/fitnesse-junit.xml\\\" />\\n         <arg line=\\\"${fitnesse.command}\\\" />\\n         <jvmarg value=\\\"-Xmx1024m\\\" />\\n        </java>\\n       </tasks>\\n      </configuration>\\n      <goals>\\n       <goal>run</goal>\\n      </goals>\\n     </execution>\\n    </executions>\\n   </plugin>\\n  </plugins>\\n </build>\\n</project>\\n```\\n\\nTo start the wiki to work with the test cases I do:\\n`mvn test`\\nAnd that will start the FitNesse wiki on *localhost*.\\n\\nWhen tests change, the changed wiki pages are committet and pushed to the remote repo.\\n\\nTo run all the tests I do:\\n`mvn test -Dfitnesse.command=\\\"-c MyTests.MySuite?suite&format=junit\\\"`\\nAnd in Jenkins I use the [Lockable Resources Plugin](https://wiki.jenkins.io/display/JENKINS/Lockable+Resources+Plugin) to allow IP port collision and only have one job running FitNesse at once.\\n\\nA pipeline `stage` may look like this:\\n```\\n...\\nstage(\'FitNesse test\') {\\n  lock(resource: \\\"compiler_${env.NODE_NAME}\\\", inversePrecedence: true) {\\n   try {\\n    sh \\\"\\\"\\\"\\n     cd fitnesse\\n     ${mvnHome}/bin/mvn test -Dfitnesse.command=\\\\\\\"-c MyTests.MySuite?suite&format=junit\\\\\\\"\\n     tar -zcvf fitnesseTestResults.tar.gz FitNesseRoot/files/testResults\\n    \\\"\\\"\\\"\\n    archiveArtifacts artifacts: \'fitnesseTestResults.tar.gz\', fingerprint: true\\n    junit \\\"**/fitnesse/target/*.xml\\\"\\n    commentMr(env.gitlabMergeRequestId, \\\"FitNesse ok :) $gitlabSourceBranch ${BUILD_URL}\\\")\\n   } catch (e) {\\n    commentMr(env.gitlabMergeRequestId, \\\"FitNesse **not ok** in $gitlabSourceBranch ${BUILD_URL}\\\")\\n    junit \\\"**/fitnesse/target/*.xml\\\"\\n    throw e\\n   }\\n  }\\n }\\n...\\n```\"}]],\"sections\":[[10,0]]}','<p>I needed to automate <a href=\"http://www.fitnesse.org/\">FitNesse</a> tests in my clients build process. I also needed the test cases in Git so that <em>testers</em> can develop tests in the same feature branches as the <em>developers</em> are using.</p>\n<p>I was looking for a simple way of doing that in a Java project where Maven and Jenkins is being used. I did not find any acceptable solutiion, here is my solution.</p>\n<h1 id=\"whatifound\">What I Found</h1>\n<p>I found <a href=\"http://www.fitnesse.org/FitNesse.UserGuide.WritingAcceptanceTests.RunningFromJunit\">Running from Junit</a> in the official documentation. But I was unable to use it.</p>\n<ul>\n<li>That API looks strange and it is unclear how to use it.</li>\n<li>I could not find any API for specifying where the jar files, with the <em>service under test</em>, are on the filesystem. When digging into the code on GitHub I concluded that there are no such feature.</li>\n</ul>\n<p>I found the <a href=\"https://wiki.jenkins.io/display/JENKINS/FitNesse+Plugin\">Jenkins Plugin</a> but I wanted testers to be able to work with the test cases locally. That plugin would only, potentially, solve it in Jenkins. There is also <a href=\"http://blog.xebia.com/how-to-integrate-fitnesse-tests-into-jenkins/\">Xebia</a> but same problem there.</p>\n<h1 id=\"whatidid\">What I Did</h1>\n<p>I created a <code>pom</code> that uses <code>antrun</code> to basically just run the <code>java</code> program with command line arguments. The version of FitNesse is managed in the <code>pom</code>.</p>\n<p>The wiki pages have:</p>\n<pre><code>!define TEST_SYSTEM {slim}\n!path target/dependencies/*.jar\n</code></pre>\n<p>Here is the <code>pom.xml</code>.</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;\n &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n &lt;artifactId&gt;bjurr-fitnesse&lt;/artifactId&gt;\n\n &lt;dependencies&gt;\n  &lt;dependency&gt;\n   &lt;groupId&gt;org.fitnesse&lt;/groupId&gt;\n   &lt;artifactId&gt;fitnesse&lt;/artifactId&gt;\n   &lt;version&gt;20161106&lt;/version&gt;\n   &lt;scope&gt;test&lt;/scope&gt;\n  &lt;/dependency&gt;\n &lt;/dependencies&gt;\n\n &lt;properties&gt;\n  &lt;fitnesse.command&gt;&lt;/fitnesse.command&gt;\n &lt;/properties&gt;\n\n &lt;build&gt;\n  &lt;plugins&gt;\n   &lt;plugin&gt;\n    &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;\n    &lt;version&gt;1.3&lt;/version&gt;\n    &lt;executions&gt;\n     &lt;execution&gt;\n      &lt;id&gt;start-fitnesse&lt;/id&gt;\n      &lt;phase&gt;test&lt;/phase&gt;\n      &lt;configuration&gt;\n       &lt;tasks&gt;\n        &lt;copy todir=&quot;target/dependencies&quot; flatten=&quot;true&quot;&gt;\n         &lt;fileset dir=&quot;../&quot;&gt;\n          &lt;include name=&quot;**/*.jar&quot; /&gt;\n         &lt;/fileset&gt;\n        &lt;/copy&gt;\n        &lt;echo message=&quot;&quot; /&gt;\n        &lt;echo message=&quot;&quot; /&gt;\n        &lt;echo message=&quot;Fitnesse wiki available here:&quot; /&gt;\n        &lt;echo message=&quot;&quot; /&gt;\n        &lt;echo message=&quot;  http://localhost:8888/MyTests.MySuite&quot; /&gt;\n        &lt;echo message=&quot;&quot; /&gt;\n        &lt;echo message=&quot;&quot; /&gt;\n        &lt;java classname=&quot;fitnesseMain.FitNesseMain&quot; classpathref=&quot;maven.test.classpath&quot;\n         fork=&quot;true&quot;&gt;\n         &lt;arg line=&quot;-p 8888&quot; /&gt;\n         &lt;arg line=&quot;-d .&quot; /&gt;\n         &lt;arg line=&quot;-e 9999&quot; /&gt;\n         &lt;arg line=&quot;-b target/fitnesse-junit.xml&quot; /&gt;\n         &lt;arg line=&quot;${fitnesse.command}&quot; /&gt;\n         &lt;jvmarg value=&quot;-Xmx1024m&quot; /&gt;\n        &lt;/java&gt;\n       &lt;/tasks&gt;\n      &lt;/configuration&gt;\n      &lt;goals&gt;\n       &lt;goal&gt;run&lt;/goal&gt;\n      &lt;/goals&gt;\n     &lt;/execution&gt;\n    &lt;/executions&gt;\n   &lt;/plugin&gt;\n  &lt;/plugins&gt;\n &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>\n<p>To start the wiki to work with the test cases I do:<br>\n<code>mvn test</code><br>\nAnd that will start the FitNesse wiki on <em>localhost</em>.</p>\n<p>When tests change, the changed wiki pages are committet and pushed to the remote repo.</p>\n<p>To run all the tests I do:<br>\n<code>mvn test -Dfitnesse.command=&quot;-c MyTests.MySuite?suite&amp;format=junit&quot;</code><br>\nAnd in Jenkins I use the <a href=\"https://wiki.jenkins.io/display/JENKINS/Lockable+Resources+Plugin\">Lockable Resources Plugin</a> to allow IP port collision and only have one job running FitNesse at once.</p>\n<p>A pipeline <code>stage</code> may look like this:</p>\n<pre><code>...\nstage(\'FitNesse test\') {\n  lock(resource: &quot;compiler_${env.NODE_NAME}&quot;, inversePrecedence: true) {\n   try {\n    sh &quot;&quot;&quot;\n     cd fitnesse\n     ${mvnHome}/bin/mvn test -Dfitnesse.command=\\&quot;-c MyTests.MySuite?suite&amp;format=junit\\&quot;\n     tar -zcvf fitnesseTestResults.tar.gz FitNesseRoot/files/testResults\n    &quot;&quot;&quot;\n    archiveArtifacts artifacts: \'fitnesseTestResults.tar.gz\', fingerprint: true\n    junit &quot;**/fitnesse/target/*.xml&quot;\n    commentMr(env.gitlabMergeRequestId, &quot;FitNesse ok :) $gitlabSourceBranch ${BUILD_URL}&quot;)\n   } catch (e) {\n    commentMr(env.gitlabMergeRequestId, &quot;FitNesse **not ok** in $gitlabSourceBranch ${BUILD_URL}&quot;)\n    junit &quot;**/fitnesse/target/*.xml&quot;\n    throw e\n   }\n  }\n }\n...\n</code></pre>\n','59e067ad8edbbd05ee0cef90','I needed to automate FitNesse [http://www.fitnesse.org/]  tests in my clients\nbuild process. I also needed the test cases in Git so that testers  can develop\ntests in the same feature branches as the developers  are using.\n\nI was looking for a simple way of doing that in a Java project where Maven and\nJenkins is being used. I did not find any acceptable solutiion, here is my\nsolution.\n\nWhat I Found\nI found Running from Junit\n[http://www.fitnesse.org/FitNesse.UserGuide.WritingAcceptanceTests.RunningFromJunit] \n in the official documentation. But I was unable to use it.\n\n * That API looks strange and it is unclear how to use it.\n * I could not find any API for specifying where the jar files, with the service\n   under test, are on the filesystem. When digging into the code on GitHub I\n   concluded that there are no such feature.\n\nI found the Jenkins Plugin\n[https://wiki.jenkins.io/display/JENKINS/FitNesse+Plugin]  but I wanted testers\nto be able to work with the test cases locally. That plugin would only,\npotentially, solve it in Jenkins. There is also Xebia\n[http://blog.xebia.com/how-to-integrate-fitnesse-tests-into-jenkins/]  but same\nproblem there.\n\nWhat I Did\nI created a pom  that uses antrun  to basically just run the java  program with\ncommand line arguments. The version of FitNesse is managed in the pom.\n\nThe wiki pages have:\n\n!define TEST_SYSTEM {slim}\n!path target/dependencies/*.jar\n\n\nHere is the pom.xml.\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n <modelVersion>4.0.0</modelVersion>\n\n <artifactId>bjurr-fitnesse</artifactId>\n\n <dependencies>\n  <dependency>\n   <groupId>org.fitnesse</groupId>\n   <artifactId>fitnesse</artifactId>\n   <version>20161106</version>\n   <scope>test</scope>\n  </dependency>\n </dependencies>\n\n <properties>\n  <fitnesse.command></fitnesse.command>\n </properties>\n\n <build>\n  <plugins>\n   <plugin>\n    <artifactId>maven-antrun-plugin</artifactId>\n    <version>1.3</version>\n    <executions>\n     <execution>\n      <id>start-fitnesse</id>\n      <phase>test</phase>\n      <configuration>\n       <tasks>\n        <copy todir=\"target/dependencies\" flatten=\"true\">\n         <fileset dir=\"../\">\n          <include name=\"**/*.jar\" />\n         </fileset>\n        </copy>\n        <echo message=\"\" />\n        <echo message=\"\" />\n        <echo message=\"Fitnesse wiki available here:\" />\n        <echo message=\"\" />\n        <echo message=\"  http://localhost:8888/MyTests.MySuite\" />\n        <echo message=\"\" />\n        <echo message=\"\" />\n        <java classname=\"fitnesseMain.FitNesseMain\" classpathref=\"maven.test.classpath\"\n         fork=\"true\">\n         <arg line=\"-p 8888\" />\n         <arg line=\"-d .\" />\n         <arg line=\"-e 9999\" />\n         <arg line=\"-b target/fitnesse-junit.xml\" />\n         <arg line=\"${fitnesse.command}\" />\n         <jvmarg value=\"-Xmx1024m\" />\n        </java>\n       </tasks>\n      </configuration>\n      <goals>\n       <goal>run</goal>\n      </goals>\n     </execution>\n    </executions>\n   </plugin>\n  </plugins>\n </build>\n</project>\n\n\nTo start the wiki to work with the test cases I do:\nmvn test\nAnd that will start the FitNesse wiki on localhost.\n\nWhen tests change, the changed wiki pages are committet and pushed to the remote\nrepo.\n\nTo run all the tests I do:\nmvn test -Dfitnesse.command=\"-c MyTests.MySuite?suite&format=junit\"\nAnd in Jenkins I use the Lockable Resources Plugin\n[https://wiki.jenkins.io/display/JENKINS/Lockable+Resources+Plugin]  to allow IP\nport collision and only have one job running FitNesse at once.\n\nA pipeline stage  may look like this:\n\n...\nstage(\'FitNesse test\') {\n  lock(resource: \"compiler_${env.NODE_NAME}\", inversePrecedence: true) {\n   try {\n    sh \"\"\"\n     cd fitnesse\n     ${mvnHome}/bin/mvn test -Dfitnesse.command=\\\"-c MyTests.MySuite?suite&format=junit\\\"\n     tar -zcvf fitnesseTestResults.tar.gz FitNesseRoot/files/testResults\n    \"\"\"\n    archiveArtifacts artifacts: \'fitnesseTestResults.tar.gz\', fingerprint: true\n    junit \"**/fitnesse/target/*.xml\"\n    commentMr(env.gitlabMergeRequestId, \"FitNesse ok :) $gitlabSourceBranch ${BUILD_URL}\")\n   } catch (e) {\n    commentMr(env.gitlabMergeRequestId, \"FitNesse **not ok** in $gitlabSourceBranch ${BUILD_URL}\")\n    junit \"**/fitnesse/target/*.xml\"\n    throw e\n   }\n  }\n }\n...','/content/images/2018/02/fitnesse-1.png',0,0,'published',NULL,'public',NULL,NULL,'1','2017-10-13 07:13:49','1','2018-02-24 09:03:08','1','2017-10-13 07:32:28','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5a23da214b28b905d8304a37','94db8a80-3e12-4300-93ee-5f0b0bb72a3d','Sharing Gradle build scripts across repositories','sharing-gradle-build-scripts-cross-repositories','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"While migrating my OSS from Maven Central to [Bintray](https://bintray.com/tomasbjerre) I realized I had a lot of duplicated Gradle scripts in different repos. I found [some alternatives](https://stackoverflow.com/questions/9539986/how-to-share-a-common-build-gradle-via-a-repository) on how to share build scripts across repositories. I built on that and here is what I did.\\n\\n# Solution\\nThe Gradle scripts are gathered in a *script repository*, see [Gradle Scripts](https://github.com/tomasbjerre/gradle-scripts). The *script repository* is a Gradle project in its own that packages the scripts into a *jar*. That *jar* is added as a build script dependency in projects that needs the scripts.\\n\\nThis is really convenient when managing a large amount of repositories. Scripts, each adding a specific ability to the build process, can be gathered at one single place. Be released and managed in branches just like any other project. The dependency will also be cached, just like any other dependency, making the build much faster then applying a script form a URL.\\n\\nThis shows another advantage of Gradle compared to Maven. If I was using Maven I would have to create a *pom* that I would use as *parent* for these projects. Anyone using the project would also need that *parent*. So adding functionality to the build process would have negative effect on the built artifact. A parent would also risk adding too much functionality to the build process. If I had a Maven plugin and a pure library, I would not be able to construct a logical *parent* for them. With Gradle I just pick the abilities I want and there will be no trace, except the intended, in the built artifact.\\n\\n# Implementation\\n\\nIn my case I use [JitPack](https://jitpack.io/) to build and supply the script jar. It means I can have a master branch where I can push changes to the scripts and immediately have those changes take effect in all my repos. But within an organization you would probably want to build and upload to something like Nexus or Artifactory.\\n\\nA project using this method will have a very slim build script. A running example is the [build script](https://github.com/tomasbjerre/violations-lib/blob/master/build.gradle) of my [Violations Lib](https://github.com/tomasbjerre/violations-lib).\\n\"}]],\"sections\":[[10,0]]}','<p>While migrating my OSS from Maven Central to <a href=\"https://bintray.com/tomasbjerre\">Bintray</a> I realized I had a lot of duplicated Gradle scripts in different repos. I found <a href=\"https://stackoverflow.com/questions/9539986/how-to-share-a-common-build-gradle-via-a-repository\">some alternatives</a> on how to share build scripts across repositories. I built on that and here is what I did.</p>\n<h1 id=\"solution\">Solution</h1>\n<p>The Gradle scripts are gathered in a <em>script repository</em>, see <a href=\"https://github.com/tomasbjerre/gradle-scripts\">Gradle Scripts</a>. The <em>script repository</em> is a Gradle project in its own that packages the scripts into a <em>jar</em>. That <em>jar</em> is added as a build script dependency in projects that needs the scripts.</p>\n<p>This is really convenient when managing a large amount of repositories. Scripts, each adding a specific ability to the build process, can be gathered at one single place. Be released and managed in branches just like any other project. The dependency will also be cached, just like any other dependency, making the build much faster then applying a script form a URL.</p>\n<p>This shows another advantage of Gradle compared to Maven. If I was using Maven I would have to create a <em>pom</em> that I would use as <em>parent</em> for these projects. Anyone using the project would also need that <em>parent</em>. So adding functionality to the build process would have negative effect on the built artifact. A parent would also risk adding too much functionality to the build process. If I had a Maven plugin and a pure library, I would not be able to construct a logical <em>parent</em> for them. With Gradle I just pick the abilities I want and there will be no trace, except the intended, in the built artifact.</p>\n<h1 id=\"implementation\">Implementation</h1>\n<p>In my case I use <a href=\"https://jitpack.io/\">JitPack</a> to build and supply the script jar. It means I can have a master branch where I can push changes to the scripts and immediately have those changes take effect in all my repos. But within an organization you would probably want to build and upload to something like Nexus or Artifactory.</p>\n<p>A project using this method will have a very slim build script. A running example is the <a href=\"https://github.com/tomasbjerre/violations-lib/blob/master/build.gradle\">build script</a> of my <a href=\"https://github.com/tomasbjerre/violations-lib\">Violations Lib</a>.</p>\n','5a23da214b28b905d8304a37','While migrating my OSS from Maven Central to Bintray\n[https://bintray.com/tomasbjerre]  I realized I had a lot of duplicated Gradle\nscripts in different repos. I found some alternatives\n[https://stackoverflow.com/questions/9539986/how-to-share-a-common-build-gradle-via-a-repository] \n on how to share build scripts across repositories. I built on that and here is\nwhat I did.\n\nSolution\nThe Gradle scripts are gathered in a script repository, see Gradle Scripts\n[https://github.com/tomasbjerre/gradle-scripts]. The script repository  is a\nGradle project in its own that packages the scripts into a jar. That jar  is\nadded as a build script dependency in projects that needs the scripts.\n\nThis is really convenient when managing a large amount of repositories. Scripts,\neach adding a specific ability to the build process, can be gathered at one\nsingle place. Be released and managed in branches just like any other project.\nThe dependency will also be cached, just like any other dependency, making the\nbuild much faster then applying a script form a URL.\n\nThis shows another advantage of Gradle compared to Maven. If I was using Maven I\nwould have to create a pom  that I would use as parent  for these projects.\nAnyone using the project would also need that parent. So adding functionality to\nthe build process would have negative effect on the built artifact. A parent\nwould also risk adding too much functionality to the build process. If I had a\nMaven plugin and a pure library, I would not be able to construct a logical \nparent  for them. With Gradle I just pick the abilities I want and there will be\nno trace, except the intended, in the built artifact.\n\nImplementation\nIn my case I use JitPack [https://jitpack.io/]  to build and supply the script\njar. It means I can have a master branch where I can push changes to the scripts\nand immediately have those changes take effect in all my repos. But within an\norganization you would probably want to build and upload to something like Nexus\nor Artifactory.\n\nA project using this method will have a very slim build script. A running\nexample is the build script\n[https://github.com/tomasbjerre/violations-lib/blob/master/build.gradle]  of my \nViolations Lib [https://github.com/tomasbjerre/violations-lib].','/content/images/2018/02/5847fb12cef1014c0b5e48d1.png',0,0,'published',NULL,'public',NULL,NULL,'1','2017-12-03 11:04:01','1','2018-02-24 09:00:04','1','2017-12-03 19:35:10','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5a755597c56a61057b219788','73afa94e-03b0-4d35-96c2-ce0bbac13826','Managing 1000+ Git repositories in Jenkins with a breeze','managing-1000-repos-in-jenkins-with-a-breeze','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This is a pattern that I find simple, easy/quick while still keeping you in control of your build flow. Should be no problem applying it in an organization with a huge amount of repositories!\\n\\nI am exclusively involved in **Java projects**. This post will sometimes assume *Maven* is being used. But it can probably inspire a solution in other projects too. I will not supply the complete running code. I\'m just going to include small snippets and focus on explaining the general idea.\\n\\nSome of this is implemented with Jenkins Configuration as Code, here: \\nhttps://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\\n\\n# Problems\\nSplitting a big application into several smaller artifacts is a good thing! To me, that is obvious but still I find myself talking to people that don\'t agree on that. Here are some of the arguments I hear on why not to split applications.\\n\\n> \\\"I want to build whatever I have checked out, locally, on my filesystem. We will need to spend many frustrating hours stepping dependencies between artifacts.\\\"\\n\\n> \\\"It is hard to detect when artifacts no longer fit together. We will find severe problems late because we don\'t continuously integrate all the artifacts.\\\"\\n\\n> \\\"We will need a huge amount of jobs in Jenkins (releasing, testing, integrating, deploying, snapshots...). We will need to spend much time managing them.\\\"\\n\\nOk! All valid points! And **all of them are more or less show stoppers if you don\'t do continuous integration right!**\\n\\n# Solution\\nIn short, I propose a solution where you:\\n\\n * Define a clear branching strategy.\\n * Define a translation strategy between branch and Maven artifact version.\\n * Define how any given repo should be built.\\n * Automate and define how repos depend on each other.\\n * Add a *Jenkinsfile* to each repo.\\n * Create a [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/).\\n * Automate the creation of the jobs.\\n\\nYou might consider [Pipeline Multibranch Plugin](https://wiki.jenkins.io/display/JENKINS/Pipeline+Multibranch+Plugin). I use [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) and [Folders Plugin](https://plugins.jenkins.io/cloudbees-folder). I have it create ordinary [pipeline](https://jenkins.io/doc/book/pipeline/)-jobs in a folder structure with a folder for each repository. \\n\\n* It gives me something static, the name of each job, to use when chaining jobs that depend on each other. There is a folder for each repository and it contains the jobs.\\n* It also allows several jobs to work with the same branch. I can easily create a release-job and a snapshot-job that both work with the *develop* branch.\\n\\nYou could have a static release-job and use mutlibranch to dynamically create every other job. But still, I feel I have more control with *Job DSL* and I feel it makes Jenkins look more organized.\\n\\n## Branching strategy\\nYou must know the meaing of the branches, in any given repo, in order to automate things. A defined branching strategy enables you to:\\n\\n * Clone any given repo.\\n * Detect what branches exists.\\n * Be sure which branch to use for *snapshots* or *releases*.\\n\\nIf your strategy is [GitFlow](https://bjurr.com/gitflow-and-when-you-should-use-it/), then:\\n\\n * The *snapshot*-job will\\n   * Build snapshots from *develop*.\\n   * Step dependencies in *develop*.\\n * The *release*-job will\\n   * Build releases from *hotfix* if it exists or else  *release*.\\n * The *feature*-job will build any *feature/X*-branches.\\n\\nEach repository has one release-cycle. Several artifacts, in the same repo, with different release-cycles are not allowed.\\n\\n## Branch to version translation\\n\\nThe integration between the Git service and Jenkins is setup so that when a commit is pushed to a feature-branch:\\n* A job is triggered.\\n* The branch name is identified.\\n* A version is derived form the branch name.\\n* Check to see if there is global a `bom` with that version\\n  * If no `bom` fall back to some default, fail or automate creation of that ´bom´-version.\\n* The artifacts are built, with a `bom` with the version, and uploaded to a Maven repository.\\n\\nThe `bom` -repo may function in the same way. Developers branch out of *develop*, getting all default versions. They set specific versions for some artifacts and commit/push. Or you automate that same procedure whenever a `bom`-version is missing. Then the artifact that originally triggered the creation of that bom can find its version and set it to itself.\\n\\nThen it will be possible to automatically create *deploy*-jobs for any \\\"deployaple\\\" repository. Where a dropdown list can automatically be populated with features. Features are found by listing feature-branches and translating to versions.\\n\\nDevelopers won\'t have to fiddle with versions locally, they just have whatever version that is also in *develop*-branches. And can clone a bunch of repos, built locally, and work with that fitting together.\\n\\n## Building\\n\\nYou must know how to build any given repo. With Maven, the aggreement might be as simple as:\\n\\n* The project is built from the root of the repo.\\n* The version, of the repository, is specified in the root of the repo. When using Maven, it is the version of the *pom.xml*.\\n* Different Maven profiles are allowed. Any profiles that produces artifacts should be specified as metadata about the repo in the *Jenkinsfile*.\\n\\nThe important thing is to define these rules. Do not start treating specific repos differently in the global build scripts. Instead specify global rules that all repos should follow.\\n\\n## Depending repositories\\n\\nTo be able to automatically chain jobs and have them trigger each other, I need to know *depending repos* of each repo. The opposite of what you have in `pom.xml`.\\n\\nOne way of doing that is with a job that:\\n\\n * Regularly finds all repos, perhaps via the Git service REST-API.\\n * Parse the `pom.xml`-files.\\n   * Find out what `artifacts` are contains in what repos.\\n   * Find out what `artifacts` are used in what repos.\\n * Create a structure with the depending repos per repo.\\n * Optimize that structure so that transitive dependencies are removed from list of direct dependencies.\\n * Store that structure as a json text -file in a repo. Making it available for snapshot/release-jobs to clone and include.\\n\\nHaving this information pre-calculated saves alot of time when it is needed by some job.\\n\\nPerhaps the depending repo structure can look something like this.\\n```\\n{\\n ...\\n \\\"PROJECT-A/example-repo-d\\\": [\\n  \\\"PROJECT-B/example-repo-b\\\",\\n  \\\"PROJECT-C/example-repo-d\\\"\\n ],\\n \\\"PROJECT-C/example-repo-d\\\": [\\n  \\\"PROJECT-E/example-repo-b\\\"\\n ]\\n ...\\n}\\n```\\n\\n## Jenkinsfile\\nIt is very small and contains only metadata about the repo. This is just like [Jenkins Infra](https://github.com/jenkins-infra/pipeline-library/blob/master/vars/buildPlugin.groovy) handles their 1000+ plugins.\\n\\nWhen using Maven, you might want to specify *profiles* to be built.\\n\\nA repo that needs to be built, nothing else, may look like this:\\n```\\nbuildRepo()\\n```\\n\\nIt may specify profiles:\\n```\\nbuildRepo(\\n profiles: [\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nAnd if a profiles are needed as well as no profile, it may look like:\\n```\\nbuildRepo(\\n profiles: [\\n  \'\',\\n  \'profile1\'\\n ]\\n)\\n```\\n\\nThis is all there is in the repositories. Only one *Jenkinsfile* and this is the only information it contains. I\'m not saying you only need this. I\'m just recommending to keep it light! Perhaps you invent thnkgs like `deployable: true` or `autoDeployEnv: \'TEST-XY\'`...\\n\\n## Shared library\\nA [shared pipeline library](https://jenkins.io/doc/book/pipeline/shared-libraries/) allows you to, only once, define how to do releases, snapshots and all other tasks.\\n\\nWith the above *Jenkinsfile* there should be a `/vars/buildRepo.groovy` containing something like:\\n```\\n...\\ndef call(Map params = [:]) {\\n...\\n if (JOB_BASE_NAME.equals(\\\"snapshot\\\")) {\\n ...\\n } else if (JOB_BASE_NAME.equals(\\\"release\\\")) {\\n ...\\n }\\n...\\n}\\n...\\n```\\n\\n## Automate creation of jobs\\nMost Git services (GitHub, GitLab, Bitbucket...) provide REST API:s. You can use that to automate creation/deletion/adjustment of jobs and always be in sync with the repos you have in your Git service. The job DSL would loop through all repositories.\\n\\n```\\n...\\n folder(\\\"gen\\\") {\\n  displayName(\\\"Generated jobs\\\")\\n  description(\\\"\\\"\\\"\\n   These are generated by ${JOB_URL}\\n  \\\"\\\"\\\")\\n }\\n\\n getJson(server+ \\\"/rest/request/to/repos...\\\")\\n  .values\\n  .each { repo ->\\n  folder(\\\"gen\\\") {\\n   displayName(\\\"gen/\\\" + repo.name)\\n   description(\\\"\\\"\\\"\\n    Generated by ${JOB_URL}\\n   \\\"\\\"\\\")\\n  }\\n\\n  pipelineJob(\\\"gen/\\\" + repo.name + \\\"/snapshot\\\") {\\n...\\n```\\n\\n## Templates\\nI use the [Job DSL](https://github.com/jenkinsci/job-dsl-plugin/wiki) plugin. Perhaps you want these jobs for every repository:\\n\\n * *snapshot*\\n * *release*\\n * *feature*\\n * *pull-request*\\n\\nAlso a global job, *release-orchestration*.\\n\\nAll of these templates are pipelines. Their logic is implemented in the shared library. The shared library will find the Git repo to use from `scm.getUserRemoteConfigs().get(0).getUrl()` and the kind of job to build from `JOB_BASE_NAME`.\\n\\n### Snapshot\\nThis job will:\\n* Make sure *develop* is using latest dependencies ([found in Maven repository](http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html)). If there are newer versions:\\n  * Step dependencies to latest version.\\n  * Commit changes.\\n  * Push changes.\\n  * Re-trigger self, to help Jenkins understand that this new commit does not need to be built again. Done.\\n* Build a snapshot version.\\n* Upload snapshot-version to Maven repository.\\n* Trigger *dependingRepos* configured in *Jenkinsfile*.\\n* Done.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release\\nThis job will:\\n* Start from commit *C1*.\\n* Set dependencies to latest released versions.\\n* Step version to next release-version.\\n* Make a commit *C2*.\\n* Set dependencies to latest snapshots.\\n* Step version to next snapshot-version.\\n* Make a commit *C3*.\\n* Try to push changes. If not successful:\\n  * Hard reset to *C1*.\\n  * Pull.\\n  * Start again with creating *C2*.\\n  * Do this loop, perhaps 5 times before giving up and fail.\\n\\nThis allows developers to work in the branches during the release-process.\\n\\nNow that we know we are in sync with remote Git repo on where to perform the release, we can continue doing so.\\n\\n* Tag *C2* with the release-version.\\n* Perform the build commands, `mvn package` and loop any profiles needed.\\n* Deploy in Maven repository, `mvn deploy`.\\n\\nWhen using Maven, you can do this with [Versions Maven Plugin](http://www.mojohaus.org/versions-maven-plugin/).\\n\\n### Release orchestration\\nThis job will:\\n * Orchestrates a release.\\n * Is parameterized with each repo.\\n * When triggered:\\n   * Calculates the order to release selected repos. With the information found in their *Jenkinsfile*:s.\\n   * Invokes the *release*-jobs of each selected repo.\\n\\n# Features\\nHere is what features this setup can provide and how I intend them to be used.\\n## Release\\n\\nA release of a single repo can be performed from its *release*-job.\\n\\nThis will look at each repo and release from the branch that is first found in this order:\\n\\n1. *hotfix*\\n2. *release*\\n3. *develop*\\n4. *master*\\n\\nSo if you want to release from a specific commit, not latest *develop*, just push a *release*, or *hotfix*, -branch that points to that commit.\\n\\n### Orchestrating a release\\n\\nA release of one, or more, repos can be performed from the global *release-orchestration*-job. This will:\\n\\n* Ensure the release of each repo\\n  * Is done in the right order.\\n  * Their dependencies will be released first, if selected.\\n  * Use the latest release of their dependencies.\\n* After release, trigger the snapshot-job of the first repo that was released. So that all the snapshot-jobs will run and step snapshot-versions.\\n\\nIt will invoke the release-jobs of each repo. This means you can have a look there for more details on that specific release.\\n\\n## Hotfix\\n\\nHaving the priority among branches, mentioned above, will enable you to push a *hotfix*-branch from any commit and have the release being performed from that commit. If your *master* points to latest installed version:\\n\\n* `git checkout master`\\n* `git checkout -b hotfix`\\n* `git push -u origin hotfix`\\n\\nThen just trigger the release.\\n# Advantages\\nYour entire **Jenkins configuration is put under version control**. Well... you need to create one Job DSL -job manually that polls, or is triggered by changes in, the git service. But that job can have its DSL in a Git-repo. This has a bunch of advantages.\\n\\n * No more browsing around in Jenkins and fiddling with settings.\\n * You can track changes in the jobs. Just use `git blame`, it is all code now!\\n * All your jobs are backed up with Git.\\n * You can easily setup a development instance of Jenkins that behaves very much like your production instance.\\n * You can generate release-jobs in one Jenkins and snapshot jobs in another. Letting only a few people use the release-jenkins and anyone use the other instance.\"}]],\"markups\":[],\"sections\":[[10,0]]}','<p>This is a pattern that I find simple, easy/quick while still keeping you in control of your build flow. Should be no problem applying it in an organization with a huge amount of repositories!</p>\n<p>I am exclusively involved in <strong>Java projects</strong>. This post will sometimes assume <em>Maven</em> is being used. But it can probably inspire a solution in other projects too. I will not supply the complete running code. I\'m just going to include small snippets and focus on explaining the general idea.</p>\n<p>Some of this is implemented with Jenkins Configuration as Code, here:<br>\n<a href=\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\">https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox</a></p>\n<h1 id=\"problems\">Problems</h1>\n<p>Splitting a big application into several smaller artifacts is a good thing! To me, that is obvious but still I find myself talking to people that don\'t agree on that. Here are some of the arguments I hear on why not to split applications.</p>\n<blockquote>\n<p>&quot;I want to build whatever I have checked out, locally, on my filesystem. We will need to spend many frustrating hours stepping dependencies between artifacts.&quot;</p>\n</blockquote>\n<blockquote>\n<p>&quot;It is hard to detect when artifacts no longer fit together. We will find severe problems late because we don\'t continuously integrate all the artifacts.&quot;</p>\n</blockquote>\n<blockquote>\n<p>&quot;We will need a huge amount of jobs in Jenkins (releasing, testing, integrating, deploying, snapshots...). We will need to spend much time managing them.&quot;</p>\n</blockquote>\n<p>Ok! All valid points! And <strong>all of them are more or less show stoppers if you don\'t do continuous integration right!</strong></p>\n<h1 id=\"solution\">Solution</h1>\n<p>In short, I propose a solution where you:</p>\n<ul>\n<li>Define a clear branching strategy.</li>\n<li>Define a translation strategy between branch and Maven artifact version.</li>\n<li>Define how any given repo should be built.</li>\n<li>Automate and define how repos depend on each other.</li>\n<li>Add a <em>Jenkinsfile</em> to each repo.</li>\n<li>Create a <a href=\"https://jenkins.io/doc/book/pipeline/shared-libraries/\">shared pipeline library</a>.</li>\n<li>Automate the creation of the jobs.</li>\n</ul>\n<p>You might consider <a href=\"https://wiki.jenkins.io/display/JENKINS/Pipeline+Multibranch+Plugin\">Pipeline Multibranch Plugin</a>. I use <a href=\"https://github.com/jenkinsci/job-dsl-plugin/wiki\">Job DSL</a> and <a href=\"https://plugins.jenkins.io/cloudbees-folder\">Folders Plugin</a>. I have it create ordinary <a href=\"https://jenkins.io/doc/book/pipeline/\">pipeline</a>-jobs in a folder structure with a folder for each repository.</p>\n<ul>\n<li>It gives me something static, the name of each job, to use when chaining jobs that depend on each other. There is a folder for each repository and it contains the jobs.</li>\n<li>It also allows several jobs to work with the same branch. I can easily create a release-job and a snapshot-job that both work with the <em>develop</em> branch.</li>\n</ul>\n<p>You could have a static release-job and use mutlibranch to dynamically create every other job. But still, I feel I have more control with <em>Job DSL</em> and I feel it makes Jenkins look more organized.</p>\n<h2 id=\"branchingstrategy\">Branching strategy</h2>\n<p>You must know the meaing of the branches, in any given repo, in order to automate things. A defined branching strategy enables you to:</p>\n<ul>\n<li>Clone any given repo.</li>\n<li>Detect what branches exists.</li>\n<li>Be sure which branch to use for <em>snapshots</em> or <em>releases</em>.</li>\n</ul>\n<p>If your strategy is <a href=\"https://bjurr.com/gitflow-and-when-you-should-use-it/\">GitFlow</a>, then:</p>\n<ul>\n<li>The <em>snapshot</em>-job will\n<ul>\n<li>Build snapshots from <em>develop</em>.</li>\n<li>Step dependencies in <em>develop</em>.</li>\n</ul>\n</li>\n<li>The <em>release</em>-job will\n<ul>\n<li>Build releases from <em>hotfix</em> if it exists or else  <em>release</em>.</li>\n</ul>\n</li>\n<li>The <em>feature</em>-job will build any <em>feature/X</em>-branches.</li>\n</ul>\n<p>Each repository has one release-cycle. Several artifacts, in the same repo, with different release-cycles are not allowed.</p>\n<h2 id=\"branchtoversiontranslation\">Branch to version translation</h2>\n<p>The integration between the Git service and Jenkins is setup so that when a commit is pushed to a feature-branch:</p>\n<ul>\n<li>A job is triggered.</li>\n<li>The branch name is identified.</li>\n<li>A version is derived form the branch name.</li>\n<li>Check to see if there is global a <code>bom</code> with that version\n<ul>\n<li>If no <code>bom</code> fall back to some default, fail or automate creation of that ´bom´-version.</li>\n</ul>\n</li>\n<li>The artifacts are built, with a <code>bom</code> with the version, and uploaded to a Maven repository.</li>\n</ul>\n<p>The <code>bom</code> -repo may function in the same way. Developers branch out of <em>develop</em>, getting all default versions. They set specific versions for some artifacts and commit/push. Or you automate that same procedure whenever a <code>bom</code>-version is missing. Then the artifact that originally triggered the creation of that bom can find its version and set it to itself.</p>\n<p>Then it will be possible to automatically create <em>deploy</em>-jobs for any &quot;deployaple&quot; repository. Where a dropdown list can automatically be populated with features. Features are found by listing feature-branches and translating to versions.</p>\n<p>Developers won\'t have to fiddle with versions locally, they just have whatever version that is also in <em>develop</em>-branches. And can clone a bunch of repos, built locally, and work with that fitting together.</p>\n<h2 id=\"building\">Building</h2>\n<p>You must know how to build any given repo. With Maven, the aggreement might be as simple as:</p>\n<ul>\n<li>The project is built from the root of the repo.</li>\n<li>The version, of the repository, is specified in the root of the repo. When using Maven, it is the version of the <em>pom.xml</em>.</li>\n<li>Different Maven profiles are allowed. Any profiles that produces artifacts should be specified as metadata about the repo in the <em>Jenkinsfile</em>.</li>\n</ul>\n<p>The important thing is to define these rules. Do not start treating specific repos differently in the global build scripts. Instead specify global rules that all repos should follow.</p>\n<h2 id=\"dependingrepositories\">Depending repositories</h2>\n<p>To be able to automatically chain jobs and have them trigger each other, I need to know <em>depending repos</em> of each repo. The opposite of what you have in <code>pom.xml</code>.</p>\n<p>One way of doing that is with a job that:</p>\n<ul>\n<li>Regularly finds all repos, perhaps via the Git service REST-API.</li>\n<li>Parse the <code>pom.xml</code>-files.\n<ul>\n<li>Find out what <code>artifacts</code> are contains in what repos.</li>\n<li>Find out what <code>artifacts</code> are used in what repos.</li>\n</ul>\n</li>\n<li>Create a structure with the depending repos per repo.</li>\n<li>Optimize that structure so that transitive dependencies are removed from list of direct dependencies.</li>\n<li>Store that structure as a json text -file in a repo. Making it available for snapshot/release-jobs to clone and include.</li>\n</ul>\n<p>Having this information pre-calculated saves alot of time when it is needed by some job.</p>\n<p>Perhaps the depending repo structure can look something like this.</p>\n<pre><code>{\n ...\n &quot;PROJECT-A/example-repo-d&quot;: [\n  &quot;PROJECT-B/example-repo-b&quot;,\n  &quot;PROJECT-C/example-repo-d&quot;\n ],\n &quot;PROJECT-C/example-repo-d&quot;: [\n  &quot;PROJECT-E/example-repo-b&quot;\n ]\n ...\n}\n</code></pre>\n<h2 id=\"jenkinsfile\">Jenkinsfile</h2>\n<p>It is very small and contains only metadata about the repo. This is just like <a href=\"https://github.com/jenkins-infra/pipeline-library/blob/master/vars/buildPlugin.groovy\">Jenkins Infra</a> handles their 1000+ plugins.</p>\n<p>When using Maven, you might want to specify <em>profiles</em> to be built.</p>\n<p>A repo that needs to be built, nothing else, may look like this:</p>\n<pre><code>buildRepo()\n</code></pre>\n<p>It may specify profiles:</p>\n<pre><code>buildRepo(\n profiles: [\n  \'profile1\'\n ]\n)\n</code></pre>\n<p>And if a profiles are needed as well as no profile, it may look like:</p>\n<pre><code>buildRepo(\n profiles: [\n  \'\',\n  \'profile1\'\n ]\n)\n</code></pre>\n<p>This is all there is in the repositories. Only one <em>Jenkinsfile</em> and this is the only information it contains. I\'m not saying you only need this. I\'m just recommending to keep it light! Perhaps you invent thnkgs like <code>deployable: true</code> or <code>autoDeployEnv: \'TEST-XY\'</code>...</p>\n<h2 id=\"sharedlibrary\">Shared library</h2>\n<p>A <a href=\"https://jenkins.io/doc/book/pipeline/shared-libraries/\">shared pipeline library</a> allows you to, only once, define how to do releases, snapshots and all other tasks.</p>\n<p>With the above <em>Jenkinsfile</em> there should be a <code>/vars/buildRepo.groovy</code> containing something like:</p>\n<pre><code>...\ndef call(Map params = [:]) {\n...\n if (JOB_BASE_NAME.equals(&quot;snapshot&quot;)) {\n ...\n } else if (JOB_BASE_NAME.equals(&quot;release&quot;)) {\n ...\n }\n...\n}\n...\n</code></pre>\n<h2 id=\"automatecreationofjobs\">Automate creation of jobs</h2>\n<p>Most Git services (GitHub, GitLab, Bitbucket...) provide REST API:s. You can use that to automate creation/deletion/adjustment of jobs and always be in sync with the repos you have in your Git service. The job DSL would loop through all repositories.</p>\n<pre><code>...\n folder(&quot;gen&quot;) {\n  displayName(&quot;Generated jobs&quot;)\n  description(&quot;&quot;&quot;\n   These are generated by ${JOB_URL}\n  &quot;&quot;&quot;)\n }\n\n getJson(server+ &quot;/rest/request/to/repos...&quot;)\n  .values\n  .each { repo -&gt;\n  folder(&quot;gen&quot;) {\n   displayName(&quot;gen/&quot; + repo.name)\n   description(&quot;&quot;&quot;\n    Generated by ${JOB_URL}\n   &quot;&quot;&quot;)\n  }\n\n  pipelineJob(&quot;gen/&quot; + repo.name + &quot;/snapshot&quot;) {\n...\n</code></pre>\n<h2 id=\"templates\">Templates</h2>\n<p>I use the <a href=\"https://github.com/jenkinsci/job-dsl-plugin/wiki\">Job DSL</a> plugin. Perhaps you want these jobs for every repository:</p>\n<ul>\n<li><em>snapshot</em></li>\n<li><em>release</em></li>\n<li><em>feature</em></li>\n<li><em>pull-request</em></li>\n</ul>\n<p>Also a global job, <em>release-orchestration</em>.</p>\n<p>All of these templates are pipelines. Their logic is implemented in the shared library. The shared library will find the Git repo to use from <code>scm.getUserRemoteConfigs().get(0).getUrl()</code> and the kind of job to build from <code>JOB_BASE_NAME</code>.</p>\n<h3 id=\"snapshot\">Snapshot</h3>\n<p>This job will:</p>\n<ul>\n<li>Make sure <em>develop</em> is using latest dependencies (<a href=\"http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html\">found in Maven repository</a>). If there are newer versions:\n<ul>\n<li>Step dependencies to latest version.</li>\n<li>Commit changes.</li>\n<li>Push changes.</li>\n<li>Re-trigger self, to help Jenkins understand that this new commit does not need to be built again. Done.</li>\n</ul>\n</li>\n<li>Build a snapshot version.</li>\n<li>Upload snapshot-version to Maven repository.</li>\n<li>Trigger <em>dependingRepos</em> configured in <em>Jenkinsfile</em>.</li>\n<li>Done.</li>\n</ul>\n<p>When using Maven, you can do this with <a href=\"http://www.mojohaus.org/versions-maven-plugin/\">Versions Maven Plugin</a>.</p>\n<h3 id=\"release\">Release</h3>\n<p>This job will:</p>\n<ul>\n<li>Start from commit <em>C1</em>.</li>\n<li>Set dependencies to latest released versions.</li>\n<li>Step version to next release-version.</li>\n<li>Make a commit <em>C2</em>.</li>\n<li>Set dependencies to latest snapshots.</li>\n<li>Step version to next snapshot-version.</li>\n<li>Make a commit <em>C3</em>.</li>\n<li>Try to push changes. If not successful:\n<ul>\n<li>Hard reset to <em>C1</em>.</li>\n<li>Pull.</li>\n<li>Start again with creating <em>C2</em>.</li>\n<li>Do this loop, perhaps 5 times before giving up and fail.</li>\n</ul>\n</li>\n</ul>\n<p>This allows developers to work in the branches during the release-process.</p>\n<p>Now that we know we are in sync with remote Git repo on where to perform the release, we can continue doing so.</p>\n<ul>\n<li>Tag <em>C2</em> with the release-version.</li>\n<li>Perform the build commands, <code>mvn package</code> and loop any profiles needed.</li>\n<li>Deploy in Maven repository, <code>mvn deploy</code>.</li>\n</ul>\n<p>When using Maven, you can do this with <a href=\"http://www.mojohaus.org/versions-maven-plugin/\">Versions Maven Plugin</a>.</p>\n<h3 id=\"releaseorchestration\">Release orchestration</h3>\n<p>This job will:</p>\n<ul>\n<li>Orchestrates a release.</li>\n<li>Is parameterized with each repo.</li>\n<li>When triggered:\n<ul>\n<li>Calculates the order to release selected repos. With the information found in their <em>Jenkinsfile</em>:s.</li>\n<li>Invokes the <em>release</em>-jobs of each selected repo.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"features\">Features</h1>\n<p>Here is what features this setup can provide and how I intend them to be used.</p>\n<h2 id=\"release\">Release</h2>\n<p>A release of a single repo can be performed from its <em>release</em>-job.</p>\n<p>This will look at each repo and release from the branch that is first found in this order:</p>\n<ol>\n<li><em>hotfix</em></li>\n<li><em>release</em></li>\n<li><em>develop</em></li>\n<li><em>master</em></li>\n</ol>\n<p>So if you want to release from a specific commit, not latest <em>develop</em>, just push a <em>release</em>, or <em>hotfix</em>, -branch that points to that commit.</p>\n<h3 id=\"orchestratingarelease\">Orchestrating a release</h3>\n<p>A release of one, or more, repos can be performed from the global <em>release-orchestration</em>-job. This will:</p>\n<ul>\n<li>Ensure the release of each repo\n<ul>\n<li>Is done in the right order.</li>\n<li>Their dependencies will be released first, if selected.</li>\n<li>Use the latest release of their dependencies.</li>\n</ul>\n</li>\n<li>After release, trigger the snapshot-job of the first repo that was released. So that all the snapshot-jobs will run and step snapshot-versions.</li>\n</ul>\n<p>It will invoke the release-jobs of each repo. This means you can have a look there for more details on that specific release.</p>\n<h2 id=\"hotfix\">Hotfix</h2>\n<p>Having the priority among branches, mentioned above, will enable you to push a <em>hotfix</em>-branch from any commit and have the release being performed from that commit. If your <em>master</em> points to latest installed version:</p>\n<ul>\n<li><code>git checkout master</code></li>\n<li><code>git checkout -b hotfix</code></li>\n<li><code>git push -u origin hotfix</code></li>\n</ul>\n<p>Then just trigger the release.</p>\n<h1 id=\"advantages\">Advantages</h1>\n<p>Your entire <strong>Jenkins configuration is put under version control</strong>. Well... you need to create one Job DSL -job manually that polls, or is triggered by changes in, the git service. But that job can have its DSL in a Git-repo. This has a bunch of advantages.</p>\n<ul>\n<li>No more browsing around in Jenkins and fiddling with settings.</li>\n<li>You can track changes in the jobs. Just use <code>git blame</code>, it is all code now!</li>\n<li>All your jobs are backed up with Git.</li>\n<li>You can easily setup a development instance of Jenkins that behaves very much like your production instance.</li>\n<li>You can generate release-jobs in one Jenkins and snapshot jobs in another. Letting only a few people use the release-jenkins and anyone use the other instance.</li>\n</ul>\n','5a755597c56a61057b219788','This is a pattern that I find simple, easy/quick while still keeping you in\ncontrol of your build flow. Should be no problem applying it in an organization\nwith a huge amount of repositories!\n\nI am exclusively involved in Java projects. This post will sometimes assume \nMaven  is being used. But it can probably inspire a solution in other projects\ntoo. I will not supply the complete running code. I\'m just going to include\nsmall snippets and focus on explaining the general idea.\n\nSome of this is implemented with Jenkins Configuration as Code, here:\nhttps://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\n\nProblems\nSplitting a big application into several smaller artifacts is a good thing! To\nme, that is obvious but still I find myself talking to people that don\'t agree\non that. Here are some of the arguments I hear on why not to split applications.\n\n\"I want to build whatever I have checked out, locally, on my filesystem. We will\nneed to spend many frustrating hours stepping dependencies between artifacts.\"\n\n\"It is hard to detect when artifacts no longer fit together. We will find severe\nproblems late because we don\'t continuously integrate all the artifacts.\"\n\n\"We will need a huge amount of jobs in Jenkins (releasing, testing, integrating,\ndeploying, snapshots...). We will need to spend much time managing them.\"\n\nOk! All valid points! And all of them are more or less show stoppers if you\ndon\'t do continuous integration right!\n\nSolution\nIn short, I propose a solution where you:\n\n * Define a clear branching strategy.\n * Define a translation strategy between branch and Maven artifact version.\n * Define how any given repo should be built.\n * Automate and define how repos depend on each other.\n * Add a Jenkinsfile  to each repo.\n * Create a shared pipeline library\n   [https://jenkins.io/doc/book/pipeline/shared-libraries/].\n * Automate the creation of the jobs.\n\nYou might consider Pipeline Multibranch Plugin\n[https://wiki.jenkins.io/display/JENKINS/Pipeline+Multibranch+Plugin]. I use \nJob\nDSL [https://github.com/jenkinsci/job-dsl-plugin/wiki]  and Folders Plugin\n[https://plugins.jenkins.io/cloudbees-folder]. I have it create ordinary \npipeline [https://jenkins.io/doc/book/pipeline/]-jobs in a folder structure with\na folder for each repository.\n\n * It gives me something static, the name of each job, to use when chaining jobs\n   that depend on each other. There is a folder for each repository and it\n   contains the jobs.\n * It also allows several jobs to work with the same branch. I can easily create\n   a release-job and a snapshot-job that both work with the develop  branch.\n\nYou could have a static release-job and use mutlibranch to dynamically create\nevery other job. But still, I feel I have more control with Job DSL  and I feel\nit makes Jenkins look more organized.\n\nBranching strategy\nYou must know the meaing of the branches, in any given repo, in order to\nautomate things. A defined branching strategy enables you to:\n\n * Clone any given repo.\n * Detect what branches exists.\n * Be sure which branch to use for snapshots  or releases.\n\nIf your strategy is GitFlow\n[https://bjurr.com/gitflow-and-when-you-should-use-it/], then:\n\n * The snapshot-job will * Build snapshots from develop.\n    * Step dependencies in develop.\n   \n   \n * The release-job will * Build releases from hotfix  if it exists or else release.\n   \n   \n * The feature-job will build any feature/X-branches.\n\nEach repository has one release-cycle. Several artifacts, in the same repo, with\ndifferent release-cycles are not allowed.\n\nBranch to version translation\nThe integration between the Git service and Jenkins is setup so that when a\ncommit is pushed to a feature-branch:\n\n * A job is triggered.\n * The branch name is identified.\n * A version is derived form the branch name.\n * Check to see if there is global a bom  with that version * If no bom  fall back to some default, fail or automate creation of that\n      ´bom´-version.\n   \n   \n * The artifacts are built, with a bom  with the version, and uploaded to a\n   Maven repository.\n\nThe bom  -repo may function in the same way. Developers branch out of develop,\ngetting all default versions. They set specific versions for some artifacts and\ncommit/push. Or you automate that same procedure whenever a bom-version is\nmissing. Then the artifact that originally triggered the creation of that bom\ncan find its version and set it to itself.\n\nThen it will be possible to automatically create deploy-jobs for any\n\"deployaple\" repository. Where a dropdown list can automatically be populated\nwith features. Features are found by listing feature-branches and translating to\nversions.\n\nDevelopers won\'t have to fiddle with versions locally, they just have whatever\nversion that is also in develop-branches. And can clone a bunch of repos, built\nlocally, and work with that fitting together.\n\nBuilding\nYou must know how to build any given repo. With Maven, the aggreement might be\nas simple as:\n\n * The project is built from the root of the repo.\n * The version, of the repository, is specified in the root of the repo. When\n   using Maven, it is the version of the pom.xml.\n * Different Maven profiles are allowed. Any profiles that produces artifacts\n   should be specified as metadata about the repo in the Jenkinsfile.\n\nThe important thing is to define these rules. Do not start treating specific\nrepos differently in the global build scripts. Instead specify global rules that\nall repos should follow.\n\nDepending repositories\nTo be able to automatically chain jobs and have them trigger each other, I need\nto know depending repos  of each repo. The opposite of what you have in pom.xml.\n\nOne way of doing that is with a job that:\n\n * Regularly finds all repos, perhaps via the Git service REST-API.\n * Parse the pom.xml-files. * Find out what artifacts  are contains in what repos.\n    * Find out what artifacts  are used in what repos.\n   \n   \n * Create a structure with the depending repos per repo.\n * Optimize that structure so that transitive dependencies are removed from list\n   of direct dependencies.\n * Store that structure as a json text -file in a repo. Making it available for\n   snapshot/release-jobs to clone and include.\n\nHaving this information pre-calculated saves alot of time when it is needed by\nsome job.\n\nPerhaps the depending repo structure can look something like this.\n\n{\n ...\n \"PROJECT-A/example-repo-d\": [\n  \"PROJECT-B/example-repo-b\",\n  \"PROJECT-C/example-repo-d\"\n ],\n \"PROJECT-C/example-repo-d\": [\n  \"PROJECT-E/example-repo-b\"\n ]\n ...\n}\n\n\nJenkinsfile\nIt is very small and contains only metadata about the repo. This is just like \nJenkins Infra\n[https://github.com/jenkins-infra/pipeline-library/blob/master/vars/buildPlugin.groovy] \n handles their 1000+ plugins.\n\nWhen using Maven, you might want to specify profiles  to be built.\n\nA repo that needs to be built, nothing else, may look like this:\n\nbuildRepo()\n\n\nIt may specify profiles:\n\nbuildRepo(\n profiles: [\n  \'profile1\'\n ]\n)\n\n\nAnd if a profiles are needed as well as no profile, it may look like:\n\nbuildRepo(\n profiles: [\n  \'\',\n  \'profile1\'\n ]\n)\n\n\nThis is all there is in the repositories. Only one Jenkinsfile  and this is the\nonly information it contains. I\'m not saying you only need this. I\'m just\nrecommending to keep it light! Perhaps you invent thnkgs like deployable: true \nor autoDeployEnv: \'TEST-XY\'...\n\nShared library\nA shared pipeline library\n[https://jenkins.io/doc/book/pipeline/shared-libraries/]  allows you to, only\nonce, define how to do releases, snapshots and all other tasks.\n\nWith the above Jenkinsfile  there should be a /vars/buildRepo.groovy  containing\nsomething like:\n\n...\ndef call(Map params = [:]) {\n...\n if (JOB_BASE_NAME.equals(\"snapshot\")) {\n ...\n } else if (JOB_BASE_NAME.equals(\"release\")) {\n ...\n }\n...\n}\n...\n\n\nAutomate creation of jobs\nMost Git services (GitHub, GitLab, Bitbucket...) provide REST API:s. You can use\nthat to automate creation/deletion/adjustment of jobs and always be in sync with\nthe repos you have in your Git service. The job DSL would loop through all\nrepositories.\n\n...\n folder(\"gen\") {\n  displayName(\"Generated jobs\")\n  description(\"\"\"\n   These are generated by ${JOB_URL}\n  \"\"\")\n }\n\n getJson(server+ \"/rest/request/to/repos...\")\n  .values\n  .each { repo ->\n  folder(\"gen\") {\n   displayName(\"gen/\" + repo.name)\n   description(\"\"\"\n    Generated by ${JOB_URL}\n   \"\"\")\n  }\n\n  pipelineJob(\"gen/\" + repo.name + \"/snapshot\") {\n...\n\n\nTemplates\nI use the Job DSL [https://github.com/jenkinsci/job-dsl-plugin/wiki]  plugin.\nPerhaps you want these jobs for every repository:\n\n * snapshot\n * release\n * feature\n * pull-request\n\nAlso a global job, release-orchestration.\n\nAll of these templates are pipelines. Their logic is implemented in the shared\nlibrary. The shared library will find the Git repo to use from \nscm.getUserRemoteConfigs().get(0).getUrl()  and the kind of job to build from \nJOB_BASE_NAME.\n\nSnapshot\nThis job will:\n\n * Make sure develop  is using latest dependencies (found in Maven repository\n   [http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html]).\n   If there are newer versions: * Step dependencies to latest version.\n    * Commit changes.\n    * Push changes.\n    * Re-trigger self, to help Jenkins understand\n      that this new commit does not need to be built again. Done.\n   \n   \n * Build a snapshot version.\n * Upload snapshot-version to Maven repository.\n * Trigger dependingRepos  configured in Jenkinsfile.\n * Done.\n\nWhen using Maven, you can do this with Versions Maven Plugin\n[http://www.mojohaus.org/versions-maven-plugin/].\n\nRelease\nThis job will:\n\n * Start from commit C1.\n * Set dependencies to latest released versions.\n * Step version to next release-version.\n * Make a commit C2.\n * Set dependencies to latest snapshots.\n * Step version to next snapshot-version.\n * Make a commit C3.\n * Try to push changes. If not successful: * Hard reset to C1.\n    * Pull.\n    * Start again with creating C2.\n    * Do this loop, perhaps 5 times\n      before giving up and fail.\n   \n   \n\nThis allows developers to work in the branches during the release-process.\n\nNow that we know we are in sync with remote Git repo on where to perform the\nrelease, we can continue doing so.\n\n * Tag C2  with the release-version.\n * Perform the build commands, mvn package  and loop any profiles needed.\n * Deploy in Maven repository, mvn deploy.\n\nWhen using Maven, you can do this with Versions Maven Plugin\n[http://www.mojohaus.org/versions-maven-plugin/].\n\nRelease orchestration\nThis job will:\n\n * Orchestrates a release.\n * Is parameterized with each repo.\n * When triggered: * Calculates the order to release selected repos. With the\n      information found in their Jenkinsfile:s.\n    * Invokes the release-jobs of each selected repo.\n   \n   \n\nFeatures\nHere is what features this setup can provide and how I intend them to be used.\n\nRelease\nA release of a single repo can be performed from its release-job.\n\nThis will look at each repo and release from the branch that is first found in\nthis order:\n\n 1. hotfix\n 2. release\n 3. develop\n 4. master\n\nSo if you want to release from a specific commit, not latest develop, just push\na release, or hotfix, -branch that points to that commit.\n\nOrchestrating a release\nA release of one, or more, repos can be performed from the global \nrelease-orchestration-job. This will:\n\n * Ensure the release of each repo * Is done in the right order.\n    * Their dependencies will be released first,\n      if selected.\n    * Use the latest release of their\n      dependencies.\n   \n   \n * After release, trigger the snapshot-job of the first repo that was released.\n   So that all the snapshot-jobs will run and step snapshot-versions.\n\nIt will invoke the release-jobs of each repo. This means you can have a look\nthere for more details on that specific release.\n\nHotfix\nHaving the priority among branches, mentioned above, will enable you to push a \nhotfix-branch from any commit and have the release being performed from that\ncommit. If your master  points to latest installed version:\n\n * git checkout master\n * git checkout -b hotfix\n * git push -u origin hotfix\n\nThen just trigger the release.\n\nAdvantages\nYour entire Jenkins configuration is put under version control. Well... you need\nto create one Job DSL -job manually that polls, or is triggered by changes in,\nthe git service. But that job can have its DSL in a Git-repo. This has a bunch\nof advantages.\n\n * No more browsing around in Jenkins and fiddling with settings.\n * You can track changes in the jobs. Just use git blame, it is all code now!\n * All your jobs are backed up with Git.\n * You can easily setup a development instance of Jenkins that behaves very much\n   like your production instance.\n * You can generate release-jobs in one Jenkins and snapshot jobs in another.\n   Letting only a few people use the release-jenkins and anyone use the other\n   instance.','/content/images/2018/02/superhero-1.png',0,0,'published',NULL,'public',NULL,NULL,'1','2018-02-03 06:24:23','1','2019-01-21 18:44:05','1','2018-02-03 06:39:55','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5ac3be89b907cf054a31895e','071a3b73-dfc8-40b7-8248-bcfa454ba56e','Jenkins Integration on Steroids','jenkins-integration-on-steroids','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Parameterized jobs in Jenkins is nothing new. But integrating with services like GitHub is much more than just accepting some *GET* parameters. [Generic Webhook Trigger](https://plugins.jenkins.io/generic-webhook-trigger) aims at simplifying triggering of jobs from complex *JSON*/*XML* webhook structures.\\n\\nI got the idea while working on the GitLab plugin. I [just needed a simple integer](https://github.com/jenkinsci/gitlab-plugin/pull/510) value from the GitLab webhook. After I opened [the PR](https://github.com/jenkinsci/gitlab-plugin/pull/510), it took 4+ months before it was merged and later released. Such a simple thing should not require that level of effort. But it is, and it is like that in other trigger plugins as well!\\n\\nThe idea with [Generic Webhook Trigger](https://plugins.jenkins.io/generic-webhook-trigger) is very simple.\\n\\n1. Post content is received.\\n2. Values are extracted from the received post content, using [JSONPath](https://github.com/json-path/JsonPath) or [XPath](https://www.w3schools.com/xml/xpath_intro.asp). And are contributed to the build with the configured variable names.\\n3. [Regular expression](https://jex.im/regulex/) is used to match the patterns that should trigger a build.\\n\\nThe webhooks provided by different services are often well documented. In contrast to all the different Jenkins plugins that consume them. That shows another advantage of using [Generic Webhook Trigger](https://plugins.jenkins.io/generic-webhook-trigger). You will be able to understand what you are doing. The alternative is to spend many frustrating hours with a bunch of very complex plugins.\\n\\nI think most Jenkins users just wants a way to consume a webhook. [This plugin](https://plugins.jenkins.io/generic-webhook-trigger) enables that and lets the users implement any use cases themselves. I think that is the best way to do it now that we have pipelines. Before pipelines there was a bigger need for complex plugins.\\n\\n## Simple use case\\n\\nLet\'s say we have a Git repository. It has some files in it, structured something like this:\\n```\\nsrc/main/java/...\\nsrc/test/java/...\\nsrc/main/resources/sql/...\\n```\\n\\n### Requirements\\n\\n* If the files in `src/main/resources/sql` is changed, then we want to run the SQL in a Docker to verify them. If these files are not changed, then we don\'t want to verify them. Perhaps it is a time consuming task and we want to avoid it.\\n* We are not allowed to use polling. We want instant builds and cannot have a large amount of Jenkins jobs polling our Git service every minute.\\n\\n### Implementation\\n\\nFirst step would be to setup the webhook in GitHub and point it to the running Jenkins instance.\\n\\n* Go to the webhooks page: https://github.com/tomasbjerre/you-repo-name/settings/hooks/\\n  * Set the *Content type* to `application/json`\\n  * Set the *Payload URL* to `http://JENKINS_URL/generic-webhook-trigger/invoke?token=abc123`\\n\\nNext step, create a job in Jenkins and configure [Generic Webhook Trigger](https://plugins.jenkins.io/generic-webhook-trigger). It supports Multibranch pipelines but I usually just create pipeline jobs using [Job DSL Plugin](https://github.com/jenkinsci/job-dsl-plugin/wiki). That is a bit off topic and covered in the documentation in GitHub.\\n\\n* Go to you Jenkins instance: http://localhost:8080/jenkins/\\n* Create a new job, perhaps Pipeline or Free Style.\\n* A new Trigger will appear, once [Generic Webhook Trigger](https://plugins.jenkins.io/generic-webhook-trigger) is installed, named \\\"*Generic Webhook Trigger*\\\". I\'m adding variable named `changed_files` with expression `$.commits[*].[\'modified\',\'added\',\'removed\'][*]`.\\n![generic_trigger_expr_files](/content/images/2018/04/generic_trigger_expr_files.png) You will probably want more variables here. Perhaps `$.ref` to get the branch name or `$.after` to get the commit that the ref points at after the change.\\n* Configure a token, this is how the invoker will know which job to trigger. If a job has a token configured then that token has to be supplied. If the job has no token configured then anyone can trigger the job.\\n![configure-token-1](/content/images/2018/08/configure-token-1.png)\\n* Add a filter with `text` like `$changed_files` and `expression` like `\\\"src/main/resources/sql/[^\\\"]+?\\\"`.\\n![generic_expr_folder](/content/images/2018/04/generic_expr_folder.png)  I\'m only using one variable here but you can resolve more variables. Add more variables to the text-field and have a more complex regular expression.\\n\\nThat is it. Save the job and it will trigger for changes in SQL-scripts and nothing else!\\n\\n### Explaination\\n\\nSo what happens in the example above? When a commit is pushed to GitHub, it will invoke the the configured URL with a webhook. In reality, there is [much more](https://developer.github.com/v3/activity/events/types/#pushevent) in the webhook then this:\\n```\\n{\\n \\\"ref\\\": \\\"refs/heads/changes\\\",\\n \\\"after\\\": \\\"0d1a26e67d8f5eaf1f6ba5c57fc3c7d91ac0fd1c\\\",\\n \\\"commits\\\": [\\n  {\\n   \\\"added\\\": [\\n    \\\".gitignore\\\"\\n   ],\\n   \\\"removed\\\": [\\n   ],\\n   \\\"modified\\\": [\\n    \\\"README.md\\\",\\n    \\\"src/main/resources/sql/some_script.sql\\\"\\n   ]\\n  }\\n ],\\n repository {\\n  \\\"git_url\\\": \\\"git://github.com/baxterthehacker/public-repo.git\\\"\\n }\\n}\\n```\\n\\nThe [JSONPath](https://github.com/json-path/JsonPath) used above will resolve to `[\\\".gitignore\\\", \\\"README.md\\\", \\\"src/main/resources/sql/some_script.sql\\\"]` and it will be available in a variable `$changed_files` (that can also be used later in the build).\\n\\nThe `text` field of the *Optional filter* will also resolve to this, because we used `$changed_files` in that field.\\n\\nThe `expression` field of the *Optional filter* will match `src/main/resources/sql/some_script.sql` which will trigger the build. Without that change, the build would not have been triggered.\\n\\nIf you also define variables like:\\n* **branch** with expression `$.ref`\\n* **clone_url** with expression `$.repository.git_url`\\n\\nThen you can have a shell script build step like:\\n```\\ngit clone $clone_url\\ngit checkout $branch\\n...\\n```\\n\\n## Further reading\\n\\nAlways turn to the wiki for the most accurate documentation:\\nhttps://plugins.jenkins.io/generic-webhook-trigger\\n\\nAny issues should be reported here:\\nhttps://github.com/jenkinsci/generic-webhook-trigger-plugin/issues\\n\"}]],\"markups\":[],\"sections\":[[10,0]]}','<p>Parameterized jobs in Jenkins is nothing new. But integrating with services like GitHub is much more than just accepting some <em>GET</em> parameters. <a href=\"https://plugins.jenkins.io/generic-webhook-trigger\">Generic Webhook Trigger</a> aims at simplifying triggering of jobs from complex <em>JSON</em>/<em>XML</em> webhook structures.</p>\n<p>I got the idea while working on the GitLab plugin. I <a href=\"https://github.com/jenkinsci/gitlab-plugin/pull/510\">just needed a simple integer</a> value from the GitLab webhook. After I opened <a href=\"https://github.com/jenkinsci/gitlab-plugin/pull/510\">the PR</a>, it took 4+ months before it was merged and later released. Such a simple thing should not require that level of effort. But it is, and it is like that in other trigger plugins as well!</p>\n<p>The idea with <a href=\"https://plugins.jenkins.io/generic-webhook-trigger\">Generic Webhook Trigger</a> is very simple.</p>\n<ol>\n<li>Post content is received.</li>\n<li>Values are extracted from the received post content, using <a href=\"https://github.com/json-path/JsonPath\">JSONPath</a> or <a href=\"https://www.w3schools.com/xml/xpath_intro.asp\">XPath</a>. And are contributed to the build with the configured variable names.</li>\n<li><a href=\"https://jex.im/regulex/\">Regular expression</a> is used to match the patterns that should trigger a build.</li>\n</ol>\n<p>The webhooks provided by different services are often well documented. In contrast to all the different Jenkins plugins that consume them. That shows another advantage of using <a href=\"https://plugins.jenkins.io/generic-webhook-trigger\">Generic Webhook Trigger</a>. You will be able to understand what you are doing. The alternative is to spend many frustrating hours with a bunch of very complex plugins.</p>\n<p>I think most Jenkins users just wants a way to consume a webhook. <a href=\"https://plugins.jenkins.io/generic-webhook-trigger\">This plugin</a> enables that and lets the users implement any use cases themselves. I think that is the best way to do it now that we have pipelines. Before pipelines there was a bigger need for complex plugins.</p>\n<h2 id=\"simpleusecase\">Simple use case</h2>\n<p>Let\'s say we have a Git repository. It has some files in it, structured something like this:</p>\n<pre><code>src/main/java/...\nsrc/test/java/...\nsrc/main/resources/sql/...\n</code></pre>\n<h3 id=\"requirements\">Requirements</h3>\n<ul>\n<li>If the files in <code>src/main/resources/sql</code> is changed, then we want to run the SQL in a Docker to verify them. If these files are not changed, then we don\'t want to verify them. Perhaps it is a time consuming task and we want to avoid it.</li>\n<li>We are not allowed to use polling. We want instant builds and cannot have a large amount of Jenkins jobs polling our Git service every minute.</li>\n</ul>\n<h3 id=\"implementation\">Implementation</h3>\n<p>First step would be to setup the webhook in GitHub and point it to the running Jenkins instance.</p>\n<ul>\n<li>Go to the webhooks page: <a href=\"https://github.com/tomasbjerre/you-repo-name/settings/hooks/\">https://github.com/tomasbjerre/you-repo-name/settings/hooks/</a>\n<ul>\n<li>Set the <em>Content type</em> to <code>application/json</code></li>\n<li>Set the <em>Payload URL</em> to <code>http://JENKINS_URL/generic-webhook-trigger/invoke?token=abc123</code></li>\n</ul>\n</li>\n</ul>\n<p>Next step, create a job in Jenkins and configure <a href=\"https://plugins.jenkins.io/generic-webhook-trigger\">Generic Webhook Trigger</a>. It supports Multibranch pipelines but I usually just create pipeline jobs using <a href=\"https://github.com/jenkinsci/job-dsl-plugin/wiki\">Job DSL Plugin</a>. That is a bit off topic and covered in the documentation in GitHub.</p>\n<ul>\n<li>Go to you Jenkins instance: <a href=\"http://localhost:8080/jenkins/\">http://localhost:8080/jenkins/</a></li>\n<li>Create a new job, perhaps Pipeline or Free Style.</li>\n<li>A new Trigger will appear, once <a href=\"https://plugins.jenkins.io/generic-webhook-trigger\">Generic Webhook Trigger</a> is installed, named &quot;<em>Generic Webhook Trigger</em>&quot;. I\'m adding variable named <code>changed_files</code> with expression <code>$.commits[*].[\'modified\',\'added\',\'removed\'][*]</code>.<br>\n<img src=\"/content/images/2018/04/generic_trigger_expr_files.png\" alt=\"generic_trigger_expr_files\"> You will probably want more variables here. Perhaps <code>$.ref</code> to get the branch name or <code>$.after</code> to get the commit that the ref points at after the change.</li>\n<li>Configure a token, this is how the invoker will know which job to trigger. If a job has a token configured then that token has to be supplied. If the job has no token configured then anyone can trigger the job.<br>\n<img src=\"/content/images/2018/08/configure-token-1.png\" alt=\"configure-token-1\"></li>\n<li>Add a filter with <code>text</code> like <code>$changed_files</code> and <code>expression</code> like <code>&quot;src/main/resources/sql/[^&quot;]+?&quot;</code>.<br>\n<img src=\"/content/images/2018/04/generic_expr_folder.png\" alt=\"generic_expr_folder\">  I\'m only using one variable here but you can resolve more variables. Add more variables to the text-field and have a more complex regular expression.</li>\n</ul>\n<p>That is it. Save the job and it will trigger for changes in SQL-scripts and nothing else!</p>\n<h3 id=\"explaination\">Explaination</h3>\n<p>So what happens in the example above? When a commit is pushed to GitHub, it will invoke the the configured URL with a webhook. In reality, there is <a href=\"https://developer.github.com/v3/activity/events/types/#pushevent\">much more</a> in the webhook then this:</p>\n<pre><code>{\n &quot;ref&quot;: &quot;refs/heads/changes&quot;,\n &quot;after&quot;: &quot;0d1a26e67d8f5eaf1f6ba5c57fc3c7d91ac0fd1c&quot;,\n &quot;commits&quot;: [\n  {\n   &quot;added&quot;: [\n    &quot;.gitignore&quot;\n   ],\n   &quot;removed&quot;: [\n   ],\n   &quot;modified&quot;: [\n    &quot;README.md&quot;,\n    &quot;src/main/resources/sql/some_script.sql&quot;\n   ]\n  }\n ],\n repository {\n  &quot;git_url&quot;: &quot;git://github.com/baxterthehacker/public-repo.git&quot;\n }\n}\n</code></pre>\n<p>The <a href=\"https://github.com/json-path/JsonPath\">JSONPath</a> used above will resolve to <code>[&quot;.gitignore&quot;, &quot;README.md&quot;, &quot;src/main/resources/sql/some_script.sql&quot;]</code> and it will be available in a variable <code>$changed_files</code> (that can also be used later in the build).</p>\n<p>The <code>text</code> field of the <em>Optional filter</em> will also resolve to this, because we used <code>$changed_files</code> in that field.</p>\n<p>The <code>expression</code> field of the <em>Optional filter</em> will match <code>src/main/resources/sql/some_script.sql</code> which will trigger the build. Without that change, the build would not have been triggered.</p>\n<p>If you also define variables like:</p>\n<ul>\n<li><strong>branch</strong> with expression <code>$.ref</code></li>\n<li><strong>clone_url</strong> with expression <code>$.repository.git_url</code></li>\n</ul>\n<p>Then you can have a shell script build step like:</p>\n<pre><code>git clone $clone_url\ngit checkout $branch\n...\n</code></pre>\n<h2 id=\"furtherreading\">Further reading</h2>\n<p>Always turn to the wiki for the most accurate documentation:<br>\n<a href=\"https://plugins.jenkins.io/generic-webhook-trigger\">https://plugins.jenkins.io/generic-webhook-trigger</a></p>\n<p>Any issues should be reported here:<br>\n<a href=\"https://github.com/jenkinsci/generic-webhook-trigger-plugin/issues\">https://github.com/jenkinsci/generic-webhook-trigger-plugin/issues</a></p>\n','5ac3be89b907cf054a31895e','Parameterized jobs in Jenkins is nothing new. But integrating with services like\nGitHub is much more than just accepting some GET  parameters. Generic Webhook\nTrigger [https://plugins.jenkins.io/generic-webhook-trigger]  aims at\nsimplifying triggering of jobs from complex JSON/XML  webhook structures.\n\nI got the idea while working on the GitLab plugin. I just needed a simple\ninteger [https://github.com/jenkinsci/gitlab-plugin/pull/510]  value from the\nGitLab webhook. After I opened the PR\n[https://github.com/jenkinsci/gitlab-plugin/pull/510], it took 4+ months before\nit was merged and later released. Such a simple thing should not require that\nlevel of effort. But it is, and it is like that in other trigger plugins as\nwell!\n\nThe idea with Generic Webhook Trigger\n[https://plugins.jenkins.io/generic-webhook-trigger]  is very simple.\n\n 1. Post content is received.\n 2. Values are extracted from the received post content, using JSONPath\n    [https://github.com/json-path/JsonPath]  or XPath\n    [https://www.w3schools.com/xml/xpath_intro.asp]. And are contributed to the\n    build with the configured variable names.\n 3. Regular expression [https://jex.im/regulex/]  is used to match the patterns\n    that should trigger a build.\n\nThe webhooks provided by different services are often well documented. In\ncontrast to all the different Jenkins plugins that consume them. That shows\nanother advantage of using Generic Webhook Trigger\n[https://plugins.jenkins.io/generic-webhook-trigger]. You will be able to\nunderstand what you are doing. The alternative is to spend many frustrating\nhours with a bunch of very complex plugins.\n\nI think most Jenkins users just wants a way to consume a webhook. This plugin\n[https://plugins.jenkins.io/generic-webhook-trigger]  enables that and lets the\nusers implement any use cases themselves. I think that is the best way to do it\nnow that we have pipelines. Before pipelines there was a bigger need for complex\nplugins.\n\nSimple use case\nLet\'s say we have a Git repository. It has some files in it, structured\nsomething like this:\n\nsrc/main/java/...\nsrc/test/java/...\nsrc/main/resources/sql/...\n\n\nRequirements\n * If the files in src/main/resources/sql  is changed, then we want to run the\n   SQL in a Docker to verify them. If these files are not changed, then we don\'t\n   want to verify them. Perhaps it is a time consuming task and we want to avoid\n   it.\n * We are not allowed to use polling. We want instant builds and cannot have a\n   large amount of Jenkins jobs polling our Git service every minute.\n\nImplementation\nFirst step would be to setup the webhook in GitHub and point it to the running\nJenkins instance.\n\n * Go to the webhooks page: \n   https://github.com/tomasbjerre/you-repo-name/settings/hooks/ * Set the Content type  to application/json\n    * Set the Payload URL  to \n      http://JENKINS_URL/generic-webhook-trigger/invoke?token=abc123\n   \n   \n\nNext step, create a job in Jenkins and configure Generic Webhook Trigger\n[https://plugins.jenkins.io/generic-webhook-trigger]. It supports Multibranch\npipelines but I usually just create pipeline jobs using Job DSL Plugin\n[https://github.com/jenkinsci/job-dsl-plugin/wiki]. That is a bit off topic and\ncovered in the documentation in GitHub.\n\n * Go to you Jenkins instance: http://localhost:8080/jenkins/\n * Create a new job, perhaps Pipeline or Free Style.\n * A new Trigger will appear, once Generic Webhook Trigger\n   [https://plugins.jenkins.io/generic-webhook-trigger]  is installed, named \"\n   Generic Webhook Trigger\". I\'m adding variable named changed_files  with\n   expression $.commits[*].[\'modified\',\'added\',\'removed\'][*].\n   You will probably want more variables here. Perhaps $.ref  to get the branch\n   name or $.after  to get the commit that the ref points at after the change.\n * Configure a token, this is how the invoker will know which job to trigger. If\n   a job has a token configured then that token has to be supplied. If the job\n   has no token configured then anyone can trigger the job.\n   \n * Add a filter with text  like $changed_files  and expression  like \n   \"src/main/resources/sql/[^\"]+?\".\n   I\'m only using one variable here but you can resolve more variables. Add more\n   variables to the text-field and have a more complex regular expression.\n\nThat is it. Save the job and it will trigger for changes in SQL-scripts and\nnothing else!\n\nExplaination\nSo what happens in the example above? When a commit is pushed to GitHub, it will\ninvoke the the configured URL with a webhook. In reality, there is much more  in\nthe webhook then this:\n\n{\n \"ref\": \"refs/heads/changes\",\n \"after\": \"0d1a26e67d8f5eaf1f6ba5c57fc3c7d91ac0fd1c\",\n \"commits\": [\n  {\n   \"added\": [\n    \".gitignore\"\n   ],\n   \"removed\": [\n   ],\n   \"modified\": [\n    \"README.md\",\n    \"src/main/resources/sql/some_script.sql\"\n   ]\n  }\n ],\n repository {\n  \"git_url\": \"git://github.com/baxterthehacker/public-repo.git\"\n }\n}\n\n\nThe JSONPath [https://github.com/json-path/JsonPath]  used above will resolve to\n [\".gitignore\", \"README.md\", \"src/main/resources/sql/some_script.sql\"]  and it\nwill be available in a variable $changed_files  (that can also be used later in\nthe build).\n\nThe text  field of the Optional filter  will also resolve to this, because we\nused $changed_files  in that field.\n\nThe expression  field of the Optional filter  will match \nsrc/main/resources/sql/some_script.sql  which will trigger the build. Without\nthat change, the build would not have been triggered.\n\nIf you also define variables like:\n\n * branch  with expression $.ref\n * clone_url  with expression $.repository.git_url\n\nThen you can have a shell script build step like:\n\ngit clone $clone_url\ngit checkout $branch\n...\n\n\nFurther reading\nAlways turn to the wiki for the most accurate documentation:\nhttps://plugins.jenkins.io/generic-webhook-trigger\n\nAny issues should be reported here:\nhttps://github.com/jenkinsci/generic-webhook-trigger-plugin/issues','/content/images/2018/04/superhero.png',0,0,'published',NULL,'public',NULL,NULL,'1','2018-04-03 17:48:57','1','2018-09-04 21:01:54','1','2018-04-03 17:51:44','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5b7be89c06392206949fa8a2','48fa21e5-97bb-4323-a45a-b32be5a634c9','Yet Another Kotlin VS Java Comparison','yet-another-kotlin-vs-java-comparison','{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently started developing an application with Kotlin. I found myself constantly wondering exactly what effect the Kotlin features have on the actual compiled classes. I was able to automatically generate the answers I needed.\\n\\nFirst of all, my solution is [here on GitHub](https://github.com/tomasbjerre/yet-another-kotlin-vs-java-comparison).\\n\\nWhat I do is:\\n\\n1. Write Kotlin code\\n2. Compile the Kotlin code\\n3. De-compile the resulting classes to Java code\\n4. Gather the original Kotlin and resulting Java code\\n5. Render markdown where I present this in a nice and tidy way\\n\\nThe input is only Kotlin and the output is a report telling me what I want to know.\\n\\nFeel free to [clone the repo](https://github.com/tomasbjerre/yet-another-kotlin-vs-java-comparison) and experiment yourself. I happily accept pull requests.\"}]],\"sections\":[[10,0]]}','<p>I recently started developing an application with Kotlin. I found myself constantly wondering exactly what effect the Kotlin features have on the actual compiled classes. I was able to automatically generate the answers I needed.</p>\n<p>First of all, my solution is <a href=\"https://github.com/tomasbjerre/yet-another-kotlin-vs-java-comparison\">here on GitHub</a>.</p>\n<p>What I do is:</p>\n<ol>\n<li>Write Kotlin code</li>\n<li>Compile the Kotlin code</li>\n<li>De-compile the resulting classes to Java code</li>\n<li>Gather the original Kotlin and resulting Java code</li>\n<li>Render markdown where I present this in a nice and tidy way</li>\n</ol>\n<p>The input is only Kotlin and the output is a report telling me what I want to know.</p>\n<p>Feel free to <a href=\"https://github.com/tomasbjerre/yet-another-kotlin-vs-java-comparison\">clone the repo</a> and experiment yourself. I happily accept pull requests.</p>\n','5b7be89c06392206949fa8a2','I recently started developing an application with Kotlin. I found myself\nconstantly wondering exactly what effect the Kotlin features have on the actual\ncompiled classes. I was able to automatically generate the answers I needed.\n\nFirst of all, my solution is here on GitHub\n[https://github.com/tomasbjerre/yet-another-kotlin-vs-java-comparison].\n\nWhat I do is:\n\n 1. Write Kotlin code\n 2. Compile the Kotlin code\n 3. De-compile the resulting classes to Java code\n 4. Gather the original Kotlin and resulting Java code\n 5. Render markdown where I present this in a nice and tidy way\n\nThe input is only Kotlin and the output is a report telling me what I want to\nknow.\n\nFeel free to clone the repo\n[https://github.com/tomasbjerre/yet-another-kotlin-vs-java-comparison]  and\nexperiment yourself. I happily accept pull requests.','/content/images/2018/08/kotlin_250x250.png',0,0,'published',NULL,'public',NULL,NULL,'1','2018-08-21 10:25:32','1','2018-08-21 10:38:20','1','2018-08-21 10:36:07','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5b858416d1c4a4085e025db0','a85ee22c-0147-4421-972f-bb075205b68e','Moving to IntelliJ IDEA from Eclipse','moving-to-intellij-idea-from-eclipse','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2018/08/intellij-settings.png\",\"caption\":\"<a>git@github.com</a>:tomasbjerre/intelij-settings.git\"}],[\"image\",{\"src\":\"/content/images/2018/08/intellij-maven-download-sources.png\",\"caption\":\"Intellij - Download sources and documentation\"}],[\"image\",{\"src\":\"/content/images/2018/08/intellij-optimize-imports-1.png\",\"caption\":\"Intellij - Optimize imports on the fly\"}],[\"image\",{\"src\":\"/content/images/2018/08/intellij-imports.png\",\"caption\":\"IntelliJ - Settings search\"}],[\"image\",{\"src\":\"/content/images/2018/08/intellij-imports-folder.png\",\"caption\":\"IntelliJ - Batch optimize imports\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://www.eclipse.org/downloads/\"]],[\"a\",[\"href\",\"https://github.com/eclipse\"]],[\"a\",[\"href\",\"https://www.jetbrains.com/idea/download/\"]],[\"a\",[\"href\",\"https://github.com/JetBrains/intellij-community\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/intelij-settings\"]],[\"a\",[\"href\",\"https://bjurr.com/java-code-formatting-with-google-java-format/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Being a long time \"],[0,[0],1,\"Eclipse\"],[0,[],0,\" user I recently started using \"],[0,[0],1,\"IntelliJ\"],[0,[],0,\". Here are some notes on how I use it for Java and Kotlin. I will probably update this post continuously.\"]]],[1,\"h1\",[[0,[],0,\"Comparison\"]]],[1,\"p\",[[0,[],0,\"Here are some of the topics that I thought were most interesting when doing the switch.\"]]],[1,\"h2\",[[0,[],0,\"Free and Open Source\"]]],[1,\"p\",[[0,[],0,\"I don\'t want to depend on software that is not free and open source. That, to me, feels like a trap and something I want to avoid. \"]]],[1,\"p\",[[0,[0],1,\"Eclipse\"],[0,[],0,\" is \"],[0,[1],1,\"free\"],[0,[],0,\" and \"],[0,[2],1,\"open source\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[0],1,\"IntelliJ\"],[0,[],0,\" has a \"],[0,[3],1,\"community edition\"],[0,[],0,\" that is also free and \"],[0,[4],1,\"open source\"],[0,[],0,\". This is the one I use.\"]]],[1,\"p\",[[0,[],0,\"This is totally fine for me. \"],[0,[5],1,\"Less is more\"],[0,[],0,\"!\"]]],[1,\"h2\",[[0,[],0,\"Synchronize Settings\"]]],[1,\"p\",[[0,[],0,\"I want to automatically have my IDE setup exactly how I want it and synchronize that to all my installations. Avoiding the time consuming task of searching for settings and clicking checkboxes.\"]]],[1,\"p\",[[0,[],0,\"In\"],[0,[0],1,\" Eclipse\"],[0,[],0,\" I use Workspace Mechanics. The mechanics files are put under version control so that all developers of a project will have changes applied immediately when someone changes the settings.\"]]],[1,\"p\",[[0,[],0,\"In\"],[0,[0],1,\" IntelliJ\"],[0,[],0,\" this is built in. In the menu, \"],[0,[5],1,\"File -> Settings Repository\"],[0,[],0,\" you can add any git-repository. I just created a new \"],[0,[6],1,\"repository on GitHub\"],[0,[],0,\" and is now using that clone URL. To publish settings you do \"],[0,[5],1,\"File -> Settings Repository\"],[0,[],0,\" and click \"],[0,[5],1,\"Merge\"],[0,[],0,\".\"]]],[10,0],[1,\"p\",[[0,[],0,\"I have seen that some settings are not synchronized when using \"],[0,[0],1,\"IntelliJ\"],[0,[],0,\". I really miss is \"],[0,[5],1,\"automatically download sources\"],[0,[],0,\" and \"],[0,[5],1,\"documentation\"],[0,[],0,\" and most annoyingly is the option to \"],[0,[5],1,\"optimize imports on the fly\"],[0,[],0,\".\"]]],[10,1],[10,2],[1,\"h2\",[[0,[],0,\"Opening Several Projects In Same Window\"]]],[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"Eclipse\"],[0,[],0,\" you would create a workspace and import projects in that workspace.\"]]],[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"IntelliJ\"],[0,[],0,\" you create a project and add modules to it. A module can be a Maven, or Gradle, project. You do:\"]]],[3,\"ul\",[[[0,[5],1,\"File -> New -> Project ...\"]],[[0,[5],1,\"Empty project\"]],[[0,[],0,\"Name it anything you like\"]],[[0,[],0,\"Finnish\"]]]],[1,\"p\",[[0,[],0,\"Now to add a Gradle project to it, you do:\"]]],[3,\"ul\",[[[0,[],0,\"File -> New -> Module From Existing Sources...\"]],[[0,[],0,\"Select the \"],[0,[5],1,\"build.gradle\"],[0,[],0,\" file. If it were Maven you would select the \"],[0,[5],1,\"pom.xml\"],[0,[],0,\" file\"]]]],[1,\"h2\",[[0,[],0,\"Maintaining a Code Standard\"]]],[1,\"p\",[[0,[],0,\"In\"],[0,[0],1,\" Eclipse\"],[0,[],0,\" you can setup save actions that will apply the code standard whenever a file is saved.\"]]],[1,\"p\",[[0,[],0,\"In\"],[0,[0],1,\" IntelliJ\"],[0,[],0,\" I have not found any way of enforcing a code standard. It does a good job on understanding and adhering to how the current file already is formatted.\"]]],[1,\"h2\",[[0,[],0,\"Automatically Boost Code Quality\"]]],[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"Eclipse\"],[0,[],0,\" I would setup save actions to automatically do things like:\"]]],[3,\"ul\",[[[0,[],0,\"Remove unused variables\"]],[[0,[],0,\"Organize imports, sorting and removing unused imports\"]],[[0,[],0,\"Adding final keyword to immutable attributes and variables\"]],[[0,[],0,\"Remove trailing white space\"]]]],[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"IntelliJ\"],[0,[],0,\" you can find these settings by simply searching in the settings menu (\"],[0,[5],1,\"File -> Settings\"],[0,[],0,\").\"]]],[10,3],[1,\"p\",[[0,[],0,\"This will organize the imports when you are working with a file. You can also batch this by marking a folder in the menu and press \"],[0,[0],1,\"CTRL-ALT-O\"],[0,[],0,\".\"]]],[10,4],[1,\"h1\",[[0,[],0,\"Conclusions\"]]],[1,\"p\",[[0,[],0,\"I am missing some features in \"],[0,[0],1,\"IntelliJ\"],[0,[],0,\" regarding automatically correcting things in the code. But that is easily solved with Maven or Gradle. I wrote about that \"],[0,[7],1,\"here\"],[0,[],0,\".\"]]]]}','<p>Being a long time <strong>Eclipse</strong> user I recently started using <strong>IntelliJ</strong>. Here are some notes on how I use it for Java and Kotlin. I will probably update this post continuously.</p><h1 id=\"comparison\">Comparison</h1><p>Here are some of the topics that I thought were most interesting when doing the switch.</p><h2 id=\"free-and-open-source\">Free and Open Source</h2><p>I don\'t want to depend on software that is not free and open source. That, to me, feels like a trap and something I want to avoid. </p><p><strong>Eclipse</strong> is <a href=\"https://www.eclipse.org/downloads/\">free</a> and <a href=\"https://github.com/eclipse\">open source</a>.</p><p><strong>IntelliJ</strong> has a <a href=\"https://www.jetbrains.com/idea/download/\">community edition</a> that is also free and <a href=\"https://github.com/JetBrains/intellij-community\">open source</a>. This is the one I use.</p><p>This is totally fine for me. <em>Less is more</em>!</p><h2 id=\"synchronize-settings\">Synchronize Settings</h2><p>I want to automatically have my IDE setup exactly how I want it and synchronize that to all my installations. Avoiding the time consuming task of searching for settings and clicking checkboxes.</p><p>In<strong> Eclipse</strong> I use Workspace Mechanics. The mechanics files are put under version control so that all developers of a project will have changes applied immediately when someone changes the settings.</p><p>In<strong> IntelliJ</strong> this is built in. In the menu, <em>File -&gt; Settings Repository</em> you can add any git-repository. I just created a new <a href=\"https://github.com/tomasbjerre/intelij-settings\">repository on GitHub</a> and is now using that clone URL. To publish settings you do <em>File -&gt; Settings Repository</em> and click <em>Merge</em>.</p><figure class=\"kg-image-card\"><img src=\"/content/images/2018/08/intellij-settings.png\" class=\"kg-image\"><figcaption><a>git@github.com</a>:tomasbjerre/intelij-settings.git</figcaption></figure><p>I have seen that some settings are not synchronized when using <strong>IntelliJ</strong>. I really miss is <em>automatically download sources</em> and <em>documentation</em> and most annoyingly is the option to <em>optimize imports on the fly</em>.</p><figure class=\"kg-image-card\"><img src=\"/content/images/2018/08/intellij-maven-download-sources.png\" class=\"kg-image\"><figcaption>Intellij - Download sources and documentation</figcaption></figure><figure class=\"kg-image-card\"><img src=\"/content/images/2018/08/intellij-optimize-imports-1.png\" class=\"kg-image\"><figcaption>Intellij - Optimize imports on the fly</figcaption></figure><h2 id=\"opening-several-projects-in-same-window\">Opening Several Projects In Same Window</h2><p>In <strong>Eclipse</strong> you would create a workspace and import projects in that workspace.</p><p>In <strong>IntelliJ</strong> you create a project and add modules to it. A module can be a Maven, or Gradle, project. You do:</p><ul><li><em>File -&gt; New -&gt; Project ...</em></li><li><em>Empty project</em></li><li>Name it anything you like</li><li>Finnish</li></ul><p>Now to add a Gradle project to it, you do:</p><ul><li>File -&gt; New -&gt; Module From Existing Sources...</li><li>Select the <em>build.gradle</em> file. If it were Maven you would select the <em>pom.xml</em> file</li></ul><h2 id=\"maintaining-a-code-standard\">Maintaining a Code Standard</h2><p>In<strong> Eclipse</strong> you can setup save actions that will apply the code standard whenever a file is saved.</p><p>In<strong> IntelliJ</strong> I have not found any way of enforcing a code standard. It does a good job on understanding and adhering to how the current file already is formatted.</p><h2 id=\"automatically-boost-code-quality\">Automatically Boost Code Quality</h2><p>In <strong>Eclipse</strong> I would setup save actions to automatically do things like:</p><ul><li>Remove unused variables</li><li>Organize imports, sorting and removing unused imports</li><li>Adding final keyword to immutable attributes and variables</li><li>Remove trailing white space</li></ul><p>In <strong>IntelliJ</strong> you can find these settings by simply searching in the settings menu (<em>File -&gt; Settings</em>).</p><figure class=\"kg-image-card\"><img src=\"/content/images/2018/08/intellij-imports.png\" class=\"kg-image\"><figcaption>IntelliJ - Settings search</figcaption></figure><p>This will organize the imports when you are working with a file. You can also batch this by marking a folder in the menu and press <strong>CTRL-ALT-O</strong>.</p><figure class=\"kg-image-card\"><img src=\"/content/images/2018/08/intellij-imports-folder.png\" class=\"kg-image\"><figcaption>IntelliJ - Batch optimize imports</figcaption></figure><h1 id=\"conclusions\">Conclusions</h1><p>I am missing some features in <strong>IntelliJ</strong> regarding automatically correcting things in the code. But that is easily solved with Maven or Gradle. I wrote about that <a href=\"https://bjurr.com/java-code-formatting-with-google-java-format/\">here</a>.</p>','5b858416d1c4a4085e025db0','Being a long time Eclipse  user I recently started using IntelliJ. Here are some\nnotes on how I use it for Java and Kotlin. I will probably update this post\ncontinuously.\n\nComparison\nHere are some of the topics that I thought were most interesting when doing the\nswitch.\n\nFree and Open Source\nI don\'t want to depend on software that is not free and open source. That, to\nme, feels like a trap and something I want to avoid. \n\nEclipse  is free [https://www.eclipse.org/downloads/]  and open source\n[https://github.com/eclipse].\n\nIntelliJ  has a community edition [https://www.jetbrains.com/idea/download/] \nthat is also free and open source\n[https://github.com/JetBrains/intellij-community]. This is the one I use.\n\nThis is totally fine for me. Less is more!\n\nSynchronize Settings\nI want to automatically have my IDE setup exactly how I want it and synchronize\nthat to all my installations. Avoiding the time consuming task of searching for\nsettings and clicking checkboxes.\n\nIn  Eclipse  I use Workspace Mechanics. The mechanics files are put under\nversion control so that all developers of a project will have changes applied\nimmediately when someone changes the settings.\n\nIn  IntelliJ  this is built in. In the menu, File -> Settings Repository  you\ncan add any git-repository. I just created a new repository on GitHub\n[https://github.com/tomasbjerre/intelij-settings]  and is now using that clone\nURL. To publish settings you do File -> Settings Repository  and click Merge.\n\ngit@github.com:tomasbjerre/intelij-settings.gitI have seen that some settings\nare not synchronized when using IntelliJ. I really miss is automatically\ndownload sources  and documentation  and most annoyingly is the option to \noptimize imports on the fly.\n\nIntellij - Download sources and documentationIntellij - Optimize imports on the\nflyOpening Several Projects In Same Window\nIn Eclipse  you would create a workspace and import projects in that workspace.\n\nIn IntelliJ  you create a project and add modules to it. A module can be a\nMaven, or Gradle, project. You do:\n\n * File -> New -> Project ...\n * Empty project\n * Name it anything you like\n * Finnish\n\nNow to add a Gradle project to it, you do:\n\n * File -> New -> Module From Existing Sources...\n * Select the build.gradle  file. If it were Maven you would select the pom.xml \n   file\n\nMaintaining a Code Standard\nIn  Eclipse  you can setup save actions that will apply the code standard\nwhenever a file is saved.\n\nIn  IntelliJ  I have not found any way of enforcing a code standard. It does a\ngood job on understanding and adhering to how the current file already is\nformatted.\n\nAutomatically Boost Code Quality\nIn Eclipse  I would setup save actions to automatically do things like:\n\n * Remove unused variables\n * Organize imports, sorting and removing unused imports\n * Adding final keyword to immutable attributes and variables\n * Remove trailing white space\n\nIn IntelliJ  you can find these settings by simply searching in the settings\nmenu (File -> Settings).\n\nIntelliJ - Settings searchThis will organize the imports when you are working\nwith a file. You can also batch this by marking a folder in the menu and press \nCTRL-ALT-O.\n\nIntelliJ - Batch optimize importsConclusions\nI am missing some features in IntelliJ  regarding automatically correcting\nthings in the code. But that is easily solved with Maven or Gradle. I wrote\nabout that here\n[https://bjurr.com/java-code-formatting-with-google-java-format/].','/content/images/2018/08/intellij.jpeg',0,0,'published',NULL,'public',NULL,NULL,'1','2018-08-28 17:19:18','1','2018-08-30 18:48:40','1','2018-08-28 17:40:40','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5c5e7a20d32e6b08b8dfd508','509bafe1-df0e-4dd7-b07c-25d712659fe1','Jenkins Configuration as Code and GitLab','jenkins-configuration-as-code-and-gitlab','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-master.png\",\"caption\":\"A master branch with bleeding edge features\"}],[\"image\",{\"src\":\"/content/images/2019/02/jenkins-sandbox-state.png\",\"cardWidth\":\"\",\"caption\":\"Different projects can use different parents of the bleeding edge master\"}],[\"embed\",{\"url\":\"https://youtu.be/3R39J-6SjV8\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/3R39J-6SjV8?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\"}]],\"markups\":[[\"a\",[\"href\",\"https://jenkins.io/projects/jcasc/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/\"]],[\"a\",[\"href\",\"https://github.com/jenkinsci/job-dsl-plugin/wiki\"]],[\"a\",[\"href\",\"https://jenkins.io/doc/book/pipeline/\"]],[\"a\",[\"href\",\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I used \"],[0,[0],1,\"Jenkins Configuration as Code\"],[0,[],0,\", \"],[0,[1],1,\"JCasC\"],[0,[],0,\", and \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to demonstrate how Jenkins and its configuration (\"],[0,[3],1,\"jobs\"],[0,[],0,\", \"],[0,[4],1,\"build process\"],[0,[],0,\" and \"],[0,[0],1,\"global configuration\"],[0,[],0,\") can be version controlled and fully automated. I also integrated with \"],[0,[2],1,\"GitLab\"],[0,[],0,\" to show how \"],[0,[5],1,\"webhooks\"],[0,[],0,\" configuration can be automated to trigger the jobs in Jenkins.\"]]],[1,\"p\",[[0,[6],1,\"TL;DR:\"],[0,[],0,\" Here is the code: \"],[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:\"]]],[3,\"ul\",[[[0,[],0,\"You don\'t know how an \"],[0,[6],1,\"update of a plugin\"],[0,[],0,\" affects all the jobs.\"]],[[0,[],0,\"Hard to keep track of what \"],[0,[6],1,\"plugins are actually needed\"],[0,[],0,\".\"]],[[0,[],0,\"Hard to keep track of \"],[0,[6],1,\"what jobs are still needed\"],[0,[],0,\". Git repositories changes name, gets removed, gets abandoned...\"]],[[0,[],0,\"Hard to \"],[0,[6],1,\"support developers\"],[0,[],0,\" because the way the different projects are using Jenkins \"],[0,[6],1,\"diverge\"],[0,[],0,\".\"]],[[0,[],0,\"Build scripts are \"],[0,[6],1,\"deployed in production for everyone\"],[0,[],0,\" at the same time.\"]],[[0,[],0,\"Blocks innovation because you \"],[0,[6],1,\"will not have confidence\"],[0,[],0,\" enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"revert changes\"],[0,[],0,\" made to the Jenkins installation.\"]],[[0,[],0,\"Cannot, easily, \"],[0,[6],1,\"work locally\"],[0,[],0,\" with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.\"]]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that \"],[0,[0],1,\"JCasC\"],[0,[],0,\" is released we have all parts need. \"]]],[3,\"ul\",[[[0,[],0,\"I have a \"],[0,[1],1,\"master \"],[0,[],0,\"branch in Git that represents the \"],[0,[1],1,\"bleeding edge\"],[0,[],0,\" setup with the latest features.\"]],[[0,[],0,\"Each project have their own branch in Git, pointing to their setup.\"]]]],[1,\"p\",[[0,[],0,\"Each project have their own installation of Jenkins. They can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the \"],[0,[1],1,\"master\"],[0,[],0,\" branch.\"]]],[10,0],[1,\"p\",[[0,[],0,\"At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here we see that:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[6],1,\"Master\"],[0,[],0,\" configuration points to Git commit \"],[0,[6],1,\"G\"],[0,[],0,\". Which means \"],[0,[6],1,\"G\"],[0,[],0,\" is the bleeding edge with the latest greatest features.\"]],[[0,[6],1,\"Project E\"],[0,[],0,\" is an early adopter and is only one commit behind the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[6],1,\"Project D\"],[0,[],0,\" is far behind.\"]]]],[1,\"p\",[[0,[],0,\"In this snapshot, perhaps, the people in \"],[0,[6],1,\"Project D\"],[0,[],0,\" may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in \"],[0,[6],1,\"Project E\"],[0,[],0,\" seems to be more enthusiastic and wants the latest features.\"]]],[1,\"p\",[[0,[],0,\"This solution has a bunch of advantages:\"]]],[3,\"ul\",[[[0,[],0,\"It \"],[0,[6],1,\"encourages innovation\"],[0,[],0,\". Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the \"],[0,[6],1,\"Master\"],[0,[],0,\".\"]],[[0,[],0,\"It \"],[0,[6],1,\"reduces time spent on support\"],[0,[],0,\". When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if \"],[0,[6],1,\"Project C\"],[0,[],0,\" has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the \"],[0,[6],1,\"Master\"],[0,[],0,\" project.\"]],[[0,[],0,\"Gives \"],[0,[6],1,\"self confidence to refactor\"],[0,[],0,\" scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.\"]],[[0,[],0,\"Allows \"],[0,[6],1,\"revert of failed upgrades\"],[0,[],0,\". If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.\"]],[[0,[6],1,\"Nice and tidy structuring of jobs\"],[0,[],0,\". You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.\"]]]],[10,2],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"The code, the Git repository containing the entire Jenkins setup, is available here:\"]]],[3,\"ul\",[[[0,[7],1,\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\"]]]],[1,\"p\",[]]]}','<p>I used <a href=\"https://jenkins.io/projects/jcasc/\">Jenkins Configuration as Code</a>, <em>JCasC</em>, and <a href=\"https://docs.gitlab.com/ce/\">GitLab</a> to demonstrate how Jenkins and its configuration (<a href=\"https://github.com/jenkinsci/job-dsl-plugin/wiki\">jobs</a>, <a href=\"https://jenkins.io/doc/book/pipeline/\">build process</a> and <a href=\"https://jenkins.io/projects/jcasc/\">global configuration</a>) can be version controlled and fully automated. I also integrated with <a href=\"https://docs.gitlab.com/ce/\">GitLab</a> to show how <a href=\"https://docs.gitlab.com/ce/user/project/integrations/webhooks.html\">webhooks</a> configuration can be automated to trigger the jobs in Jenkins.</p><p><strong>TL;DR:</strong> Here is the code: <a href=\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\">https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox</a></p><h2 id=\"the-problem\">The Problem</h2><p>When I started with this I was at a client who had several large projects. Each project with several developers working full time. They all developed different systems. They all shared one Jenkins installation with a vast amount of jobs. Some of the problems with this setup are:</p><ul><li>You don\'t know how an <strong>update of a plugin</strong> affects all the jobs.</li><li>Hard to keep track of what <strong>plugins are actually needed</strong>.</li><li>Hard to keep track of <strong>what jobs are still needed</strong>. Git repositories changes name, gets removed, gets abandoned...</li><li>Hard to <strong>support developers</strong> because the way the different projects are using Jenkins <strong>diverge</strong>.</li><li>Build scripts are <strong>deployed in production for everyone</strong> at the same time.</li><li>Blocks innovation because you <strong>will not have confidence</strong> enough to refactor build scripts. You will instead just make smallest possible changes to global scripts and pray that it works for everyone.</li><li>Cannot, easily, <strong>revert changes</strong> made to the Jenkins installation.</li><li>Cannot, easily, <strong>work locally</strong> with the Jenkins installation. Cannot work in feature branches, just like you would in any other application repository.</li></ul><h2 id=\"the-solution\">The Solution</h2><p>My solution is to express the entire Jenkins setup with scripts. Scripts that are version controlled in a Git repository. Now that <a href=\"https://jenkins.io/projects/jcasc/\">JCasC</a> is released we have all parts need. </p><ul><li>I have a <em>master </em>branch in Git that represents the <em>bleeding edge</em> setup with the latest features.</li><li>Each project have their own branch in Git, pointing to their setup.</li></ul><p>Each project have their own installation of Jenkins. They can decide for themselves when they want to follow the road map and get the latest features. The road map is the path, a bunch of commits in Git, between the projects current branch and the <em>master</em> branch.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/02/jenkins-sandbox-master.png\" class=\"kg-image\"><figcaption>A master branch with bleeding edge features</figcaption></figure><p>At some point in time a snapshot of the current state, of the Jenkins configuration Git repository, may look like this.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/02/jenkins-sandbox-state.png\" class=\"kg-image\"><figcaption>Different projects can use different parents of the bleeding edge master</figcaption></figure><p>Here we see that:</p><ul><li>The <strong>Master</strong> configuration points to Git commit <strong>G</strong>. Which means <strong>G</strong> is the bleeding edge with the latest greatest features.</li><li><strong>Project E</strong> is an early adopter and is only one commit behind the <strong>Master</strong>.</li><li><strong>Project D</strong> is far behind.</li></ul><p>In this snapshot, perhaps, the people in <strong>Project D</strong> may not have much interest in CI/CD. They just want it to do its thing while they focus on their application code. The people in <strong>Project E</strong> seems to be more enthusiastic and wants the latest features.</p><p>This solution has a bunch of advantages:</p><ul><li>It <strong>encourages innovation</strong>. Anyone, in any project, can create their own feature branch in the Git repository to try any idea they might have. They can, easily, setup the entire Jenkins installation locally to fiddle with it. Develop scripts and try ideas. Even push the feature branch to a remote and suggest features to be included in the <strong>Master</strong>.</li><li>It <strong>reduces time spent on support</strong>. When the way the different projects use Jenkins does not diverge, their can be documentation that all projects can use. It also means that if <strong>Project C</strong> has a problem, they can talk to any member of any of the other projects. They don\'t necessarily need to talk to a member of the <strong>Master</strong> project.</li><li>Gives <strong>self confidence to refactor</strong> scripts. Ideas can be tested locally without having to be deployed in a shared Jenkins instance.</li><li>Allows <strong>revert of failed upgrades</strong>. If a project upgrades and something goes wrong, you simply revert the change in Git and you will be back to where you were before the upgrade.</li><li><strong>Nice and tidy structuring of jobs</strong>. You can organize the Jenkins jobs in folders to create a structure that maps to the structure you have in your Git service. Like in GitLab you have namespaces and I have a Jenkins folder for each namespace.</li></ul><figure class=\"kg-card kg-embed-card\"><iframe width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/3R39J-6SjV8?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><h2 id=\"implementation\">Implementation</h2><p>The code, the Git repository containing the entire Jenkins setup, is available here:</p><ul><li><a href=\"https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\">https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox</a></li></ul>','5c5e7a20d32e6b08b8dfd508','I used Jenkins Configuration as Code [https://jenkins.io/projects/jcasc/], JCasC\n, and GitLab [https://docs.gitlab.com/ce/]  to demonstrate how Jenkins and its\nconfiguration (jobs [https://github.com/jenkinsci/job-dsl-plugin/wiki], build\nprocess [https://jenkins.io/doc/book/pipeline/]  and global configuration\n[https://jenkins.io/projects/jcasc/]) can be version controlled and fully\nautomated. I also integrated with GitLab [https://docs.gitlab.com/ce/]  to show\nhow webhooks\n[https://docs.gitlab.com/ce/user/project/integrations/webhooks.html] \nconfiguration can be automated to trigger the jobs in Jenkins.\n\nTL;DR:  Here is the code: \nhttps://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox\n\nThe Problem\nWhen I started with this I was at a client who had several large projects. Each\nproject with several developers working full time. They all developed different\nsystems. They all shared one Jenkins installation with a vast amount of jobs.\nSome of the problems with this setup are:\n\n * You don\'t know how an update of a plugin  affects all the jobs.\n * Hard to keep track of what plugins are actually needed.\n * Hard to keep track of what jobs are still needed. Git repositories changes\n   name, gets removed, gets abandoned...\n * Hard to support developers  because the way the different projects are using\n   Jenkins diverge.\n * Build scripts are deployed in production for everyone  at the same time.\n * Blocks innovation because you will not have confidence  enough to refactor\n   build scripts. You will instead just make smallest possible changes to global\n   scripts and pray that it works for everyone.\n * Cannot, easily, revert changes  made to the Jenkins installation.\n * Cannot, easily, work locally  with the Jenkins installation. Cannot work in\n   feature branches, just like you would in any other application repository.\n\nThe Solution\nMy solution is to express the entire Jenkins setup with scripts. Scripts that\nare version controlled in a Git repository. Now that JCasC\n[https://jenkins.io/projects/jcasc/]  is released we have all parts need. \n\n * I have a master branch in Git that represents the bleeding edge  setup with\n   the latest features.\n * Each project have their own branch in Git, pointing to their setup.\n\nEach project have their own installation of Jenkins. They can decide for\nthemselves when they want to follow the road map and get the latest features.\nThe road map is the path, a bunch of commits in Git, between the projects\ncurrent branch and the master  branch.\n\nA master branch with bleeding edge featuresAt some point in time a snapshot of\nthe current state, of the Jenkins configuration Git repository, may look like\nthis.\n\nDifferent projects can use different parents of the bleeding edge masterHere we\nsee that:\n\n * The Master  configuration points to Git commit G. Which means G  is the\n   bleeding edge with the latest greatest features.\n * Project E  is an early adopter and is only one commit behind the Master.\n * Project D  is far behind.\n\nIn this snapshot, perhaps, the people in Project D  may not have much interest\nin CI/CD. They just want it to do its thing while they focus on their\napplication code. The people in Project E  seems to be more enthusiastic and\nwants the latest features.\n\nThis solution has a bunch of advantages:\n\n * It encourages innovation. Anyone, in any project, can create their own\n   feature branch in the Git repository to try any idea they might have. They\n   can, easily, setup the entire Jenkins installation locally to fiddle with it.\n   Develop scripts and try ideas. Even push the feature branch to a remote and\n   suggest features to be included in the Master.\n * It reduces time spent on support. When the way the different projects use\n   Jenkins does not diverge, their can be documentation that all projects can\n   use. It also means that if Project C  has a problem, they can talk to any\n   member of any of the other projects. They don\'t necessarily need to talk to a\n   member of the Master  project.\n * Gives self confidence to refactor  scripts. Ideas can be tested locally\n   without having to be deployed in a shared Jenkins instance.\n * Allows revert of failed upgrades. If a project upgrades and something goes\n   wrong, you simply revert the change in Git and you will be back to where you\n   were before the upgrade.\n * Nice and tidy structuring of jobs. You can organize the Jenkins jobs in\n   folders to create a structure that maps to the structure you have in your Git\n   service. Like in GitLab you have namespaces and I have a Jenkins folder for\n   each namespace.\n\nImplementation\nThe code, the Git repository containing the entire Jenkins setup, is available\nhere:\n\n * https://github.com/tomasbjerre/jenkins-configuration-as-code-sandbox','/content/images/2019/02/superhero.png',0,0,'published',NULL,'public',NULL,NULL,'1','2019-02-09 06:58:40','1','2019-02-10 06:52:55','1','2019-02-09 07:07:58','1','I used Jenkins Configuration as Code and GitLab to demonstrate how Jenkins and its configuration (jobs, build process and global configuration) can be version controlled and fully automated.','','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5cc35015bbdcf706bbdbc74f','52983f8e-2e1c-46f5-b672-59a1b23b4a40','Automatic stubs with Wiremock and JAX-RS','automatic-stubs-with-wiremock','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```java\\n@Path(\\\"/\\\")\\npublic interface ExampleResource {\\n  @GET\\n  @Path(\\\"/get\\\")\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public ItemDTO getItem(@QueryParam(\\\"filter1\\\") String filter1);\\n\\n  @POST\\n  @Path(\\\"/create\\\")\\n  @Consumes(MediaType.APPLICATION_JSON)\\n  @Produces(MediaType.APPLICATION_JSON)\\n  public void createItem(ItemDTO item);\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```java\\nstubFor(\\n   invocation(ExampleResource.class, (r) -> r.getItem(\\\"abc\\\")) //\\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\\n\\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.getItem(itemDto.getAttr1())) \\n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\\n\\n  stubFor(\\n     invocation(ExampleResource.class, (r) -> r.createItem(itemDto)) //\\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\\n}\\n```\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/wiremock-jaxrs\"]],[\"a\",[\"href\",\"http://wiremock.org/docs/stubbing/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I often see \"],[0,[0],1,\"mock\"],[0,[],0,\", or \"],[0,[0],1,\"stub\"],[0,[],0,\", servers being configured in frontend projects. I see hard coded \"],[0,[0],1,\"JSON\"],[0,[],0,\" files that are very likely to not match the true responses of the actual API. Here is how I automate creation of a type safe, reliable, stub server in a runnable, self contained, \"],[0,[0],1,\"JAR\"],[0,[],0,\". Packaged whenever the \"],[0,[0],1,\"API\"],[0,[],0,\" is packaged.\"]]],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A fully working example is implemented here, with Maven:\"],[1,[],0,0],[0,[1],1,\"https://github.com/tomasbjerre/wiremock-jaxrs-example\"]]],[1,\"p\",[[0,[],0,\"I have a multi module Maven setup with these modules:\"]]],[3,\"ul\",[[[0,[2],1,\"module\"],[0,[],0,\" - Parent pom.\"]],[[0,[2],1,\"module-api\"],[0,[],0,\" - JAX-RS annotated API. Very slim API jar.\"]],[[0,[2],1,\"module-api-mock\"],[0,[],0,\" - Runnable mock-server based on \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[0],1,\"module-api-mock\"],[0,[],0,\" I create a test case that produces the configuration of the stub. I use a test case just because that is an easy way to get some code to run during the build.\"]]],[1,\"p\",[[0,[],0,\"I use \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\" to automatically configure the stub. By looking at how may \"],[0,[0],1,\"JAX-RS\"],[0,[],0,\" resource is annotated.\"]]],[1,\"p\",[[0,[],0,\"My resource looks something like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"And I configure the stub something like this:\"]]],[10,1],[1,\"p\",[[0,[],0,\"In this example the \"],[0,[0],1,\"invocation(...)\"],[0,[],0,\" comes from \"],[0,[3],1,\"Wiremock JAX-RS\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"This will make \"],[0,[4],0,\"Wiremock produce \"],[0,[0],2,\"JSON\"],[0,[],0,\". I then package Wiremock-standalone together with this \"],[0,[0],1,\"JSON\"],[0,[],0,\" into a self contained \"],[0,[0],1,\"JAR\"],[0,[],0,\". This \"],[0,[0],1,\"JAR\"],[0,[],0,\" can then be run from command line to get a reliable (type safe) mock server to use for other development, like frontend.\"]]],[1,\"p\",[[0,[],0,\"Something like this will give me a reliable server responding with correct headers and datastructures:\"]]],[1,\"p\",[[0,[0],1,\"java -jar mymodule-api-mock.jar\"]]],[1,\"h1\",[[0,[],0,\"Advantages\"]]],[1,\"p\",[[0,[],0,\"The stubs are created with the same language, Java, as the service is created in. Convenient if you are a Java developer. While still the result is \"],[0,[0],1,\"JSON\"],[0,[],0,\", convenient for frontend developers.\"]]],[1,\"p\",[[0,[],0,\"If the \"],[0,[0],1,\"API\"],[0,[],0,\" changes, the stub will automatically change. The stub will always match the \"],[0,[0],1,\"API\"],[0,[],0,\". Same types and same headers (accept, content type).\"]]],[1,\"h1\",[[0,[],0,\"What about Mockito?\"]]],[1,\"p\",[[0,[],0,\"You may also do this instead of something like Mockito to in the test cases.\"]]],[3,\"ul\",[[[0,[],0,\"Mockito will not test that datastructures serialize and deserialize.\"]],[[0,[],0,\"Will not test that you are using annotations in a \\\"sane\\\" way.\"]]]],[1,\"p\",[]]]}','<p>I often see <em>mock</em>, or <em>stub</em>, servers being configured in frontend projects. I see hard coded <em>JSON</em> files that are very likely to not match the true responses of the actual API. Here is how I automate creation of a type safe, reliable, stub server in a runnable, self contained, <em>JAR</em>. Packaged whenever the <em>API</em> is packaged.</p><h1 id=\"implementation\">Implementation</h1><p>A fully working example is implemented here, with Maven:<br><a href=\"https://github.com/tomasbjerre/wiremock-jaxrs-example\">https://github.com/tomasbjerre/wiremock-jaxrs-example</a></p><p>I have a multi module Maven setup with these modules:</p><ul><li><code>module</code> - Parent pom.</li><li><code>module-api</code> - JAX-RS annotated API. Very slim API jar.</li><li><code>module-api-mock</code> - Runnable mock-server based on <a href=\"https://github.com/tomasbjerre/wiremock-jaxrs\">Wiremock JAX-RS</a>.</li></ul><p>In the <em>module-api-mock</em> I create a test case that produces the configuration of the stub. I use a test case just because that is an easy way to get some code to run during the build.</p><p>I use <a href=\"https://github.com/tomasbjerre/wiremock-jaxrs\">Wiremock JAX-RS</a> to automatically configure the stub. By looking at how may <em>JAX-RS</em> resource is annotated.</p><p>My resource looks something like this:</p><pre><code class=\"language-java\">@Path(&quot;/&quot;)\npublic interface ExampleResource {\n  @GET\n  @Path(&quot;/get&quot;)\n  @Produces(MediaType.APPLICATION_JSON)\n  public ItemDTO getItem(@QueryParam(&quot;filter1&quot;) String filter1);\n\n  @POST\n  @Path(&quot;/create&quot;)\n  @Consumes(MediaType.APPLICATION_JSON)\n  @Produces(MediaType.APPLICATION_JSON)\n  public void createItem(ItemDTO item);\n}\n</code></pre>\n<p>And I configure the stub something like this:</p><pre><code class=\"language-java\">stubFor(\n   invocation(ExampleResource.class, (r) -&gt; r.getItem(&quot;abc&quot;)) //\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\n\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\n  stubFor(\n     invocation(ExampleResource.class, (r) -&gt; r.getItem(itemDto.getAttr1())) \n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\n\n  stubFor(\n     invocation(ExampleResource.class, (r) -&gt; r.createItem(itemDto)) //\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\n}\n</code></pre>\n<p>In this example the <em>invocation(...)</em> comes from <a href=\"https://github.com/tomasbjerre/wiremock-jaxrs\">Wiremock JAX-RS</a>.</p><p>This will make <a href=\"http://wiremock.org/docs/stubbing/\">Wiremock produce <em>JSON</em></a>. I then package Wiremock-standalone together with this <em>JSON</em> into a self contained <em>JAR</em>. This <em>JAR</em> can then be run from command line to get a reliable (type safe) mock server to use for other development, like frontend.</p><p>Something like this will give me a reliable server responding with correct headers and datastructures:</p><p><em>java -jar mymodule-api-mock.jar</em></p><h1 id=\"advantages\">Advantages</h1><p>The stubs are created with the same language, Java, as the service is created in. Convenient if you are a Java developer. While still the result is <em>JSON</em>, convenient for frontend developers.</p><p>If the <em>API</em> changes, the stub will automatically change. The stub will always match the <em>API</em>. Same types and same headers (accept, content type).</p><h1 id=\"what-about-mockito\">What about Mockito?</h1><p>You may also do this instead of something like Mockito to in the test cases.</p><ul><li>Mockito will not test that datastructures serialize and deserialize.</li><li>Will not test that you are using annotations in a \"sane\" way.</li></ul>','5cc35015bbdcf706bbdbc74f','I often see mock, or stub, servers being configured in frontend projects. I see\nhard coded JSON  files that are very likely to not match the true responses of\nthe actual API. Here is how I automate creation of a type safe, reliable, stub\nserver in a runnable, self contained, JAR. Packaged whenever the API  is\npackaged.\n\nImplementation\nA fully working example is implemented here, with Maven:\nhttps://github.com/tomasbjerre/wiremock-jaxrs-example\n\nI have a multi module Maven setup with these modules:\n\n * module  - Parent pom.\n * module-api  - JAX-RS annotated API. Very slim API jar.\n * module-api-mock  - Runnable mock-server based on Wiremock JAX-RS\n   [https://github.com/tomasbjerre/wiremock-jaxrs].\n\nIn the module-api-mock  I create a test case that produces the configuration of\nthe stub. I use a test case just because that is an easy way to get some code to\nrun during the build.\n\nI use Wiremock JAX-RS [https://github.com/tomasbjerre/wiremock-jaxrs]  to\nautomatically configure the stub. By looking at how may JAX-RS  resource is\nannotated.\n\nMy resource looks something like this:\n\n@Path(\"/\")\npublic interface ExampleResource {\n  @GET\n  @Path(\"/get\")\n  @Produces(MediaType.APPLICATION_JSON)\n  public ItemDTO getItem(@QueryParam(\"filter1\") String filter1);\n\n  @POST\n  @Path(\"/create\")\n  @Consumes(MediaType.APPLICATION_JSON)\n  @Produces(MediaType.APPLICATION_JSON)\n  public void createItem(ItemDTO item);\n}\n\n\nAnd I configure the stub something like this:\n\nstubFor(\n   invocation(ExampleResource.class, (r) -> r.getItem(\"abc\")) //\n      .willReturn(aResponse().withStatus(SC_NOT_FOUND)));\n\nfor (final ItemDTO itemDto : MockFactory.getAllItems()) {\n  stubFor(\n     invocation(ExampleResource.class, (r) -> r.getItem(itemDto.getAttr1())) \n        .willReturn(aResponse().withStatus(SC_ACCEPTED), itemDto));\n\n  stubFor(\n     invocation(ExampleResource.class, (r) -> r.createItem(itemDto)) //\n       .willReturn(aResponse().withStatus(SC_ACCEPTED)));\n}\n\n\nIn this example the invocation(...)  comes from Wiremock JAX-RS\n[https://github.com/tomasbjerre/wiremock-jaxrs].\n\nThis will make Wiremock produce JSON [http://wiremock.org/docs/stubbing/]. I\nthen package Wiremock-standalone together with this JSON  into a self contained \nJAR. This JAR  can then be run from command line to get a reliable (type safe)\nmock server to use for other development, like frontend.\n\nSomething like this will give me a reliable server responding with correct\nheaders and datastructures:\n\njava -jar mymodule-api-mock.jar\n\nAdvantages\nThe stubs are created with the same language, Java, as the service is created\nin. Convenient if you are a Java developer. While still the result is JSON,\nconvenient for frontend developers.\n\nIf the API  changes, the stub will automatically change. The stub will always\nmatch the API. Same types and same headers (accept, content type).\n\nWhat about Mockito?\nYou may also do this instead of something like Mockito to in the test cases.\n\n * Mockito will not test that datastructures serialize and deserialize.\n * Will not test that you are using annotations in a \"sane\" way.','/content/images/2019/04/wm.png',0,0,'published',NULL,'public',NULL,NULL,'1','2019-04-26 18:38:13','1','2019-04-27 04:50:13','1','2019-04-26 18:40:58','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5cd9a5ee52156f04cb9dc69e','2b17b724-d580-4060-9d3c-3e5653609d75','Dependents of Maven artifacts','dependents-of-maven-artifacts','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Have a browsable website, that is automatically updated, that shows:\\n    * What artifacts are being produced within an organization.\\n    * Their effective dependencies, including transitives.\\n    * Their dependents, artifacts that have them as dependency.\\n\\n* Have, instantly updated, lists of dependents of each artifact. So that:\\n    * Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.\\n    * Where, probably Git reposiroties, to find these dependent artifacts.\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-downloader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]],[\"a\",[\"href\",\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.\"]]],[1,\"h1\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"I developed 3 tools:\"]]],[3,\"ul\",[[[0,[],0,\"Pom Downloader\"],[1,[],0,0],[0,[0],1,\"https://github.com/tomasbjerre/pom-downloader\"],[1,[],0,1],[0,[],0,\"Given a \"],[0,[1],1,\"groupId\"],[0,[],0,\", it will download all \"],[0,[1],1,\"pom\"],[0,[],0,\"-files withing that group.\"]],[[0,[],0,\"Pom Dependency Analyzer\"],[1,[],0,2],[0,[2],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer\"],[1,[],0,3],[0,[],0,\"Will analyze a \"],[0,[1],1,\"pom\"],[0,[],0,\"-file and output the effective \"],[0,[1],1,\"dependencies\"],[0,[],0,\", \"],[0,[1],1,\"groupId\"],[0,[],0,\", \"],[0,[1],1,\"artifactId\"],[0,[],0,\", \"],[0,[1],1,\"version\"],[0,[],0,\". Also update all \"],[0,[1],1,\"dependencies\"],[0,[],0,\" to have its \"],[0,[1],1,\"dependents\"],[0,[],0,\" refer to the \"],[0,[1],1,\"dependency\"],[0,[],0,\".\"]],[[0,[],0,\"Pom Dependency Analyzer Web\"],[1,[],0,4],[0,[3],1,\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\"],[1,[],0,5],[0,[],0,\"Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: \"],[0,[4],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"]]]],[1,\"h1\",[[0,[],0,\"Usage\"]]],[1,\"p\",[[0,[],0,\"Check the \"],[0,[1],1,\"README:s\"],[0,[],0,\" of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Downloader in a scheduled job\"]],[[0,[],0,\"Running Pom Dependency Analyzer on each pom downloaded from first step\"]],[[0,[],0,\"Running Pom Dependency Analyzer Web on the output of second step\"]]]],[1,\"p\",[[0,[],0,\"The second use case is solved by:\"]]],[3,\"ul\",[[[0,[],0,\"Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.\"]]]],[1,\"p\",[[0,[],0,\"I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.\"]]],[1,\"h1\",[[0,[],0,\"Example\"]]],[1,\"p\",[[0,[],0,\"I have my example running here: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\"],[1,[],0,6],[0,[],0,\"So that you can view dependents of a specific version like this: \"],[0,[5],1,\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\"]]],[1,\"p\",[]]]}','<p>Finding out dependencies of an artifact is easy, but what about dependents? Who is depending on the artifact? That is a common problem when working with Java. The answer is needed in order to, among other things, automate build flows. This is one way of doing it that will work with anything (Maven, Gradle, ...) that is published to a Maven repository like Nexus or Bintray.</p><h1 id=\"overview\">Overview</h1><p>I developed 3 tools:</p><ul><li>Pom Downloader<br><a href=\"https://github.com/tomasbjerre/pom-downloader\">https://github.com/tomasbjerre/pom-downloader</a><br>Given a <em>groupId</em>, it will download all <em>pom</em>-files withing that group.</li><li>Pom Dependency Analyzer<br><a href=\"https://github.com/tomasbjerre/pom-dependency-analyzer\">https://github.com/tomasbjerre/pom-dependency-analyzer</a><br>Will analyze a <em>pom</em>-file and output the effective <em>dependencies</em>, <em>groupId</em>, <em>artifactId</em>, <em>version</em>. Also update all <em>dependencies</em> to have its <em>dependents</em> refer to the <em>dependency</em>.</li><li>Pom Dependency Analyzer Web<br><a href=\"https://github.com/tomasbjerre/pom-dependency-analyzer-web\">https://github.com/tomasbjerre/pom-dependency-analyzer-web</a><br>Will transform the output of Pom Dependency Analyzer to a static REST API that can be deployed on a static web server, like Github pages: <a href=\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/\">https://tomasbjerre.github.io/pom-dependency-analyzer-web/</a></li></ul><h1 id=\"usage\">Usage</h1><p>Check the <em>README:s</em> of each tool to get the accurate features and commands. I see 2 main use cases that I wanted to solve:</p><ul>\n<li>\n<p>Have a browsable website, that is automatically updated, that shows:</p>\n<ul>\n<li>What artifacts are being produced within an organization.</li>\n<li>Their effective dependencies, including transitives.</li>\n<li>Their dependents, artifacts that have them as dependency.</li>\n</ul>\n</li>\n<li>\n<p>Have, instantly updated, lists of dependents of each artifact. So that:</p>\n<ul>\n<li>Whenever an artifact is built, you can know what other artifacts needs to be verified against the newly changed artifact.</li>\n<li>Where, probably Git reposiroties, to find these dependent artifacts.</li>\n</ul>\n</li>\n</ul>\n<p>The first use case is solved by:</p><ul><li>Running Pom Downloader in a scheduled job</li><li>Running Pom Dependency Analyzer on each pom downloaded from first step</li><li>Running Pom Dependency Analyzer Web on the output of second step</li></ul><p>The second use case is solved by:</p><ul><li>Running Pom Dependency Analyzer on each newly produced artifact, snapshot-jobs and release-jobs.</li></ul><p>I\'m storing output of Pom Dependency Analyzer in a shared folder but you might want to store it in its own Git repository.</p><h1 id=\"example\">Example</h1><p>I have my example running here: <a href=\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\">https://tomasbjerre.github.io/pom-dependency-analyzer-web/</a><br>So that you can view dependents of a specific version like this: <a href=\"https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95\">https://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95</a></p>','5cd9a5ee52156f04cb9dc69e','Finding out dependencies of an artifact is easy, but what about dependents? Who\nis depending on the artifact? That is a common problem when working with Java.\nThe answer is needed in order to, among other things, automate build flows. This\nis one way of doing it that will work with anything (Maven, Gradle, ...) that is\npublished to a Maven repository like Nexus or Bintray.\n\nOverview\nI developed 3 tools:\n\n * Pom Downloader\n   https://github.com/tomasbjerre/pom-downloader\n   Given a groupId, it will download all pom-files withing that group.\n * Pom Dependency Analyzer\n   https://github.com/tomasbjerre/pom-dependency-analyzer\n   Will analyze a pom-file and output the effective dependencies, groupId, \n   artifactId, version. Also update all dependencies  to have its dependents \n   refer to the dependency.\n * Pom Dependency Analyzer Web\n   https://github.com/tomasbjerre/pom-dependency-analyzer-web\n   Will transform the output of Pom Dependency Analyzer to a static REST API\n   that can be deployed on a static web server, like Github pages: \n   https://tomasbjerre.github.io/pom-dependency-analyzer-web/\n\nUsage\nCheck the README:s  of each tool to get the accurate features and commands. I\nsee 2 main use cases that I wanted to solve:\n\n * Have a browsable website, that is automatically updated, that shows:\n   \n    * What artifacts are being produced within an organization.\n    * Their effective dependencies, including transitives.\n    * Their dependents, artifacts that have them as dependency.\n   \n   \n * Have, instantly updated, lists of dependents of each artifact. So that:\n   \n    * Whenever an artifact is built, you can know what other artifacts needs to\n      be verified against the newly changed artifact.\n    * Where, probably Git reposiroties, to find these dependent artifacts.\n   \n   \n\nThe first use case is solved by:\n\n * Running Pom Downloader in a scheduled job\n * Running Pom Dependency Analyzer on each pom downloaded from first step\n * Running Pom Dependency Analyzer Web on the output of second step\n\nThe second use case is solved by:\n\n * Running Pom Dependency Analyzer on each newly produced artifact,\n   snapshot-jobs and release-jobs.\n\nI\'m storing output of Pom Dependency Analyzer in a shared folder but you might\nwant to store it in its own Git repository.\n\nExample\nI have my example running here: \nhttps://tomasbjerre.github.io/pom-dependency-analyzer-web/\nSo that you can view dependents of a specific version like this: \nhttps://tomasbjerre.github.io/pom-dependency-analyzer-web/#/groupId/se.bjurr.violations/artifactId/violation-comments-lib/version/1.95','/content/images/2019/05/pdaw.png',0,0,'published',NULL,'public',NULL,NULL,'1','2019-05-13 17:14:22','1','2019-05-13 17:48:08','1','2019-05-13 17:46:36','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL),('5d5988255a223c04f054fad7','de0a3721-5c8c-429c-bd92-28fc90948629','Release from Jenkins by pushing Git tags','release-from-jenkins-by-pushing-git-tags','{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2019/08/bbs-jenkins-tag-flow.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/stages.png\",\"caption\":\"The stages.\"}],[\"image\",{\"src\":\"/content/images/2019/08/build-info.png\",\"caption\":\"A triggered build.\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-push.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-eventlog.png\"}],[\"image\",{\"src\":\"/content/images/2019/08/bbs-webhook-response.png\"}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=8mrJNkofxq4\",\"html\":\"<iframe width=\\\"480\\\" height=\\\"270\\\" src=\\\"https://www.youtube.com/embed/8mrJNkofxq4?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]],[\"a\",[\"href\",\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This post will explain how to setup a single job in Jenkins that performs releases when tags are pushed to any number of repositories. This means you can:\"]]],[3,\"ul\",[[[0,[],0,\"Let developers perform releases \"],[0,[0],1,\"without admin permissions in Jenkins\"],[0,[],0,\".\"]],[[0,[],0,\"Let Jenkins build releases \"],[0,[0],1,\"without giving Jenkins permission to push to Git\"],[0,[],0,\".\"]],[[0,[],0,\"No Jenkinsfiles in the repositories, \"],[0,[0],1,\"eliminate code duplication\"],[0,[],0,\".\"]],[[0,[],0,\"Trace whoever made the release by looking at the \"],[0,[0],1,\"Git log\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"The code used in the post is here: \"],[0,[1],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\"]]],[1,\"h1\",[[0,[],0,\"Workflow\"]]],[1,\"p\",[[0,[],0,\"I implement a flow like this:\"]]],[3,\"ul\",[[[0,[],0,\"The user pushes a tag to Git\"],[1,[],0,0],[0,[2],0,\"git tag 1.0\"],[1,[],0,1],[0,[],1,\"git push -u origin 1.0\"]],[[0,[],0,\"Git (in my case Bitbucket Server) invokes Jenkins with a webhook\"]],[[0,[],0,\"Jenkins will clone repository, checkout the commit, perform the release and email the user.\"]]]],[10,0],[1,\"h1\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"A single pipeline is created in Jenkins, completely generic and can handle any number of Git repositories. The webhook needs to be added to the Git repositories that should use the job.\"]]],[1,\"h2\",[[0,[],0,\"Jenkins\"]]],[1,\"p\",[[0,[],0,\"A Jenkins pipeline job is created. Triggered by \"],[0,[3],1,\"Generic Webhook Trigger\"],[0,[],0,\". The pipeline can be found here: \"],[0,[4],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"The build log may look like this: \"],[0,[5],1,\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\"]]],[1,\"h2\",[[0,[],0,\"Bitbucket Server\"]]],[1,\"p\",[[0,[],0,\"This workflow is turned on for any repository by configuring the repository with a webhook that triggers on push events. In Bitbucket Server, this includes tag events.\"]]],[10,3],[1,\"p\",[[0,[],0,\"When fiddling with it you may want to check the event log.\"]]],[10,4],[1,\"p\",[[0,[],0,\"It may have some useful info in the response.\"]]],[10,5],[1,\"h1\",[[0,[],0,\"Youtube\"]]],[1,\"p\",[[0,[],0,\"Here is a Youtube clip showing this flow:\"]]],[10,6],[1,\"p\",[]]]}','<p>This post will explain how to setup a single job in Jenkins that performs releases when tags are pushed to any number of repositories. This means you can:</p><ul><li>Let developers perform releases <strong>without admin permissions in Jenkins</strong>.</li><li>Let Jenkins build releases <strong>without giving Jenkins permission to push to Git</strong>.</li><li>No Jenkinsfiles in the repositories, <strong>eliminate code duplication</strong>.</li><li>Trace whoever made the release by looking at the <strong>Git log</strong>.</li></ul><p>The code used in the post is here: <a href=\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\">https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags</a></p><h1 id=\"workflow\">Workflow</h1><p>I implement a flow like this:</p><ul><li>The user pushes a tag to Git<br><em>git tag 1.0<br>git push -u origin 1.0</em></li><li>Git (in my case Bitbucket Server) invokes Jenkins with a webhook</li><li>Jenkins will clone repository, checkout the commit, perform the release and email the user.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/08/bbs-jenkins-tag-flow.png\" class=\"kg-image\"></figure><h1 id=\"implementation\">Implementation</h1><p>A single pipeline is created in Jenkins, completely generic and can handle any number of Git repositories. The webhook needs to be added to the Git repositories that should use the job.</p><h2 id=\"jenkins\">Jenkins</h2><p>A Jenkins pipeline job is created. Triggered by <a href=\"https://github.com/jenkinsci/generic-webhook-trigger-plugin\">Generic Webhook Trigger</a>. The pipeline can be found here: <a href=\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\">https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile</a></p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/08/stages.png\" class=\"kg-image\"><figcaption>The stages.</figcaption></figure><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/08/build-info.png\" class=\"kg-image\"><figcaption>A triggered build.</figcaption></figure><p>The build log may look like this: <a href=\"https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\">https://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt</a></p><h2 id=\"bitbucket-server\">Bitbucket Server</h2><p>This workflow is turned on for any repository by configuring the repository with a webhook that triggers on push events. In Bitbucket Server, this includes tag events.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/08/bbs-webhook-push.png\" class=\"kg-image\"></figure><p>When fiddling with it you may want to check the event log.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/08/bbs-webhook-eventlog.png\" class=\"kg-image\"></figure><p>It may have some useful info in the response.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/08/bbs-webhook-response.png\" class=\"kg-image\"></figure><h1 id=\"youtube\">Youtube</h1><p>Here is a Youtube clip showing this flow:</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/8mrJNkofxq4?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure>','5d5988255a223c04f054fad7','This post will explain how to setup a single job in Jenkins that performs\nreleases when tags are pushed to any number of repositories. This means you can:\n\n * Let developers perform releases without admin permissions in Jenkins.\n * Let Jenkins build releases without giving Jenkins permission to push to Git.\n * No Jenkinsfiles in the repositories, eliminate code duplication.\n * Trace whoever made the release by looking at the Git log.\n\nThe code used in the post is here: \nhttps://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags\n\nWorkflow\nI implement a flow like this:\n\n * The user pushes a tag to Git\n   git tag 1.0\n   git push -u origin 1.0\n * Git (in my case Bitbucket Server) invokes Jenkins with a webhook\n * Jenkins will clone repository, checkout the commit, perform the release and\n   email the user.\n\nImplementation\nA single pipeline is created in Jenkins, completely generic and can handle any\nnumber of Git repositories. The webhook needs to be added to the Git\nrepositories that should use the job.\n\nJenkins\nA Jenkins pipeline job is created. Triggered by Generic Webhook Trigger\n[https://github.com/jenkinsci/generic-webhook-trigger-plugin]. The pipeline can\nbe found here: \nhttps://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/Jenkinsfile\n\nThe stages.A triggered build.The build log may look like this: \nhttps://github.com/tomasbjerre/bitbucket-server-jenkins-release-tags/blob/master/build-log-sample.txt\n\nBitbucket Server\nThis workflow is turned on for any repository by configuring the repository with\na webhook that triggers on push events. In Bitbucket Server, this includes tag\nevents.\n\nWhen fiddling with it you may want to check the event log.\n\nIt may have some useful info in the response.\n\nYoutube\nHere is a Youtube clip showing this flow:','/content/images/2019/08/superhero.png',0,0,'published',NULL,'public',NULL,NULL,'1','2019-08-18 17:17:25','1','2019-09-15 16:35:12','1','2019-08-18 17:52:08','1',NULL,'','',NULL,NULL,NULL,NULL,NULL,NULL,NULL);
/*!40000 ALTER TABLE `posts` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts_authors`
--

DROP TABLE IF EXISTS `posts_authors`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `posts_authors` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `author_id` varchar(24) NOT NULL,
  `sort_order` int(10) unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `posts_authors_author_id_foreign` (`author_id`),
  KEY `posts_authors_post_id_foreign` (`post_id`),
  CONSTRAINT `posts_authors_author_id_foreign` FOREIGN KEY (`author_id`) REFERENCES `users` (`id`),
  CONSTRAINT `posts_authors_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts_authors`
--

LOCK TABLES `posts_authors` WRITE;
/*!40000 ALTER TABLE `posts_authors` DISABLE KEYS */;
INSERT INTO `posts_authors` VALUES ('5b1ad9f6c706ac1a0fe204f1','597b4433e521cb4fbd918692','1',0),('5b1ad9f6c706ac1a0fe204f2','597b4433e521cb4fbd918693','1',0),('5b1ad9f6c706ac1a0fe204f3','597b4433e521cb4fbd918694','1',0),('5b1ad9f6c706ac1a0fe204f4','597b4433e521cb4fbd918696','1',0),('5b1ad9f6c706ac1a0fe204f5','597b4433e521cb4fbd918697','1',0),('5b1ad9f6c706ac1a0fe204f6','597b4433e521cb4fbd918698','1',0),('5b1ad9f6c706ac1a0fe204f7','597b4433e521cb4fbd918699','1',0),('5b1ad9f6c706ac1a0fe204f8','597b4433e521cb4fbd91869b','1',0),('5b1ad9f6c706ac1a0fe204fa','597b4433e521cb4fbd91869e','1',0),('5b1ad9f6c706ac1a0fe204fb','597b4433e521cb4fbd91869f','1',0),('5b1ad9f6c706ac1a0fe204fd','597b4433e521cb4fbd9186a1','1',0),('5b1ad9f6c706ac1a0fe204fe','597b4433e521cb4fbd9186a2','1',0),('5b1ad9f6c706ac1a0fe204ff','597b4433e521cb4fbd9186a3','1',0),('5b1ad9f6c706ac1a0fe20500','597b4433e521cb4fbd9186a4','1',0),('5b1ad9f6c706ac1a0fe20501','597b4433e521cb4fbd9186a5','1',0),('5b1ad9f6c706ac1a0fe20502','597b4433e521cb4fbd9186a6','1',0),('5b1ad9f6c706ac1a0fe20503','597b4433e521cb4fbd9186a7','1',0),('5b1ad9f6c706ac1a0fe20504','597b4433e521cb4fbd9186a8','1',0),('5b1ad9f6c706ac1a0fe20506','59e067ad8edbbd05ee0cef90','1',0),('5b1ad9f6c706ac1a0fe20507','5a23da214b28b905d8304a37','1',0),('5b1ad9f6c706ac1a0fe20508','5a755597c56a61057b219788','1',0),('5b1ad9f6c706ac1a0fe20509','5ac3be89b907cf054a31895e','1',0),('5b7be89c06392206949fa8a3','5b7be89c06392206949fa8a2','1',0),('5b858416d1c4a4085e025db1','5b858416d1c4a4085e025db0','1',0),('5c5e7a20d32e6b08b8dfd509','5c5e7a20d32e6b08b8dfd508','1',0),('5cc35015bbdcf706bbdbc750','5cc35015bbdcf706bbdbc74f','1',0),('5cd9a5ee52156f04cb9dc69f','5cd9a5ee52156f04cb9dc69e','1',0),('5d5988255a223c04f054fad8','5d5988255a223c04f054fad7','1',0);
/*!40000 ALTER TABLE `posts_authors` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts_tags`
--

DROP TABLE IF EXISTS `posts_tags`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `posts_tags` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `tag_id` varchar(24) NOT NULL,
  `sort_order` int(10) unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `posts_tags_tag_id_foreign` (`tag_id`),
  KEY `posts_tags_post_id_foreign` (`post_id`),
  CONSTRAINT `posts_tags_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`),
  CONSTRAINT `posts_tags_tag_id_foreign` FOREIGN KEY (`tag_id`) REFERENCES `tags` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts_tags`
--

LOCK TABLES `posts_tags` WRITE;
/*!40000 ALTER TABLE `posts_tags` DISABLE KEYS */;
INSERT INTO `posts_tags` VALUES ('597b4433e521cb4fbd9186ac','597b4433e521cb4fbd918692','597b4433e521cb4fbd91866f',0),('597b4433e521cb4fbd9186ad','597b4433e521cb4fbd918693','597b4433e521cb4fbd91866f',0),('597b4433e521cb4fbd9186ae','597b4433e521cb4fbd918694','597b4433e521cb4fbd91866f',0),('597b4433e521cb4fbd9186b0','597b4433e521cb4fbd918696','597b4432e521cb4fbd91866a',0),('597b4433e521cb4fbd9186b1','597b4433e521cb4fbd918697','597b4432e521cb4fbd918669',0),('597b4433e521cb4fbd9186b2','597b4433e521cb4fbd918698','597b4433e521cb4fbd91866e',0),('597b4433e521cb4fbd9186b3','597b4433e521cb4fbd918699','597b4433e521cb4fbd91866e',0),('597b4433e521cb4fbd9186b5','597b4433e521cb4fbd91869e','597b4433e521cb4fbd918680',0),('597b4433e521cb4fbd9186b6','597b4433e521cb4fbd91869f','597b4433e521cb4fbd918683',0),('597b4433e521cb4fbd9186b8','597b4433e521cb4fbd9186a1','597b4433e521cb4fbd918681',0),('597b4433e521cb4fbd9186b9','597b4433e521cb4fbd9186a2','597b4433e521cb4fbd918688',0),('597b4433e521cb4fbd9186ba','597b4433e521cb4fbd9186a3','597b4433e521cb4fbd91866f',0),('597b4433e521cb4fbd9186bb','597b4433e521cb4fbd9186a4','597b4433e521cb4fbd918680',0),('597b4433e521cb4fbd9186bc','597b4433e521cb4fbd9186a8','597b4433e521cb4fbd918676',0),('597b4433e521cb4fbd9186bf','597b4433e521cb4fbd918694','597b4433e521cb4fbd918671',1),('597b4433e521cb4fbd9186c1','597b4433e521cb4fbd918696','597b4433e521cb4fbd91866c',1),('597b4433e521cb4fbd9186c2','597b4433e521cb4fbd918697','597b4433e521cb4fbd91866e',1),('597b4433e521cb4fbd9186c3','597b4433e521cb4fbd918698','597b4433e521cb4fbd918675',1),('597b4433e521cb4fbd9186c5','597b4433e521cb4fbd91869e','597b4433e521cb4fbd918681',1),('597b4433e521cb4fbd9186c6','597b4433e521cb4fbd91869f','597b4433e521cb4fbd918689',1),('597b4433e521cb4fbd9186c8','597b4433e521cb4fbd9186a1','597b4433e521cb4fbd918680',1),('597b4433e521cb4fbd9186c9','597b4433e521cb4fbd9186a2','597b4433e521cb4fbd918686',1),('597b4433e521cb4fbd9186ca','597b4433e521cb4fbd9186a3','597b4433e521cb4fbd91868a',1),('597b4433e521cb4fbd9186cb','597b4433e521cb4fbd9186a4','597b4433e521cb4fbd918686',1),('597b4433e521cb4fbd9186cc','597b4433e521cb4fbd9186a8','597b4433e521cb4fbd91868c',1),('597b4434e521cb4fbd9186cf','597b4433e521cb4fbd918694','597b4433e521cb4fbd91867c',2),('597b4434e521cb4fbd9186d0','597b4433e521cb4fbd918696','597b4433e521cb4fbd91866d',2),('597b4434e521cb4fbd9186d1','597b4433e521cb4fbd918697','597b4433e521cb4fbd918672',2),('597b4434e521cb4fbd9186d3','597b4433e521cb4fbd91869e','597b4433e521cb4fbd918682',2),('597b4434e521cb4fbd9186d4','597b4433e521cb4fbd9186a1','597b4433e521cb4fbd918683',2),('597b4434e521cb4fbd9186d5','597b4433e521cb4fbd9186a2','597b4433e521cb4fbd91868a',2),('597b4434e521cb4fbd9186d6','597b4433e521cb4fbd9186a4','597b4433e521cb4fbd918687',2),('597b4434e521cb4fbd9186d7','597b4433e521cb4fbd9186a8','597b4433e521cb4fbd91868d',2),('597b4434e521cb4fbd9186da','597b4433e521cb4fbd918696','597b4433e521cb4fbd91867f',3),('597b4434e521cb4fbd9186dc','597b4433e521cb4fbd91869e','597b4433e521cb4fbd918683',3),('597b4434e521cb4fbd9186dd','597b4433e521cb4fbd9186a1','597b4433e521cb4fbd918685',3),('597b4434e521cb4fbd9186de','597b4433e521cb4fbd9186a2','597b4433e521cb4fbd91868b',3),('597b4434e521cb4fbd9186df','597b4433e521cb4fbd9186a4','597b4433e521cb4fbd918688',3),('597b4434e521cb4fbd9186e2','597b4433e521cb4fbd91869e','597b4433e521cb4fbd918684',4),('597b4434e521cb4fbd9186e3','597b4433e521cb4fbd9186a1','597b4433e521cb4fbd918687',4),('597b4434e521cb4fbd9186e4','597b4433e521cb4fbd9186a2','597b4433e521cb4fbd918680',4),('597b4434e521cb4fbd9186e5','597b4433e521cb4fbd9186a4','597b4433e521cb4fbd918681',4),('597b4434e521cb4fbd9186e6','597b4433e521cb4fbd9186a4','597b4433e521cb4fbd918689',5),('59e06c588edbbd05ee0cef92','59e067ad8edbbd05ee0cef90','59e06c588edbbd05ee0cef91',0),('59e06c588edbbd05ee0cef93','59e067ad8edbbd05ee0cef90','597b4433e521cb4fbd918680',1),('59e06c588edbbd05ee0cef95','59e067ad8edbbd05ee0cef90','59e06c588edbbd05ee0cef94',2),('5a3a309178ae32059caf0fc7','5a23da214b28b905d8304a37','5a3a309178ae32059caf0fc6',0),('5a3a309178ae32059caf0fc9','5a23da214b28b905d8304a37','597b4433e521cb4fbd91866f',1),('5a7557efc56a61057b21978b','5a755597c56a61057b219788','597b4433e521cb4fbd918680',0),('5a7557efc56a61057b21978d','5a755597c56a61057b219788','5a7557efc56a61057b219789',1),('5a7557efc56a61057b219790','5a755597c56a61057b219788','5a7557efc56a61057b21978a',2),('5a7557efc56a61057b219794','5a755597c56a61057b219788','597b4433e521cb4fbd918681',3),('5a912890c4446d055b89f2ec','597b4433e521cb4fbd9186a7','597b4433e521cb4fbd91866f',0),('5a912890c4446d055b89f2ee','597b4433e521cb4fbd9186a7','5a7557efc56a61057b21978a',1),('5a912890c4446d055b89f2f1','597b4433e521cb4fbd9186a7','5a912890c4446d055b89f2eb',2),('5a912890c4446d055b89f2f5','597b4433e521cb4fbd9186a7','59e06c588edbbd05ee0cef94',3),('5a912890c4446d055b89f2fa','597b4433e521cb4fbd9186a7','5a3a309178ae32059caf0fc6',4),('5a9128efc4446d055b89f2fc','597b4433e521cb4fbd9186a6','5a9128efc4446d055b89f2fb',0),('5a9128efc4446d055b89f2fe','597b4433e521cb4fbd9186a6','597b4433e521cb4fbd918680',1),('5a9128efc4446d055b89f301','597b4433e521cb4fbd9186a6','5a7557efc56a61057b21978a',2),('5a9128efc4446d055b89f305','597b4433e521cb4fbd9186a6','597b4433e521cb4fbd918681',3),('5a912b36c4446d055b89f306','597b4433e521cb4fbd9186a5','597b4433e521cb4fbd918681',0),('5a912b89c4446d055b89f307','597b4433e521cb4fbd9186a4','5a7557efc56a61057b21978a',6),('5a912c07c4446d055b89f308','597b4433e521cb4fbd9186a2','5a7557efc56a61057b21978a',5),('5a912c5ac4446d055b89f309','597b4433e521cb4fbd9186a1','5a7557efc56a61057b21978a',5),('5a912c88c4446d055b89f30b','597b4433e521cb4fbd91869f','5a912c88c4446d055b89f30a',2),('5a912c88c4446d055b89f30d','597b4433e521cb4fbd91869f','5a7557efc56a61057b21978a',3),('5a912c88c4446d055b89f310','597b4433e521cb4fbd91869f','597b4433e521cb4fbd918681',4),('5ac3bf30b907cf054a31895f','5ac3be89b907cf054a31895e','597b4433e521cb4fbd918680',0),('5ac3bf30b907cf054a318960','5ac3be89b907cf054a31895e','5a7557efc56a61057b219789',1),('5ac3bf30b907cf054a318961','5ac3be89b907cf054a31895e','5a7557efc56a61057b21978a',2),('5ac3bf30b907cf054a318962','5ac3be89b907cf054a31895e','597b4433e521cb4fbd918681',3),('5ac3bf30b907cf054a318963','5ac3be89b907cf054a31895e','5a912890c4446d055b89f2eb',4),('5ac3bf30b907cf054a318964','5ac3be89b907cf054a31895e','5a9128efc4446d055b89f2fb',5),('5ac3bf30b907cf054a318965','5ac3be89b907cf054a31895e','597b4433e521cb4fbd918686',6),('5ac3bf30b907cf054a318966','5ac3be89b907cf054a31895e','597b4433e521cb4fbd918687',7),('5ac3bf30b907cf054a318967','5ac3be89b907cf054a31895e','597b4433e521cb4fbd918683',8),('5b7beb9c06392206949fa8a5','5b7be89c06392206949fa8a2','5b7beb9c06392206949fa8a4',0),('5b7beb9c06392206949fa8a6','5b7be89c06392206949fa8a2','597b4433e521cb4fbd91866f',1),('5b858918d1c4a4085e025db5','5b858416d1c4a4085e025db0','5b858918d1c4a4085e025db2',0),('5b858918d1c4a4085e025db6','5b858416d1c4a4085e025db0','597b4433e521cb4fbd91866f',1),('5b858918d1c4a4085e025db7','5b858416d1c4a4085e025db0','5b858918d1c4a4085e025db3',2),('5b858918d1c4a4085e025db8','5b858416d1c4a4085e025db0','5b858918d1c4a4085e025db4',3),('5c5e853ed32e6b08b8dfd520','5c5e7a20d32e6b08b8dfd508','597b4433e521cb4fbd918680',0),('5c5e853ed32e6b08b8dfd521','5c5e7a20d32e6b08b8dfd508','5a7557efc56a61057b219789',1),('5c5e853ed32e6b08b8dfd522','5c5e7a20d32e6b08b8dfd508','5a9128efc4446d055b89f2fb',2),('5c5e853ed32e6b08b8dfd523','5c5e7a20d32e6b08b8dfd508','5a7557efc56a61057b21978a',3),('5cc3508dbbdcf706bbdbc756','5cc35015bbdcf706bbdbc74f','5cc3508dbbdcf706bbdbc752',0),('5cc3508dbbdcf706bbdbc757','5cc35015bbdcf706bbdbc74f','5cc3508dbbdcf706bbdbc753',1),('5cc3508dbbdcf706bbdbc758','5cc35015bbdcf706bbdbc74f','5cc3508dbbdcf706bbdbc754',2),('5cc3508dbbdcf706bbdbc759','5cc35015bbdcf706bbdbc74f','5cc3508dbbdcf706bbdbc755',3),('5cc3508dbbdcf706bbdbc75a','5cc35015bbdcf706bbdbc74f','597b4433e521cb4fbd91866f',4),('5cd9add852156f04cb9dc704','5cd9a5ee52156f04cb9dc69e','59e06c588edbbd05ee0cef94',0),('5cd9add852156f04cb9dc705','5cd9a5ee52156f04cb9dc69e','5a3a309178ae32059caf0fc6',1),('5d5988a05a223c04f054fada','5d5988255a223c04f054fad7','597b4433e521cb4fbd918680',0),('5d5988a05a223c04f054fadb','5d5988255a223c04f054fad7','597b4433e521cb4fbd918681',1),('5d5988a05a223c04f054fadc','5d5988255a223c04f054fad7','597b4433e521cb4fbd918687',2);
/*!40000 ALTER TABLE `posts_tags` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `refreshtokens`
--

DROP TABLE IF EXISTS `refreshtokens`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `refreshtokens` (
  `id` varchar(24) NOT NULL,
  `token` varchar(191) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  `client_id` varchar(24) NOT NULL,
  `expires` bigint(20) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `refreshtokens_token_unique` (`token`),
  KEY `refreshtokens_user_id_foreign` (`user_id`),
  KEY `refreshtokens_client_id_foreign` (`client_id`),
  CONSTRAINT `refreshtokens_client_id_foreign` FOREIGN KEY (`client_id`) REFERENCES `clients` (`id`),
  CONSTRAINT `refreshtokens_user_id_foreign` FOREIGN KEY (`user_id`) REFERENCES `users` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `refreshtokens`
--

LOCK TABLES `refreshtokens` WRITE;
/*!40000 ALTER TABLE `refreshtokens` DISABLE KEYS */;
/*!40000 ALTER TABLE `refreshtokens` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `roles`
--

DROP TABLE IF EXISTS `roles`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `roles` (
  `id` varchar(24) NOT NULL,
  `name` varchar(50) NOT NULL,
  `description` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `roles_name_unique` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `roles`
--

LOCK TABLES `roles` WRITE;
/*!40000 ALTER TABLE `roles` DISABLE KEYS */;
INSERT INTO `roles` VALUES ('597b43301ffc934f6c0277bc','Administrator','Administrators','2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277bd','Editor','Editors','2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277be','Author','Authors','2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('597b43301ffc934f6c0277bf','Owner','Blog Owner','2017-07-28 13:59:12','1','2017-07-28 13:59:12','1'),('5a9845ce6b6eb71021b13f88','Contributor','Contributors','2018-03-01 18:26:22','1','2018-03-01 18:26:22','1'),('5bd215b976568b252315b9af','Admin Integration','External Apps','2018-10-25 19:12:57','1','2018-10-25 19:12:57','1');
/*!40000 ALTER TABLE `roles` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `roles_users`
--

DROP TABLE IF EXISTS `roles_users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `roles_users` (
  `id` varchar(24) NOT NULL,
  `role_id` varchar(24) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `roles_users`
--

LOCK TABLES `roles_users` WRITE;
/*!40000 ALTER TABLE `roles_users` DISABLE KEYS */;
INSERT INTO `roles_users` VALUES ('597b43311ffc934f6c0277f1','597b43301ffc934f6c0277be','5951f5fca366002ebd5dbef7'),('597b43311ffc934f6c02785b','597b43301ffc934f6c0277bf','1');
/*!40000 ALTER TABLE `roles_users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `sessions`
--

DROP TABLE IF EXISTS `sessions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `sessions` (
  `id` varchar(24) NOT NULL,
  `session_id` varchar(32) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  `session_data` varchar(2000) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `sessions_session_id_unique` (`session_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `sessions`
--

LOCK TABLES `sessions` WRITE;
/*!40000 ALTER TABLE `sessions` DISABLE KEYS */;
INSERT INTO `sessions` VALUES ('5bd2162476568b252315b9f7','Oo_7s_HxH_l6Z_yPdbT1uHkE35W-bi2Y','1','{\"cookie\":{\"originalMaxAge\":15768000000,\"expires\":\"2019-04-26T07:14:44.067Z\",\"secure\":false,\"httpOnly\":true,\"path\":\"/ghost\",\"sameSite\":\"lax\"},\"user_id\":\"1\",\"origin\":\"http://127.0.0.1:2368\",\"user_agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\",\"ip\":\"127.0.0.1\"}','2018-10-25 19:14:44','2018-10-25 19:14:44'),('5cc34fe6bbdcf706bbdbc74e','EsqE3edcNmdh7rHaOHMpZtHZ8pjZhjZD','1','{\"cookie\":{\"originalMaxAge\":15768000000,\"expires\":\"2019-10-26T06:37:26.456Z\",\"secure\":false,\"httpOnly\":true,\"path\":\"/ghost\",\"sameSite\":\"lax\"},\"user_id\":\"1\",\"origin\":\"http://127.0.0.1:2368\",\"user_agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\",\"ip\":\"127.0.0.1\"}','2019-04-26 18:37:26','2019-04-26 18:37:26'),('5e0dbb71bab3f604b5c1c800','5DCcEZvpfPRB5msn8dz6ugsBVlWHA_WP','1','{\"cookie\":{\"originalMaxAge\":15768000000,\"expires\":\"2020-07-02T21:44:17.160Z\",\"secure\":false,\"httpOnly\":true,\"path\":\"/ghost\",\"sameSite\":\"lax\"},\"user_id\":\"1\",\"origin\":\"http://127.0.0.1:2368\",\"user_agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\",\"ip\":\"127.0.0.1\"}','2020-01-02 09:44:17','2020-01-02 09:44:17');
/*!40000 ALTER TABLE `sessions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `settings`
--

DROP TABLE IF EXISTS `settings`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `settings` (
  `id` varchar(24) NOT NULL,
  `key` varchar(50) NOT NULL,
  `value` text,
  `type` varchar(50) NOT NULL DEFAULT 'core',
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `settings_key_unique` (`key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `settings`
--

LOCK TABLES `settings` WRITE;
/*!40000 ALTER TABLE `settings` DISABLE KEYS */;
INSERT INTO `settings` VALUES ('597b4336e521cb4fbd918641','db_hash','f07e86ed-63bb-4c95-8c99-79ee6f39ff43','core','2017-07-28 13:59:18','1','2017-07-28 13:59:18','1'),('597b4336e521cb4fbd918642','next_update_check','1578044655','core','2017-07-28 13:59:18','1','2020-01-02 09:44:15','1'),('597b4336e521cb4fbd918645','title','Tomas Bjerre:s blog','blog','2014-10-04 13:54:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918646','description','My blog on things I find interesting.','blog','2014-10-04 13:54:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918647','logo','/content/images/2017/07/kna_crop_600.png','blog','2014-10-04 13:54:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918648','cover_image','/content/images/2017/07/kna_crop.jpg','blog','2014-10-04 13:54:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918649','icon','/content/images/2017/07/kna_crop_fyrt.png','blog','2017-07-28 13:59:18','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd91864a','default_locale','en','blog','2017-07-28 13:59:18','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd91864b','active_timezone','Europe/Amsterdam','blog','2016-09-10 13:47:09','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd91864c','force_i18n','true','blog','2017-07-28 13:59:18','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd91864d','permalinks','/:slug/','blog','2014-10-04 13:54:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd91864e','amp','false','blog','2017-01-19 20:26:24','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd91864f','ghost_head','','blog','2015-10-05 19:01:28','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918650','ghost_foot','<script>\n  (function(i,s,o,g,r,a,m){i[\'GoogleAnalyticsObject\']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,\'script\',\'//www.google-analytics.com/analytics.js\',\'ga\');\n\n  ga(\'create\', \'UA-52248240-1\', \'auto\');\n  ga(\'send\', \'pageview\');\n\n</script>','blog','2015-10-05 19:01:28','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918651','facebook','tomas.bjerre','blog','2016-09-10 13:47:09','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918652','twitter','','blog','2016-09-10 13:47:09','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918653','labs','{\"publicAPI\":true}','blog','2015-10-05 19:01:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918654','navigation','[]','blog','2015-10-05 19:01:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918655','slack','[{\"url\":\"\"}]','blog','2016-09-10 13:47:09','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd918656','active_theme','casper-2.7.1','theme','2017-07-28 13:59:18','1','2019-01-23 17:35:38','1'),('597b4336e521cb4fbd918657','active_apps','[]','app','2017-07-28 13:59:18','1','2017-07-28 13:59:18','1'),('597b4336e521cb4fbd918658','installed_apps','[]','app','2017-07-28 13:59:18','1','2017-11-29 15:58:51','1'),('597b4336e521cb4fbd918659','is_private','false','private','2015-10-05 19:01:29','1','2017-07-30 06:51:18','1'),('597b4336e521cb4fbd91865a','password','','private','2015-10-05 19:01:29','1','2017-07-30 06:51:18','1'),('59a12a184060832d0a3ebaef','unsplash','','blog','2017-08-26 07:58:16','1','2017-08-26 07:58:16','1'),('59e30f45a6f4d31287474009','public_hash','39254190745ce8652e6dcc96ddab1d','private','2017-10-15 07:33:25','1','2017-10-15 07:33:25','1'),('5a9845d18441db108c7f4854','notifications','[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"1426c720-2199-11ea-bd7d-eb32987a0437\",\"custom\":false,\"createdAt\":\"2019-12-18T13:19:54.000Z\",\"type\":\"info\",\"top\":false,\"message\":\"Ghost <a href=\\\"https://github.com/TryGhost/Ghost/releases\\\">2.37.2</a> has been released, <a href=\\\"https://ghost.org/update/?v=2.11.1\\\">click here</a> to upgrade.\",\"seen\":false,\"addedAt\":\"2020-01-02T09:44:15.098Z\"}]','core','2018-03-01 18:26:25','1','2020-01-02 09:44:15','1'),('5bd215b876568b252315b9ae','session_secret','a6020f0afc349ed3090a5f4042d0980be486fba993a60fd95f8e7a9782578943','core','2018-10-25 19:12:56','1','2018-10-25 19:12:56','1'),('5c41e2bcf581f72431885b9f','members_public_key','-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAIcB8eScCmGAkBhdHRG0PXRF52EYOBUP6KTbdlyKkvzNwKR7M6mKail87OH7CVZZ\nMCJG+zgrAgKTSf67h7kLuoDeBy7uwqI/UEktBZaoij21JvKqeytMoXNshFE5xSB5bZNQh7v2\n4sI6AA7CtdlfHhq87ApceoamLbaXFjH6FqnfAgMBAAE=\n-----END RSA PUBLIC KEY-----\n','members','2019-01-18 14:29:16','1','2019-01-18 14:29:16','1'),('5c41e2bcf581f72431885ba0','members_private_key','-----BEGIN RSA PRIVATE KEY-----\nMIICXQIBAAKBgQCHAfHknAphgJAYXR0RtD10RedhGDgVD+ik23ZcipL8zcCkezOpimopfOzh\n+wlWWTAiRvs4KwICk0n+u4e5C7qA3gcu7sKiP1BJLQWWqIo9tSbyqnsrTKFzbIRROcUgeW2T\nUIe79uLCOgAOwrXZXx4avOwKXHqGpi22lxYx+hap3wIDAQABAoGAYge8f/X557B1cOtzpx5M\nHTiA+NSgLFL/52zftfR9JIm2HvjCm5HA4l0umCwzLHrYY5tM4Q0BiaMvcIL02x20DrvXx2e8\n59qRSBp0S8YeLtSIBPAVpwkMvBHmRo8fW6mJ9rvGhwC2QAKpvOF5zHEzT+JvLelVE+hkfLgj\nb1AsSeECQQDW30yG0KdewZUVThgJxa04GPjR6YUYqVONbmgM2Pu5z5guXULqiMoUAaNmItFa\nE+vomrm8jIqDe6V/m/rgTwYPAkEAoNlKzNf+ROh8WbY5SnFIZHGSXO/MHWMHFYfnDGMOS2XW\nemaqpwZh3SIzogsI80sx95CHbJYqrA6CcpzbiUpvMQJBAKwMyL36PMoGv1T73wuUGQseE74g\n+c2xZ5GLog0HgShbLUOSsY4SUZyLY0bZAkDaSr9JhQeQkSiOzQnM36jUHckCQC61SsJXSpAK\nDuoiP1q9sK7yR/VS8SvrcMgw0chID4OXjAO5Qn8EHHERmHrKx4uW32irgJfot3WFNR5BLIMe\nqVECQQCCIWWhdCbzl50QupC5TRhsg3IWPew7c2FHF9qdLq63XsDaxH6wTDqvh4LoJPSMWJoP\nAylJJ9hPiyTYrHoqK7ax\n-----END RSA PRIVATE KEY-----\n','members','2019-01-18 14:29:16','1','2019-01-18 14:29:16','1'),('5c41e2bcf581f72431885ba1','members_session_secret','0014c52b3e51a160fdff2aad813c1f012286bd45e16f93b32aaddc0527745a18','members','2019-01-18 14:29:16','1','2019-01-18 14:29:16','1');
/*!40000 ALTER TABLE `settings` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `subscribers`
--

DROP TABLE IF EXISTS `subscribers`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `subscribers` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) DEFAULT NULL,
  `email` varchar(191) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'pending',
  `post_id` varchar(24) DEFAULT NULL,
  `subscribed_url` varchar(2000) DEFAULT NULL,
  `subscribed_referrer` varchar(2000) DEFAULT NULL,
  `unsubscribed_url` varchar(2000) DEFAULT NULL,
  `unsubscribed_at` datetime DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `subscribers_email_unique` (`email`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `subscribers`
--

LOCK TABLES `subscribers` WRITE;
/*!40000 ALTER TABLE `subscribers` DISABLE KEYS */;
/*!40000 ALTER TABLE `subscribers` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `tags`
--

DROP TABLE IF EXISTS `tags`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `tags` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `description` text,
  `feature_image` varchar(2000) DEFAULT NULL,
  `parent_id` varchar(191) DEFAULT NULL,
  `visibility` varchar(50) NOT NULL DEFAULT 'public',
  `meta_title` varchar(2000) DEFAULT NULL,
  `meta_description` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `tags_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `tags`
--

LOCK TABLES `tags` WRITE;
/*!40000 ALTER TABLE `tags` DISABLE KEYS */;
INSERT INTO `tags` VALUES ('597b4432e521cb4fbd918661','Getting Started','getting-started',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:54:26','1','2014-10-04 13:54:26','1'),('597b4432e521cb4fbd918662','ghost','ghost-post',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4432e521cb4fbd918663','npm','npm',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4432e521cb4fbd918664','Ghost','ghost-post-2',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2018-05-18 19:09:18','1'),('597b4432e521cb4fbd918665','Blog','blog-2',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2018-05-18 19:09:18','1'),('597b4432e521cb4fbd918666','NodeJS','nodejs-2',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4432e521cb4fbd918667','NPM','npm-2',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4432e521cb4fbd918668','sipervisor','sipervisor',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4432e521cb4fbd918669','Linux','linux',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:23:49','1'),('597b4432e521cb4fbd91866a','Owncloud','owncloud',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4432e521cb4fbd91866b','Supervisor','supervisor',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd91866c','PHP','php',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd91866d','OpenSSL','openssl',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd91866e','Raspberry','raspberry',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd91866f','Java','java',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2018-02-26 08:29:01','1'),('597b4433e521cb4fbd918670','ByggarMonster','byggarmonster',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:22:41','1'),('597b4433e521cb4fbd918671','HTMLUnitGenerator','htmlunitgenerator',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd918672','ArchLinux','archlinux',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd918673','Apache','apache',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:22:14','1'),('597b4433e521cb4fbd918674','disk_cache','disk_cache',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:22:14','1'),('597b4433e521cb4fbd918675','Surveillance','surveillance',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd918676','Swedish','swedish',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2018-06-08 19:31:20','1'),('597b4433e521cb4fbd918677','Ubuntu','ubuntu',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:23:49','1'),('597b4433e521cb4fbd918678','TrueCrypt','truecrypt',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:23:49','1'),('597b4433e521cb4fbd918679','VirtualBox','virtualbox',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:23:49','1'),('597b4433e521cb4fbd91867a','Grails','grails',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:22:14','1'),('597b4433e521cb4fbd91867b','Windows','windows',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2017-12-20 06:23:49','1'),('597b4433e521cb4fbd91867c','HTMLUnit','htmlunit',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd91867d','blog','blog',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd91867e','nodejs','nodejs',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd91867f','Lighttpd','lighttpd',NULL,NULL,NULL,'public',NULL,NULL,'2014-10-04 13:57:36','2','2014-10-04 13:57:36','1'),('597b4433e521cb4fbd918680','Jenkins','jenkins',NULL,NULL,NULL,'public',NULL,NULL,'2015-02-18 17:55:53','1','2018-04-04 04:10:58','1'),('597b4433e521cb4fbd918681','Git','git',NULL,NULL,NULL,'public',NULL,NULL,'2015-02-18 17:55:53','1','2018-04-04 04:10:58','1'),('597b4433e521cb4fbd918682','Atlassian','atlassian',NULL,NULL,NULL,'public',NULL,NULL,'2015-02-18 17:55:53','1','2015-02-18 17:55:53','1'),('597b4433e521cb4fbd918683','Stash','stash',NULL,NULL,NULL,'public',NULL,NULL,'2015-02-18 17:55:53','1','2018-04-04 04:10:58','1'),('597b4433e521cb4fbd918684','Pull Request','pull-request',NULL,NULL,NULL,'public',NULL,NULL,'2015-02-18 17:55:53','1','2015-02-18 17:55:53','1'),('597b4433e521cb4fbd918685','changelog','changelog',NULL,NULL,NULL,'public',NULL,NULL,'2015-12-19 09:28:11','1','2018-02-24 09:11:54','1'),('597b4433e521cb4fbd918686','GitHub','github',NULL,NULL,NULL,'public',NULL,NULL,'2016-09-11 07:01:02','1','2018-04-04 04:10:58','1'),('597b4433e521cb4fbd918687','Bitbucket Server','bitbucket-server',NULL,NULL,NULL,'public',NULL,NULL,'2016-09-11 07:01:02','1','2018-04-04 04:10:58','1'),('597b4433e521cb4fbd918688','Static Code Analysis','static-code-analysis',NULL,NULL,NULL,'public',NULL,NULL,'2016-09-11 07:01:02','1','2018-02-24 09:10:31','1'),('597b4433e521cb4fbd918689','Plugins','plugins',NULL,NULL,NULL,'public',NULL,NULL,'2016-09-11 07:01:02','1','2018-02-24 09:12:56','1'),('597b4433e521cb4fbd91868a','Clean Code','clean-code',NULL,NULL,NULL,'public',NULL,NULL,'2016-09-11 07:01:24','1','2018-02-24 09:10:31','1'),('597b4433e521cb4fbd91868b','Travis CI','travis-ci',NULL,NULL,NULL,'public',NULL,NULL,'2016-09-11 07:02:08','1','2018-02-24 09:10:31','1'),('597b4433e521cb4fbd91868c','aktiebolag','aktiebolag',NULL,NULL,NULL,'public',NULL,NULL,'2017-06-30 07:03:09','1','2018-06-08 19:31:20','1'),('597b4433e521cb4fbd91868d','bokföring','bokforing',NULL,NULL,NULL,'public',NULL,NULL,'2017-06-30 07:03:09','1','2018-06-08 19:31:20','1'),('597b696908a2db60d1f648bd','cloudflare','cloudflare',NULL,NULL,NULL,'public',NULL,NULL,'2017-07-28 16:42:17','1','2018-05-18 19:09:18','1'),('597b696908a2db60d1f648bf','buster','buster',NULL,NULL,NULL,'public',NULL,NULL,'2017-07-28 16:42:17','1','2018-05-18 19:09:18','1'),('59e06c588edbbd05ee0cef91','fitnesse','fitnesse',NULL,NULL,NULL,'public',NULL,NULL,'2017-10-13 07:33:44','1','2018-02-24 09:03:08','1'),('59e06c588edbbd05ee0cef94','maven','maven',NULL,NULL,NULL,'public',NULL,NULL,'2017-10-13 07:33:44','1','2018-02-26 08:29:01','1'),('5a3a309178ae32059caf0fc6','Gradle','gradle',NULL,NULL,NULL,'public',NULL,NULL,'2017-12-20 09:42:41','1','2018-02-26 08:29:01','1'),('5a7557efc56a61057b219789','pipeline','pipeline',NULL,NULL,NULL,'public',NULL,NULL,'2018-02-03 06:34:23','1','2018-04-04 04:10:58','1'),('5a7557efc56a61057b21978a','continuous-integration','continuous-integration',NULL,NULL,NULL,'public',NULL,NULL,'2018-02-03 06:34:23','1','2018-04-04 04:10:58','1'),('5a912890c4446d055b89f2eb','tools','tools',NULL,NULL,NULL,'public',NULL,NULL,'2018-02-24 08:55:44','1','2018-04-04 04:10:58','1'),('5a9128efc4446d055b89f2fb','GitLab','gitlab',NULL,NULL,NULL,'public',NULL,NULL,'2018-02-24 08:57:19','1','2018-04-04 04:10:58','1'),('5a912c88c4446d055b89f30a','Bitbucket-Server','bitbucket-server-2',NULL,NULL,NULL,'public',NULL,NULL,'2018-02-24 09:12:40','1','2018-02-24 09:12:56','1'),('5b7beb9c06392206949fa8a4','kotlin','kotlin',NULL,NULL,NULL,'public',NULL,NULL,'2018-08-21 10:38:20','1','2018-08-21 10:38:20','1'),('5b858918d1c4a4085e025db2','intellij','intellij',NULL,NULL,NULL,'public',NULL,NULL,'2018-08-28 17:40:40','1','2018-08-28 17:40:40','1'),('5b858918d1c4a4085e025db3','eclipse','eclipse',NULL,NULL,NULL,'public',NULL,NULL,'2018-08-28 17:40:40','1','2018-08-28 17:40:40','1'),('5b858918d1c4a4085e025db4','ide','ide',NULL,NULL,NULL,'public',NULL,NULL,'2018-08-28 17:40:40','1','2018-08-28 17:40:40','1'),('5cc3508dbbdcf706bbdbc752','wiremock','wiremock',NULL,NULL,NULL,'public',NULL,NULL,'2019-04-26 18:40:13','1','2019-04-26 18:40:13','1'),('5cc3508dbbdcf706bbdbc753','jaxrs','jaxrs',NULL,NULL,NULL,'public',NULL,NULL,'2019-04-26 18:40:13','1','2019-04-26 18:40:13','1'),('5cc3508dbbdcf706bbdbc754','mock','mock',NULL,NULL,NULL,'public',NULL,NULL,'2019-04-26 18:40:13','1','2019-04-26 18:40:13','1'),('5cc3508dbbdcf706bbdbc755','stub','stub',NULL,NULL,NULL,'public',NULL,NULL,'2019-04-26 18:40:13','1','2019-04-26 18:40:13','1');
/*!40000 ALTER TABLE `tags` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `users`
--

DROP TABLE IF EXISTS `users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `users` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `ghost_auth_access_token` varchar(32) DEFAULT NULL,
  `ghost_auth_id` varchar(24) DEFAULT NULL,
  `password` varchar(60) NOT NULL,
  `email` varchar(191) NOT NULL,
  `profile_image` varchar(2000) DEFAULT NULL,
  `cover_image` varchar(2000) DEFAULT NULL,
  `bio` text,
  `website` varchar(2000) DEFAULT NULL,
  `location` text,
  `facebook` varchar(2000) DEFAULT NULL,
  `twitter` varchar(2000) DEFAULT NULL,
  `accessibility` text,
  `status` varchar(50) NOT NULL DEFAULT 'active',
  `locale` varchar(6) DEFAULT NULL,
  `visibility` varchar(50) NOT NULL DEFAULT 'public',
  `meta_title` varchar(2000) DEFAULT NULL,
  `meta_description` varchar(2000) DEFAULT NULL,
  `tour` text,
  `last_seen` datetime DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `users_slug_unique` (`slug`),
  UNIQUE KEY `users_email_unique` (`email`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `users`
--

LOCK TABLES `users` WRITE;
/*!40000 ALTER TABLE `users` DISABLE KEYS */;
INSERT INTO `users` VALUES ('1','Tomas Bjerre','tomas',NULL,NULL,'$2a$10$iD8F/pG2N7EUGkS.NSNSYe8LbhbSUe2/GBcPdBAvPMs5Wac.kqvLW','tomas.bjerre85@gmail.com','//www.gravatar.com/avatar/8b306fbe92f324ae9717c03ec7651116?s=250&d=mm&r=x',NULL,NULL,'http://tomasbjerreab.se/','Sweden','tomas.bjerre',NULL,NULL,'active',NULL,'public',NULL,NULL,'[\"using-the-editor\",\"getting-started\",\"upload-a-theme\"]','2020-01-02 09:44:17','2017-07-28 13:59:12','1','2020-01-02 09:44:17','1'),('5951f5fca366002ebd5dbef7','Ghost','ghost',NULL,NULL,'$2a$10$uze1/90yOzDy279TMLotMuuIMDJFHmTsck15PWrbkk0EjYGBwZsf.','ghost-author@example.com',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'active',NULL,'public',NULL,NULL,NULL,NULL,'2017-07-28 13:59:12','1','2017-07-28 13:59:12','1');
/*!40000 ALTER TABLE `users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `webhooks`
--

DROP TABLE IF EXISTS `webhooks`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `webhooks` (
  `id` varchar(24) NOT NULL,
  `event` varchar(50) NOT NULL,
  `target_url` varchar(2000) NOT NULL,
  `created_at` datetime NOT NULL,
  `created_by` varchar(24) NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `updated_by` varchar(24) DEFAULT NULL,
  `name` varchar(191) DEFAULT NULL,
  `secret` varchar(191) DEFAULT NULL,
  `api_version` varchar(50) NOT NULL DEFAULT 'v2',
  `integration_id` varchar(24) DEFAULT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'available',
  `last_triggered_at` datetime DEFAULT NULL,
  `last_triggered_status` varchar(50) DEFAULT NULL,
  `last_triggered_error` varchar(50) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `webhooks`
--

LOCK TABLES `webhooks` WRITE;
/*!40000 ALTER TABLE `webhooks` DISABLE KEYS */;
/*!40000 ALTER TABLE `webhooks` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2020-01-02 10:46:22
